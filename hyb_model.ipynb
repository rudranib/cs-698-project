{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def get_cv_idxs(n, cv_idx=0, val_pct=0.2, seed=42):\n",
    "    \"\"\" Get a list of index values for Validation set from a dataset\n",
    "\n",
    "    Arguments:\n",
    "        n : int, Total number of elements in the data set.\n",
    "        cv_idx : int, starting index [idx_start = cv_idx*int(val_pct*n)]\n",
    "        val_pct : (int, float), validation set percentage\n",
    "        seed : seed value for RandomState\n",
    "\n",
    "    Returns:\n",
    "        list of indexes\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n_val = int(val_pct * n)\n",
    "    idx_start = cv_idx * n_val\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[idx_start:idx_start + n_val]\n",
    "\n",
    "\n",
    "def split_by_idx(idxs, *a):\n",
    "    \"\"\"\n",
    "    Split each array passed as *a, to a pair of arrays like this (elements selected by idxs,  the remaining elements)\n",
    "    This can be used to split multiple arrays containing training data to validation and training set.\n",
    "\n",
    "    :param idxs [int]: list of indexes selected\n",
    "    :param a list: list of np.array, each array should have same amount of elements in the first dimension\n",
    "    :return: list of tuples, each containing a split of corresponding array from *a.\n",
    "            First element of each tuple is an array composed from elements selected by idxs,\n",
    "            second element is an array of remaining elements.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(len(a[0]), dtype=bool)\n",
    "    mask[np.array(idxs)] = True\n",
    "    return [(o[mask], o[~mask]) for o in a]\n",
    "\n",
    "\n",
    "class AutoEncoder(object):\n",
    "\n",
    "    def __init__(self, data, validation_perc=0.2, lr=0.001,\n",
    "                 intermediate_size=1000, encoded_size=100):\n",
    "\n",
    "        # create training dataloader and validation tensor\n",
    "        self.data = data\n",
    "        self.val_idxs = get_cv_idxs(n=data.shape[0], val_pct=validation_perc)\n",
    "        [(self.val, self.train)] = split_by_idx(self.val_idxs, data)\n",
    "        self.dataset = AETrainingData(self.train)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=64, shuffle=True,\n",
    "                                     num_workers=multiprocessing.cpu_count())\n",
    "        #print('datal=',self.dataloader)\n",
    "        self.val = torch.from_numpy(self.val.values).\\\n",
    "            type(torch.FloatTensor).cuda()\n",
    "\n",
    "        # instantiate the encoder and decoder nets\n",
    "        size = data.shape[1]\n",
    "        self.encoder = Encoder(size, intermediate_size, encoded_size).cuda()\n",
    "        self.decoder = Decoder(size, intermediate_size, encoded_size).cuda()\n",
    "\n",
    "        # instantiate the optimizers\n",
    "        self.encoder_optimizer = optim.Adam(\n",
    "            self.encoder.parameters(), lr=lr, weight_decay=1e-8)\n",
    "        self.decoder_optimizer = optim.Adam(\n",
    "            self.decoder.parameters(), lr=lr, weight_decay=1e-8)\n",
    "\n",
    "        # instantiate the loss criterion\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_step(self, input_tensor, target_tensor):\n",
    "        # clear the gradients in the optimizers\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through\n",
    "        encoded_representation = self.encoder(input_tensor)\n",
    "        reconstruction = self.decoder(encoded_representation)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.criterion(reconstruction, target_tensor)\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Step the optimizers to update the model weights\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        # Return the loss value to track training progress\n",
    "        return loss.item()\n",
    "    \n",
    "    def reset(self, train=True):\n",
    "        # due to dropout the network behaves differently in training and\n",
    "        # evaluation modes\n",
    "        if train: self.encoder.train(); self.decoder.train()\n",
    "        else: self.encoder.eval(); self.decoder.eval()\n",
    "\n",
    "    def get_val_loss(self, input_tensor, target_tensor):\n",
    "        self.reset(train=False)\n",
    "        encoded = self.encoder(input_tensor)\n",
    "        decoded = self.decoder(encoded)\n",
    "        loss = self.criterion(decoded, target_tensor)\n",
    "        return loss.item()\n",
    "\n",
    "    def train_loop(self, epochs, print_every_n_batches=20):\n",
    "\n",
    "        # Cycle through epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "            # Cycle through batches\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                #print(i,batch)\n",
    "                \n",
    "                self.reset(train=True)\n",
    "\n",
    "                input_tensor = batch['input'].cuda()\n",
    "                target_tensor = batch['target'].cuda()\n",
    "\n",
    "                loss = self.train_step(input_tensor, target_tensor)\n",
    "\n",
    "                if i % print_every_n_batches == 0 and i != 0:\n",
    "                    #print('i=',i)\n",
    "                    val_loss = self.get_val_loss(self.val, self.val)\n",
    "                    print(f'train loss: {round(loss, 8)} | ' +\n",
    "                          f'validation loss: {round(val_loss, 8)}')\n",
    "                    self.train_losses.append(loss)\n",
    "                    self.val_losses.append(val_loss)\n",
    "\n",
    "    def get_encoded_representations(self):\n",
    "        to_encode = torch.from_numpy(self.data.values).type(\n",
    "            torch.FloatTensor).cuda()\n",
    "        self.reset(train=False)\n",
    "        encodings = self.encoder(to_encode).cpu().data.numpy()\n",
    "        return encodings\n",
    "\n",
    "\n",
    "class AETrainingData(Dataset):\n",
    "    \"\"\"\n",
    "    Format the training dataset to be input into the auto encoder.\n",
    "    Takes in dataframe and converts it to a PyTorch Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_train):\n",
    "        self.x = x_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a example from the data set as a pytorch tensor.\n",
    "        \"\"\"\n",
    "        # Get example/target pair at idx as numpy arrays\n",
    "        x, y = self.x.iloc[idx].values, self.x.iloc[idx].values\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "\n",
    "        # Return pair\n",
    "        return {'input': x, 'target': y}\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size, encoding_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, intermediate_size),\n",
    "            nn.BatchNorm1d(intermediate_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(intermediate_size, encoding_size),\n",
    "            nn.BatchNorm1d(encoding_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, intermediate_size, encoding_size):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_size, intermediate_size),\n",
    "            nn.BatchNorm1d(intermediate_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(intermediate_size, output_size),\n",
    "            nn.BatchNorm1d(output_size),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'new_df_matrix.pkl', 'rb') as fh:\n",
    "    new_df1 = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=new_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=ndf.apply(lambda x: x/x.max(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'tfidf_matrix.pkl', 'rb') as fh:\n",
    "    tfidf_df1 = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        float32\n",
       "1        float32\n",
       "2        float32\n",
       "3        float32\n",
       "4        float32\n",
       "          ...   \n",
       "69873    float32\n",
       "69874    float32\n",
       "69875    float32\n",
       "69876    float32\n",
       "69877    float32\n",
       "Length: 69878, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf=ndf.astype(np.float32)\n",
    "ndf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       float32\n",
       "1       float32\n",
       "2       float32\n",
       "3       float32\n",
       "4       float32\n",
       "         ...   \n",
       "6245    float32\n",
       "6246    float32\n",
       "6247    float32\n",
       "6248    float32\n",
       "6249    float32\n",
       "Length: 6250, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df1=tfidf_df1.astype(np.float32)\n",
    "tfidf_df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([ndf, tfidf_df1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 76128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 69878)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 6250)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_1 = AutoEncoder(df_new, validation_perc=0.1, lr=1e-3, intermediate_size=1000, encoded_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "train loss: 0.22646828 | validation loss: 0.23503129\n",
      "train loss: 0.22065391 | validation loss: 0.21594258\n",
      "train loss: 0.21307196 | validation loss: 0.20262966\n",
      "train loss: 0.20799197 | validation loss: 0.19725658\n",
      "train loss: 0.2080341 | validation loss: 0.20119549\n",
      "train loss: 0.20572947 | validation loss: 0.19951518\n",
      "train loss: 0.19499739 | validation loss: 0.19331014\n",
      "Epoch 2/100\n",
      "train loss: 0.18429087 | validation loss: 0.17570373\n",
      "train loss: 0.18372478 | validation loss: 0.17426759\n",
      "train loss: 0.1862165 | validation loss: 0.16731024\n",
      "train loss: 0.18219544 | validation loss: 0.16120487\n",
      "train loss: 0.17845634 | validation loss: 0.16109593\n",
      "train loss: 0.17349663 | validation loss: 0.17318565\n",
      "train loss: 0.16278948 | validation loss: 0.15721422\n",
      "Epoch 3/100\n",
      "train loss: 0.15996629 | validation loss: 0.1490335\n",
      "train loss: 0.15556537 | validation loss: 0.14511812\n",
      "train loss: 0.15418683 | validation loss: 0.14615913\n",
      "train loss: 0.14689085 | validation loss: 0.13781169\n",
      "train loss: 0.14607467 | validation loss: 0.13566533\n",
      "train loss: 0.14696142 | validation loss: 0.13158467\n",
      "train loss: 0.14712456 | validation loss: 0.1324921\n",
      "Epoch 4/100\n",
      "train loss: 0.13949807 | validation loss: 0.12461053\n",
      "train loss: 0.14390942 | validation loss: 0.13752148\n",
      "train loss: 0.13816059 | validation loss: 0.12913167\n",
      "train loss: 0.13683276 | validation loss: 0.1243179\n",
      "train loss: 0.1247545 | validation loss: 0.11602502\n",
      "train loss: 0.12086353 | validation loss: 0.11409263\n",
      "train loss: 0.1206303 | validation loss: 0.10920659\n",
      "Epoch 5/100\n",
      "train loss: 0.13095269 | validation loss: 0.10798554\n",
      "train loss: 0.11194471 | validation loss: 0.11149683\n",
      "train loss: 0.11223468 | validation loss: 0.1025854\n",
      "train loss: 0.11297902 | validation loss: 0.0986675\n",
      "train loss: 0.1122816 | validation loss: 0.10047892\n",
      "train loss: 0.11227354 | validation loss: 0.0976134\n",
      "train loss: 0.10680774 | validation loss: 0.09770622\n",
      "Epoch 6/100\n",
      "train loss: 0.10289426 | validation loss: 0.09001773\n",
      "train loss: 0.10071729 | validation loss: 0.09296863\n",
      "train loss: 0.09672613 | validation loss: 0.0922325\n",
      "train loss: 0.10144196 | validation loss: 0.08790738\n",
      "train loss: 0.09175511 | validation loss: 0.0883699\n",
      "train loss: 0.10532007 | validation loss: 0.08670258\n",
      "train loss: 0.09102145 | validation loss: 0.08175155\n",
      "Epoch 7/100\n",
      "train loss: 0.08346365 | validation loss: 0.08363651\n",
      "train loss: 0.08679961 | validation loss: 0.07947484\n",
      "train loss: 0.08086544 | validation loss: 0.0743843\n",
      "train loss: 0.08264498 | validation loss: 0.07737555\n",
      "train loss: 0.08439629 | validation loss: 0.07732234\n",
      "train loss: 0.08863612 | validation loss: 0.07353578\n",
      "train loss: 0.08024348 | validation loss: 0.07158888\n",
      "Epoch 8/100\n",
      "train loss: 0.07710958 | validation loss: 0.06614448\n",
      "train loss: 0.07637698 | validation loss: 0.06866895\n",
      "train loss: 0.07484642 | validation loss: 0.06819263\n",
      "train loss: 0.0735933 | validation loss: 0.06857579\n",
      "train loss: 0.07207624 | validation loss: 0.06676338\n",
      "train loss: 0.06771515 | validation loss: 0.06310993\n",
      "train loss: 0.07847448 | validation loss: 0.06421251\n",
      "Epoch 9/100\n",
      "train loss: 0.06595048 | validation loss: 0.06121366\n",
      "train loss: 0.06246497 | validation loss: 0.06252488\n",
      "train loss: 0.06692111 | validation loss: 0.06104418\n",
      "train loss: 0.066231 | validation loss: 0.06001268\n",
      "train loss: 0.06469245 | validation loss: 0.06002598\n",
      "train loss: 0.07767266 | validation loss: 0.05624589\n",
      "train loss: 0.07242654 | validation loss: 0.05534473\n",
      "Epoch 10/100\n",
      "train loss: 0.06136933 | validation loss: 0.05583619\n",
      "train loss: 0.0600955 | validation loss: 0.05459562\n",
      "train loss: 0.06768099 | validation loss: 0.05187743\n",
      "train loss: 0.07168631 | validation loss: 0.05277548\n",
      "train loss: 0.05936169 | validation loss: 0.05356937\n",
      "train loss: 0.05674313 | validation loss: 0.05152492\n",
      "train loss: 0.05602403 | validation loss: 0.0503099\n",
      "Epoch 11/100\n",
      "train loss: 0.05265253 | validation loss: 0.0441849\n",
      "train loss: 0.0524643 | validation loss: 0.04787572\n",
      "train loss: 0.05077387 | validation loss: 0.04596075\n",
      "train loss: 0.05503394 | validation loss: 0.0462079\n",
      "train loss: 0.05245405 | validation loss: 0.04710668\n",
      "train loss: 0.04939313 | validation loss: 0.04405527\n",
      "train loss: 0.059127 | validation loss: 0.04800977\n",
      "Epoch 12/100\n",
      "train loss: 0.0468165 | validation loss: 0.04415536\n",
      "train loss: 0.05084118 | validation loss: 0.05018415\n",
      "train loss: 0.05064886 | validation loss: 0.04597306\n",
      "train loss: 0.05783038 | validation loss: 0.04211662\n",
      "train loss: 0.0493522 | validation loss: 0.04271231\n",
      "train loss: 0.04940398 | validation loss: 0.04623979\n",
      "train loss: 0.04630123 | validation loss: 0.04118054\n",
      "Epoch 13/100\n",
      "train loss: 0.04609779 | validation loss: 0.03960394\n",
      "train loss: 0.04951243 | validation loss: 0.0384958\n",
      "train loss: 0.04441919 | validation loss: 0.04382584\n",
      "train loss: 0.04543833 | validation loss: 0.03995762\n",
      "train loss: 0.05283441 | validation loss: 0.0381051\n",
      "train loss: 0.05636696 | validation loss: 0.03727927\n",
      "train loss: 0.05262404 | validation loss: 0.03802056\n",
      "Epoch 14/100\n",
      "train loss: 0.05642649 | validation loss: 0.04208408\n",
      "train loss: 0.0432703 | validation loss: 0.03827796\n",
      "train loss: 0.04204395 | validation loss: 0.03745993\n",
      "train loss: 0.05288157 | validation loss: 0.03630651\n",
      "train loss: 0.04950773 | validation loss: 0.03450278\n",
      "train loss: 0.04162172 | validation loss: 0.03492926\n",
      "train loss: 0.06646857 | validation loss: 0.03496204\n",
      "Epoch 15/100\n",
      "train loss: 0.03967196 | validation loss: 0.03430487\n",
      "train loss: 0.04962967 | validation loss: 0.03444855\n",
      "train loss: 0.03518071 | validation loss: 0.03118066\n",
      "train loss: 0.03919612 | validation loss: 0.03240146\n",
      "train loss: 0.03782208 | validation loss: 0.03318917\n",
      "train loss: 0.03741125 | validation loss: 0.03151921\n",
      "train loss: 0.03523024 | validation loss: 0.03110942\n",
      "Epoch 16/100\n",
      "train loss: 0.04776472 | validation loss: 0.03296129\n",
      "train loss: 0.04769976 | validation loss: 0.03360277\n",
      "train loss: 0.0362403 | validation loss: 0.0319486\n",
      "train loss: 0.03900083 | validation loss: 0.02971444\n",
      "train loss: 0.03552739 | validation loss: 0.02837353\n",
      "train loss: 0.03644656 | validation loss: 0.03355198\n",
      "train loss: 0.03584037 | validation loss: 0.0307148\n",
      "Epoch 17/100\n",
      "train loss: 0.03350254 | validation loss: 0.03104827\n",
      "train loss: 0.03356358 | validation loss: 0.0296113\n",
      "train loss: 0.03209407 | validation loss: 0.03057324\n",
      "train loss: 0.03829744 | validation loss: 0.03118463\n",
      "train loss: 0.03557044 | validation loss: 0.02824884\n",
      "train loss: 0.03392073 | validation loss: 0.03285217\n",
      "train loss: 0.04758991 | validation loss: 0.02885919\n",
      "Epoch 18/100\n",
      "train loss: 0.04183895 | validation loss: 0.02765186\n",
      "train loss: 0.04577324 | validation loss: 0.02997329\n",
      "train loss: 0.03417543 | validation loss: 0.02812433\n",
      "train loss: 0.04209896 | validation loss: 0.02845844\n",
      "train loss: 0.03390544 | validation loss: 0.02914889\n",
      "train loss: 0.03270335 | validation loss: 0.02754858\n",
      "train loss: 0.03423969 | validation loss: 0.02839992\n",
      "Epoch 19/100\n",
      "train loss: 0.03349688 | validation loss: 0.02797321\n",
      "train loss: 0.03297151 | validation loss: 0.02703465\n",
      "train loss: 0.03086619 | validation loss: 0.02537811\n",
      "train loss: 0.02997559 | validation loss: 0.0296738\n",
      "train loss: 0.03176567 | validation loss: 0.02800899\n",
      "train loss: 0.03950803 | validation loss: 0.02576279\n",
      "train loss: 0.02997351 | validation loss: 0.02509064\n",
      "Epoch 20/100\n",
      "train loss: 0.03180121 | validation loss: 0.02595264\n",
      "train loss: 0.03052294 | validation loss: 0.02629375\n",
      "train loss: 0.03110804 | validation loss: 0.02674052\n",
      "train loss: 0.02810755 | validation loss: 0.02419461\n",
      "train loss: 0.02826637 | validation loss: 0.02365983\n",
      "train loss: 0.02817408 | validation loss: 0.02352539\n",
      "train loss: 0.03063615 | validation loss: 0.02379036\n",
      "Epoch 21/100\n",
      "train loss: 0.02954412 | validation loss: 0.02404332\n",
      "train loss: 0.02865445 | validation loss: 0.02382512\n",
      "train loss: 0.0295466 | validation loss: 0.0257585\n",
      "train loss: 0.02912642 | validation loss: 0.02306954\n",
      "train loss: 0.02644842 | validation loss: 0.02114347\n",
      "train loss: 0.02780865 | validation loss: 0.02266375\n",
      "train loss: 0.02631882 | validation loss: 0.02268834\n",
      "Epoch 22/100\n",
      "train loss: 0.04435417 | validation loss: 0.02144612\n",
      "train loss: 0.02839816 | validation loss: 0.02571718\n",
      "train loss: 0.04225943 | validation loss: 0.02571348\n",
      "train loss: 0.03194269 | validation loss: 0.02448043\n",
      "train loss: 0.02612272 | validation loss: 0.02234929\n",
      "train loss: 0.04226457 | validation loss: 0.0231648\n",
      "train loss: 0.02576086 | validation loss: 0.02315367\n",
      "Epoch 23/100\n",
      "train loss: 0.02695696 | validation loss: 0.02718988\n",
      "train loss: 0.03871436 | validation loss: 0.02549148\n",
      "train loss: 0.02711788 | validation loss: 0.02265557\n",
      "train loss: 0.03825125 | validation loss: 0.0274722\n",
      "train loss: 0.02791027 | validation loss: 0.02000666\n",
      "train loss: 0.0272479 | validation loss: 0.02442476\n",
      "train loss: 0.02454268 | validation loss: 0.02465948\n",
      "Epoch 24/100\n",
      "train loss: 0.02626614 | validation loss: 0.02262924\n",
      "train loss: 0.02644145 | validation loss: 0.02252202\n",
      "train loss: 0.02304353 | validation loss: 0.02039641\n",
      "train loss: 0.02373146 | validation loss: 0.02091379\n",
      "train loss: 0.03386172 | validation loss: 0.02096273\n",
      "train loss: 0.02655572 | validation loss: 0.02254955\n",
      "train loss: 0.02444714 | validation loss: 0.02259888\n",
      "Epoch 25/100\n",
      "train loss: 0.02638005 | validation loss: 0.01895066\n",
      "train loss: 0.02856246 | validation loss: 0.02229884\n",
      "train loss: 0.02626683 | validation loss: 0.02167863\n",
      "train loss: 0.02590332 | validation loss: 0.02135963\n",
      "train loss: 0.02308272 | validation loss: 0.02155099\n",
      "train loss: 0.0296988 | validation loss: 0.02148008\n",
      "train loss: 0.02320913 | validation loss: 0.02116474\n",
      "Epoch 26/100\n",
      "train loss: 0.02890465 | validation loss: 0.02008004\n",
      "train loss: 0.02163497 | validation loss: 0.01745963\n",
      "train loss: 0.02153959 | validation loss: 0.02034713\n",
      "train loss: 0.02120876 | validation loss: 0.01881318\n",
      "train loss: 0.02317694 | validation loss: 0.01996954\n",
      "train loss: 0.02122709 | validation loss: 0.0201987\n",
      "train loss: 0.02990101 | validation loss: 0.02007172\n",
      "Epoch 27/100\n",
      "train loss: 0.02534363 | validation loss: 0.01865781\n",
      "train loss: 0.0210263 | validation loss: 0.01739209\n",
      "train loss: 0.02907849 | validation loss: 0.01910673\n",
      "train loss: 0.02431639 | validation loss: 0.01996651\n",
      "train loss: 0.02141798 | validation loss: 0.01638438\n",
      "train loss: 0.02227832 | validation loss: 0.01760974\n",
      "train loss: 0.02133155 | validation loss: 0.01925479\n",
      "Epoch 28/100\n",
      "train loss: 0.02817282 | validation loss: 0.01840157\n",
      "train loss: 0.02801365 | validation loss: 0.01955871\n",
      "train loss: 0.0194857 | validation loss: 0.01620012\n",
      "train loss: 0.02062717 | validation loss: 0.0178317\n",
      "train loss: 0.01949254 | validation loss: 0.01693532\n",
      "train loss: 0.0266235 | validation loss: 0.01758368\n",
      "train loss: 0.02219203 | validation loss: 0.01778715\n",
      "Epoch 29/100\n",
      "train loss: 0.02434787 | validation loss: 0.0179452\n",
      "train loss: 0.01800036 | validation loss: 0.01624836\n",
      "train loss: 0.02562089 | validation loss: 0.01648394\n",
      "train loss: 0.02075801 | validation loss: 0.01829929\n",
      "train loss: 0.02483915 | validation loss: 0.01574923\n",
      "train loss: 0.01845173 | validation loss: 0.0162838\n",
      "train loss: 0.01752684 | validation loss: 0.01783768\n",
      "Epoch 30/100\n",
      "train loss: 0.02023945 | validation loss: 0.016538\n",
      "train loss: 0.01927376 | validation loss: 0.01534171\n",
      "train loss: 0.01553246 | validation loss: 0.01507541\n",
      "train loss: 0.0201616 | validation loss: 0.01608373\n",
      "train loss: 0.01543813 | validation loss: 0.01668531\n",
      "train loss: 0.01653747 | validation loss: 0.01456678\n",
      "train loss: 0.01569437 | validation loss: 0.01581551\n",
      "Epoch 31/100\n",
      "train loss: 0.02087325 | validation loss: 0.01582147\n",
      "train loss: 0.01510951 | validation loss: 0.01465055\n",
      "train loss: 0.01871723 | validation loss: 0.01501665\n",
      "train loss: 0.01535022 | validation loss: 0.01528785\n",
      "train loss: 0.01894126 | validation loss: 0.01403638\n",
      "train loss: 0.01875167 | validation loss: 0.01468605\n",
      "train loss: 0.01299559 | validation loss: 0.01464351\n",
      "Epoch 32/100\n",
      "train loss: 0.0148913 | validation loss: 0.01343803\n",
      "train loss: 0.01780571 | validation loss: 0.01367761\n",
      "train loss: 0.0156523 | validation loss: 0.01349867\n",
      "train loss: 0.01631301 | validation loss: 0.0139519\n",
      "train loss: 0.01281214 | validation loss: 0.01369717\n",
      "train loss: 0.01433647 | validation loss: 0.01329881\n",
      "train loss: 0.015449 | validation loss: 0.0133868\n",
      "Epoch 33/100\n",
      "train loss: 0.0122408 | validation loss: 0.0134517\n",
      "train loss: 0.01141614 | validation loss: 0.01327274\n",
      "train loss: 0.01608995 | validation loss: 0.01332182\n",
      "train loss: 0.012406 | validation loss: 0.01237245\n",
      "train loss: 0.01164753 | validation loss: 0.01277861\n",
      "train loss: 0.01554887 | validation loss: 0.0126799\n",
      "train loss: 0.01189022 | validation loss: 0.01288659\n",
      "Epoch 34/100\n",
      "train loss: 0.01102059 | validation loss: 0.01276784\n",
      "train loss: 0.01812392 | validation loss: 0.01177636\n",
      "train loss: 0.01156626 | validation loss: 0.01190877\n",
      "train loss: 0.01588475 | validation loss: 0.01214914\n",
      "train loss: 0.01352134 | validation loss: 0.01187474\n",
      "train loss: 0.01114487 | validation loss: 0.01200667\n",
      "train loss: 0.01303027 | validation loss: 0.011827\n",
      "Epoch 35/100\n",
      "train loss: 0.01522374 | validation loss: 0.01145572\n",
      "train loss: 0.01101096 | validation loss: 0.01159812\n",
      "train loss: 0.01106345 | validation loss: 0.0110482\n",
      "train loss: 0.01512364 | validation loss: 0.01112324\n",
      "train loss: 0.0104484 | validation loss: 0.01149001\n",
      "train loss: 0.00907548 | validation loss: 0.01123355\n",
      "train loss: 0.01002211 | validation loss: 0.01099786\n",
      "Epoch 36/100\n",
      "train loss: 0.01107821 | validation loss: 0.01096417\n",
      "train loss: 0.00958614 | validation loss: 0.01099651\n",
      "train loss: 0.01096306 | validation loss: 0.01045337\n",
      "train loss: 0.01388029 | validation loss: 0.01045262\n",
      "train loss: 0.0106161 | validation loss: 0.01069501\n",
      "train loss: 0.01025722 | validation loss: 0.01083941\n",
      "train loss: 0.00924777 | validation loss: 0.01073093\n",
      "Epoch 37/100\n",
      "train loss: 0.01437324 | validation loss: 0.01001766\n",
      "train loss: 0.01100198 | validation loss: 0.00996638\n",
      "train loss: 0.01187649 | validation loss: 0.01003554\n",
      "train loss: 0.00926371 | validation loss: 0.01018144\n",
      "train loss: 0.01133499 | validation loss: 0.01000973\n",
      "train loss: 0.00800238 | validation loss: 0.00978429\n",
      "train loss: 0.00965314 | validation loss: 0.00985288\n",
      "Epoch 38/100\n",
      "train loss: 0.00835818 | validation loss: 0.0103882\n",
      "train loss: 0.01330089 | validation loss: 0.01005708\n",
      "train loss: 0.01016052 | validation loss: 0.01006429\n",
      "train loss: 0.01162479 | validation loss: 0.00956819\n",
      "train loss: 0.00822937 | validation loss: 0.00944558\n",
      "train loss: 0.00830368 | validation loss: 0.00945101\n",
      "train loss: 0.00843042 | validation loss: 0.00955745\n",
      "Epoch 39/100\n",
      "train loss: 0.00884268 | validation loss: 0.00942206\n",
      "train loss: 0.01035624 | validation loss: 0.00955415\n",
      "train loss: 0.00860229 | validation loss: 0.00953904\n",
      "train loss: 0.01222769 | validation loss: 0.0091552\n",
      "train loss: 0.00963775 | validation loss: 0.00909689\n",
      "train loss: 0.01004922 | validation loss: 0.00955319\n",
      "train loss: 0.00845889 | validation loss: 0.00897243\n",
      "Epoch 40/100\n",
      "train loss: 0.00819197 | validation loss: 0.00920061\n",
      "train loss: 0.01134564 | validation loss: 0.00884654\n",
      "train loss: 0.0074561 | validation loss: 0.00883187\n",
      "train loss: 0.00940419 | validation loss: 0.00879771\n",
      "train loss: 0.01150091 | validation loss: 0.00881489\n",
      "train loss: 0.00964159 | validation loss: 0.00878805\n",
      "train loss: 0.00890347 | validation loss: 0.00884147\n",
      "Epoch 41/100\n",
      "train loss: 0.01311531 | validation loss: 0.00882006\n",
      "train loss: 0.0111555 | validation loss: 0.00883649\n",
      "train loss: 0.01176626 | validation loss: 0.00871154\n",
      "train loss: 0.00714614 | validation loss: 0.00878518\n",
      "train loss: 0.00939717 | validation loss: 0.00860028\n",
      "train loss: 0.00896354 | validation loss: 0.00887138\n",
      "train loss: 0.00794457 | validation loss: 0.00855069\n",
      "Epoch 42/100\n",
      "train loss: 0.01191938 | validation loss: 0.00868688\n",
      "train loss: 0.00797883 | validation loss: 0.00845555\n",
      "train loss: 0.00743357 | validation loss: 0.00856833\n",
      "train loss: 0.00882462 | validation loss: 0.00844406\n",
      "train loss: 0.01011503 | validation loss: 0.00837155\n",
      "train loss: 0.01014593 | validation loss: 0.00838769\n",
      "train loss: 0.00662916 | validation loss: 0.00833028\n",
      "Epoch 43/100\n",
      "train loss: 0.00924544 | validation loss: 0.00836136\n",
      "train loss: 0.00698081 | validation loss: 0.00825826\n",
      "train loss: 0.00765371 | validation loss: 0.00828042\n",
      "train loss: 0.00895322 | validation loss: 0.00814813\n",
      "train loss: 0.00747059 | validation loss: 0.00815809\n",
      "train loss: 0.01073662 | validation loss: 0.00819011\n",
      "train loss: 0.00703428 | validation loss: 0.00819468\n",
      "Epoch 44/100\n",
      "train loss: 0.00968732 | validation loss: 0.00802206\n",
      "train loss: 0.00665763 | validation loss: 0.00821914\n",
      "train loss: 0.01321117 | validation loss: 0.00797806\n",
      "train loss: 0.00790131 | validation loss: 0.00796087\n",
      "train loss: 0.0079154 | validation loss: 0.00793931\n",
      "train loss: 0.00836412 | validation loss: 0.00789123\n",
      "train loss: 0.00735913 | validation loss: 0.00785625\n",
      "Epoch 45/100\n",
      "train loss: 0.01120947 | validation loss: 0.00803066\n",
      "train loss: 0.00719306 | validation loss: 0.00787507\n",
      "train loss: 0.00801892 | validation loss: 0.00784265\n",
      "train loss: 0.01037994 | validation loss: 0.00776838\n",
      "train loss: 0.00711751 | validation loss: 0.00783103\n",
      "train loss: 0.00960721 | validation loss: 0.00779469\n",
      "train loss: 0.00687327 | validation loss: 0.00786352\n",
      "Epoch 46/100\n",
      "train loss: 0.00888413 | validation loss: 0.00765111\n",
      "train loss: 0.00822071 | validation loss: 0.00779912\n",
      "train loss: 0.00634039 | validation loss: 0.00774257\n",
      "train loss: 0.00582064 | validation loss: 0.00764761\n",
      "train loss: 0.00952957 | validation loss: 0.00754326\n",
      "train loss: 0.00640831 | validation loss: 0.00744338\n",
      "train loss: 0.0073329 | validation loss: 0.00773989\n",
      "Epoch 47/100\n",
      "train loss: 0.00826214 | validation loss: 0.00751477\n",
      "train loss: 0.00784384 | validation loss: 0.00745626\n",
      "train loss: 0.01182805 | validation loss: 0.0074182\n",
      "train loss: 0.00780632 | validation loss: 0.00742568\n",
      "train loss: 0.00872937 | validation loss: 0.00748439\n",
      "train loss: 0.00537146 | validation loss: 0.00747647\n",
      "train loss: 0.00783425 | validation loss: 0.00762737\n",
      "Epoch 48/100\n",
      "train loss: 0.00902821 | validation loss: 0.00737965\n",
      "train loss: 0.00760378 | validation loss: 0.0075925\n",
      "train loss: 0.00587468 | validation loss: 0.00744421\n",
      "train loss: 0.00728442 | validation loss: 0.00742423\n",
      "train loss: 0.00931943 | validation loss: 0.00724606\n",
      "train loss: 0.00602242 | validation loss: 0.00728774\n",
      "train loss: 0.00636229 | validation loss: 0.00722594\n",
      "Epoch 49/100\n",
      "train loss: 0.00675979 | validation loss: 0.00723012\n",
      "train loss: 0.00582311 | validation loss: 0.00740234\n",
      "train loss: 0.00611233 | validation loss: 0.00727699\n",
      "train loss: 0.00644971 | validation loss: 0.00716091\n",
      "train loss: 0.00790522 | validation loss: 0.00725695\n",
      "train loss: 0.00678954 | validation loss: 0.00711234\n",
      "train loss: 0.00960028 | validation loss: 0.00715095\n",
      "Epoch 50/100\n",
      "train loss: 0.00652601 | validation loss: 0.00732584\n",
      "train loss: 0.00566943 | validation loss: 0.00711314\n",
      "train loss: 0.00550586 | validation loss: 0.00703822\n",
      "train loss: 0.00514919 | validation loss: 0.00709196\n",
      "train loss: 0.00662443 | validation loss: 0.00701453\n",
      "train loss: 0.00635433 | validation loss: 0.00697068\n",
      "train loss: 0.00788594 | validation loss: 0.00710383\n",
      "Epoch 51/100\n",
      "train loss: 0.00824997 | validation loss: 0.00703188\n",
      "train loss: 0.00650815 | validation loss: 0.00705242\n",
      "train loss: 0.0056964 | validation loss: 0.00703371\n",
      "train loss: 0.00730934 | validation loss: 0.00709133\n",
      "train loss: 0.00792456 | validation loss: 0.00707614\n",
      "train loss: 0.00890427 | validation loss: 0.00690369\n",
      "train loss: 0.00845031 | validation loss: 0.00691463\n",
      "Epoch 52/100\n",
      "train loss: 0.00692865 | validation loss: 0.00684798\n",
      "train loss: 0.01102567 | validation loss: 0.0070248\n",
      "train loss: 0.00686139 | validation loss: 0.00703361\n",
      "train loss: 0.00886961 | validation loss: 0.00707098\n",
      "train loss: 0.00443729 | validation loss: 0.00693265\n",
      "train loss: 0.00818713 | validation loss: 0.00682464\n",
      "train loss: 0.00820983 | validation loss: 0.00682999\n",
      "Epoch 53/100\n",
      "train loss: 0.00590189 | validation loss: 0.00687456\n",
      "train loss: 0.0061757 | validation loss: 0.00693989\n",
      "train loss: 0.00555176 | validation loss: 0.00688447\n",
      "train loss: 0.01057114 | validation loss: 0.00680305\n",
      "train loss: 0.00664337 | validation loss: 0.00678339\n",
      "train loss: 0.0066134 | validation loss: 0.00680829\n",
      "train loss: 0.00644832 | validation loss: 0.00677191\n",
      "Epoch 54/100\n",
      "train loss: 0.00456534 | validation loss: 0.00675673\n",
      "train loss: 0.00787436 | validation loss: 0.00673266\n",
      "train loss: 0.00576734 | validation loss: 0.00680358\n",
      "train loss: 0.00859043 | validation loss: 0.00669365\n",
      "train loss: 0.00892635 | validation loss: 0.00677043\n",
      "train loss: 0.00624775 | validation loss: 0.00665409\n",
      "train loss: 0.0059687 | validation loss: 0.00664589\n",
      "Epoch 55/100\n",
      "train loss: 0.00642522 | validation loss: 0.0066245\n",
      "train loss: 0.00463965 | validation loss: 0.00687035\n",
      "train loss: 0.00411197 | validation loss: 0.0066298\n",
      "train loss: 0.01066151 | validation loss: 0.00657132\n",
      "train loss: 0.00556748 | validation loss: 0.00668084\n",
      "train loss: 0.00536639 | validation loss: 0.00671536\n",
      "train loss: 0.00493084 | validation loss: 0.00669366\n",
      "Epoch 56/100\n",
      "train loss: 0.00740651 | validation loss: 0.00664161\n",
      "train loss: 0.00633409 | validation loss: 0.00641885\n",
      "train loss: 0.00867659 | validation loss: 0.00640228\n",
      "train loss: 0.00555626 | validation loss: 0.00631239\n",
      "train loss: 0.00727367 | validation loss: 0.00616902\n",
      "train loss: 0.00688798 | validation loss: 0.0061659\n",
      "train loss: 0.00641994 | validation loss: 0.00626539\n",
      "Epoch 57/100\n",
      "train loss: 0.00431069 | validation loss: 0.00636713\n",
      "train loss: 0.00594821 | validation loss: 0.00619968\n",
      "train loss: 0.00623696 | validation loss: 0.00613239\n",
      "train loss: 0.00391762 | validation loss: 0.00625085\n",
      "train loss: 0.00439664 | validation loss: 0.00615914\n",
      "train loss: 0.00609137 | validation loss: 0.00613229\n",
      "train loss: 0.0058777 | validation loss: 0.00611178\n",
      "Epoch 58/100\n",
      "train loss: 0.00661016 | validation loss: 0.00604799\n",
      "train loss: 0.00671137 | validation loss: 0.00607183\n",
      "train loss: 0.00567626 | validation loss: 0.00602546\n",
      "train loss: 0.00552497 | validation loss: 0.00607251\n",
      "train loss: 0.00545081 | validation loss: 0.00612783\n",
      "train loss: 0.00403557 | validation loss: 0.00603049\n",
      "train loss: 0.00725161 | validation loss: 0.0059881\n",
      "Epoch 59/100\n",
      "train loss: 0.00683121 | validation loss: 0.006063\n",
      "train loss: 0.00494625 | validation loss: 0.00605417\n",
      "train loss: 0.00512806 | validation loss: 0.00601414\n",
      "train loss: 0.00414021 | validation loss: 0.00604374\n",
      "train loss: 0.00489461 | validation loss: 0.0060112\n",
      "train loss: 0.00415305 | validation loss: 0.00614253\n",
      "train loss: 0.00562285 | validation loss: 0.00605017\n",
      "Epoch 60/100\n",
      "train loss: 0.00759294 | validation loss: 0.00597664\n",
      "train loss: 0.00641835 | validation loss: 0.00601345\n",
      "train loss: 0.0052905 | validation loss: 0.00611159\n",
      "train loss: 0.0064394 | validation loss: 0.00597548\n",
      "train loss: 0.0103 | validation loss: 0.00597315\n",
      "train loss: 0.00512344 | validation loss: 0.00599647\n",
      "train loss: 0.00621745 | validation loss: 0.00598469\n",
      "Epoch 61/100\n",
      "train loss: 0.00551112 | validation loss: 0.0059543\n",
      "train loss: 0.00381087 | validation loss: 0.005979\n",
      "train loss: 0.00883603 | validation loss: 0.00596728\n",
      "train loss: 0.00497847 | validation loss: 0.00593998\n",
      "train loss: 0.00468704 | validation loss: 0.00597266\n",
      "train loss: 0.00452997 | validation loss: 0.00600717\n",
      "train loss: 0.00548707 | validation loss: 0.0059443\n",
      "Epoch 62/100\n",
      "train loss: 0.00490253 | validation loss: 0.00595233\n",
      "train loss: 0.00755965 | validation loss: 0.00591802\n",
      "train loss: 0.0077109 | validation loss: 0.00590553\n",
      "train loss: 0.00470974 | validation loss: 0.00585709\n",
      "train loss: 0.00748514 | validation loss: 0.00592693\n",
      "train loss: 0.00846033 | validation loss: 0.00591545\n",
      "train loss: 0.00503543 | validation loss: 0.00591323\n",
      "Epoch 63/100\n",
      "train loss: 0.00420554 | validation loss: 0.00590345\n",
      "train loss: 0.00589788 | validation loss: 0.00586925\n",
      "train loss: 0.00720074 | validation loss: 0.00596315\n",
      "train loss: 0.00465672 | validation loss: 0.00595652\n",
      "train loss: 0.00432592 | validation loss: 0.00593522\n",
      "train loss: 0.00704272 | validation loss: 0.0058993\n",
      "train loss: 0.00663741 | validation loss: 0.00587525\n",
      "Epoch 64/100\n",
      "train loss: 0.00451666 | validation loss: 0.00588857\n",
      "train loss: 0.00515892 | validation loss: 0.00585373\n",
      "train loss: 0.00382966 | validation loss: 0.00588567\n",
      "train loss: 0.00796689 | validation loss: 0.005874\n",
      "train loss: 0.00462838 | validation loss: 0.00589004\n",
      "train loss: 0.00663261 | validation loss: 0.00595667\n",
      "train loss: 0.00585917 | validation loss: 0.00586754\n",
      "Epoch 65/100\n",
      "train loss: 0.0067243 | validation loss: 0.00589094\n",
      "train loss: 0.00415045 | validation loss: 0.00585579\n",
      "train loss: 0.00533676 | validation loss: 0.00583827\n",
      "train loss: 0.009211 | validation loss: 0.00581046\n",
      "train loss: 0.00470667 | validation loss: 0.00586935\n",
      "train loss: 0.00552902 | validation loss: 0.00585488\n",
      "train loss: 0.00733778 | validation loss: 0.00584399\n",
      "Epoch 66/100\n",
      "train loss: 0.00600012 | validation loss: 0.00580067\n",
      "train loss: 0.00560312 | validation loss: 0.00580572\n",
      "train loss: 0.00535922 | validation loss: 0.00583397\n",
      "train loss: 0.0057363 | validation loss: 0.00580173\n",
      "train loss: 0.00570546 | validation loss: 0.00577049\n",
      "train loss: 0.0060858 | validation loss: 0.00582597\n",
      "train loss: 0.00663283 | validation loss: 0.00583948\n",
      "Epoch 67/100\n",
      "train loss: 0.00685426 | validation loss: 0.0058925\n",
      "train loss: 0.0051217 | validation loss: 0.00583417\n",
      "train loss: 0.00668643 | validation loss: 0.00581348\n",
      "train loss: 0.00667548 | validation loss: 0.00580066\n",
      "train loss: 0.00526294 | validation loss: 0.00578025\n",
      "train loss: 0.00495134 | validation loss: 0.00576304\n",
      "train loss: 0.00811548 | validation loss: 0.00577923\n",
      "Epoch 68/100\n",
      "train loss: 0.00446513 | validation loss: 0.00577256\n",
      "train loss: 0.00554234 | validation loss: 0.00576255\n",
      "train loss: 0.00726946 | validation loss: 0.0057937\n",
      "train loss: 0.00538052 | validation loss: 0.00581574\n",
      "train loss: 0.00494993 | validation loss: 0.00582524\n",
      "train loss: 0.00650473 | validation loss: 0.00579291\n",
      "train loss: 0.00502904 | validation loss: 0.0057471\n",
      "Epoch 69/100\n",
      "train loss: 0.00554639 | validation loss: 0.0057533\n",
      "train loss: 0.00436424 | validation loss: 0.00574948\n",
      "train loss: 0.00457918 | validation loss: 0.00575146\n",
      "train loss: 0.00411658 | validation loss: 0.00575533\n",
      "train loss: 0.00740331 | validation loss: 0.00584378\n",
      "train loss: 0.00448324 | validation loss: 0.00578119\n",
      "train loss: 0.00619121 | validation loss: 0.00578638\n",
      "Epoch 70/100\n",
      "train loss: 0.00334427 | validation loss: 0.00577786\n",
      "train loss: 0.00997861 | validation loss: 0.00577817\n",
      "train loss: 0.004122 | validation loss: 0.00581175\n",
      "train loss: 0.00780395 | validation loss: 0.00583149\n",
      "train loss: 0.00536488 | validation loss: 0.00579709\n",
      "train loss: 0.00681326 | validation loss: 0.00572416\n",
      "train loss: 0.00648241 | validation loss: 0.00575824\n",
      "Epoch 71/100\n",
      "train loss: 0.00741631 | validation loss: 0.00578869\n",
      "train loss: 0.00672443 | validation loss: 0.00576191\n",
      "train loss: 0.00444742 | validation loss: 0.00574523\n",
      "train loss: 0.0062679 | validation loss: 0.00570912\n",
      "train loss: 0.00373054 | validation loss: 0.00575783\n",
      "train loss: 0.00614349 | validation loss: 0.0057466\n",
      "train loss: 0.00579064 | validation loss: 0.00579404\n",
      "Epoch 72/100\n",
      "train loss: 0.00541374 | validation loss: 0.00574938\n",
      "train loss: 0.0070021 | validation loss: 0.00575225\n",
      "train loss: 0.00599334 | validation loss: 0.00577659\n",
      "train loss: 0.00485685 | validation loss: 0.00574384\n",
      "train loss: 0.0059587 | validation loss: 0.00574002\n",
      "train loss: 0.00757914 | validation loss: 0.00570521\n",
      "train loss: 0.00599236 | validation loss: 0.00573672\n",
      "Epoch 73/100\n",
      "train loss: 0.0059384 | validation loss: 0.00575601\n",
      "train loss: 0.00430172 | validation loss: 0.00568546\n",
      "train loss: 0.00499853 | validation loss: 0.00577785\n",
      "train loss: 0.00531681 | validation loss: 0.00575896\n",
      "train loss: 0.00469809 | validation loss: 0.00577269\n",
      "train loss: 0.00565291 | validation loss: 0.00571603\n",
      "train loss: 0.00658453 | validation loss: 0.00572997\n",
      "Epoch 74/100\n",
      "train loss: 0.00418917 | validation loss: 0.00580841\n",
      "train loss: 0.0079629 | validation loss: 0.00581249\n",
      "train loss: 0.00472898 | validation loss: 0.00576209\n",
      "train loss: 0.00473721 | validation loss: 0.00568606\n",
      "train loss: 0.00702587 | validation loss: 0.0056728\n",
      "train loss: 0.00859838 | validation loss: 0.00572008\n",
      "train loss: 0.00460881 | validation loss: 0.00568124\n",
      "Epoch 75/100\n",
      "train loss: 0.00623178 | validation loss: 0.00565915\n",
      "train loss: 0.0036064 | validation loss: 0.00566104\n",
      "train loss: 0.00583724 | validation loss: 0.00573048\n",
      "train loss: 0.00751537 | validation loss: 0.00574112\n",
      "train loss: 0.00572628 | validation loss: 0.0057407\n",
      "train loss: 0.00608917 | validation loss: 0.00569718\n",
      "train loss: 0.00454584 | validation loss: 0.00573566\n",
      "Epoch 76/100\n",
      "train loss: 0.00587504 | validation loss: 0.00573087\n",
      "train loss: 0.0041555 | validation loss: 0.0057117\n",
      "train loss: 0.00536373 | validation loss: 0.00573408\n",
      "train loss: 0.00362559 | validation loss: 0.00577894\n",
      "train loss: 0.00633443 | validation loss: 0.00566637\n",
      "train loss: 0.00468799 | validation loss: 0.00567276\n",
      "train loss: 0.00488237 | validation loss: 0.0057081\n",
      "Epoch 77/100\n",
      "train loss: 0.00503185 | validation loss: 0.0057174\n",
      "train loss: 0.00499108 | validation loss: 0.00572156\n",
      "train loss: 0.00611754 | validation loss: 0.00572365\n",
      "train loss: 0.00513311 | validation loss: 0.00566685\n",
      "train loss: 0.00598841 | validation loss: 0.00570504\n",
      "train loss: 0.00582317 | validation loss: 0.00568702\n",
      "train loss: 0.00629893 | validation loss: 0.00566449\n",
      "Epoch 78/100\n",
      "train loss: 0.00766628 | validation loss: 0.00572453\n",
      "train loss: 0.00536335 | validation loss: 0.00571541\n",
      "train loss: 0.00674242 | validation loss: 0.00569238\n",
      "train loss: 0.00651951 | validation loss: 0.00568237\n",
      "train loss: 0.00443555 | validation loss: 0.00567578\n",
      "train loss: 0.00515852 | validation loss: 0.00565915\n",
      "train loss: 0.00420049 | validation loss: 0.00570002\n",
      "Epoch 79/100\n",
      "train loss: 0.00517307 | validation loss: 0.00572553\n",
      "train loss: 0.00445841 | validation loss: 0.00576366\n",
      "train loss: 0.0065303 | validation loss: 0.00569466\n",
      "train loss: 0.00700535 | validation loss: 0.00574614\n",
      "train loss: 0.00520826 | validation loss: 0.0056735\n",
      "train loss: 0.00639462 | validation loss: 0.00565799\n",
      "train loss: 0.00458959 | validation loss: 0.00569071\n",
      "Epoch 80/100\n",
      "train loss: 0.00477517 | validation loss: 0.00565813\n",
      "train loss: 0.00433523 | validation loss: 0.005705\n",
      "train loss: 0.00503572 | validation loss: 0.00569382\n",
      "train loss: 0.0059054 | validation loss: 0.00570976\n",
      "train loss: 0.00471518 | validation loss: 0.00570991\n",
      "train loss: 0.00598928 | validation loss: 0.00579365\n",
      "train loss: 0.00549414 | validation loss: 0.00569702\n",
      "Epoch 81/100\n",
      "train loss: 0.00637943 | validation loss: 0.00570945\n",
      "train loss: 0.0053301 | validation loss: 0.00566842\n",
      "train loss: 0.00487315 | validation loss: 0.00575443\n",
      "train loss: 0.00596151 | validation loss: 0.00566159\n",
      "train loss: 0.0045601 | validation loss: 0.00563851\n",
      "train loss: 0.00559338 | validation loss: 0.00567443\n",
      "train loss: 0.00526046 | validation loss: 0.00570186\n",
      "Epoch 82/100\n",
      "train loss: 0.00455498 | validation loss: 0.00570418\n",
      "train loss: 0.00434815 | validation loss: 0.00568643\n",
      "train loss: 0.00628693 | validation loss: 0.00564165\n",
      "train loss: 0.00445587 | validation loss: 0.00572253\n",
      "train loss: 0.00486279 | validation loss: 0.00568186\n",
      "train loss: 0.00671872 | validation loss: 0.00567124\n",
      "train loss: 0.00465951 | validation loss: 0.00564105\n",
      "Epoch 83/100\n",
      "train loss: 0.00350746 | validation loss: 0.00571616\n",
      "train loss: 0.0086466 | validation loss: 0.00571645\n",
      "train loss: 0.00490372 | validation loss: 0.00563887\n",
      "train loss: 0.00513856 | validation loss: 0.00563493\n",
      "train loss: 0.00425411 | validation loss: 0.00567278\n",
      "train loss: 0.00433478 | validation loss: 0.00570587\n",
      "train loss: 0.00439325 | validation loss: 0.0056718\n",
      "Epoch 84/100\n",
      "train loss: 0.00444689 | validation loss: 0.00567651\n",
      "train loss: 0.0037864 | validation loss: 0.00565769\n",
      "train loss: 0.0049188 | validation loss: 0.0057729\n",
      "train loss: 0.0055855 | validation loss: 0.00566896\n",
      "train loss: 0.00555858 | validation loss: 0.00571255\n",
      "train loss: 0.00467564 | validation loss: 0.00563359\n",
      "train loss: 0.00633399 | validation loss: 0.00568278\n",
      "Epoch 85/100\n",
      "train loss: 0.00485412 | validation loss: 0.00562828\n",
      "train loss: 0.00510333 | validation loss: 0.00572717\n",
      "train loss: 0.005645 | validation loss: 0.00573283\n",
      "train loss: 0.01032947 | validation loss: 0.00572501\n",
      "train loss: 0.0050097 | validation loss: 0.00565532\n",
      "train loss: 0.00760175 | validation loss: 0.00565292\n",
      "train loss: 0.00514706 | validation loss: 0.00565402\n",
      "Epoch 86/100\n",
      "train loss: 0.00463997 | validation loss: 0.00569247\n",
      "train loss: 0.00477099 | validation loss: 0.00565343\n",
      "train loss: 0.00381168 | validation loss: 0.00566231\n",
      "train loss: 0.00727107 | validation loss: 0.00561485\n",
      "train loss: 0.00501116 | validation loss: 0.00570699\n",
      "train loss: 0.00619188 | validation loss: 0.00566331\n",
      "train loss: 0.00504133 | validation loss: 0.00562409\n",
      "Epoch 87/100\n",
      "train loss: 0.00718468 | validation loss: 0.00567708\n",
      "train loss: 0.0055153 | validation loss: 0.00569887\n",
      "train loss: 0.00592064 | validation loss: 0.00567066\n",
      "train loss: 0.00516939 | validation loss: 0.0056513\n",
      "train loss: 0.0071575 | validation loss: 0.00567429\n",
      "train loss: 0.00462015 | validation loss: 0.00563925\n",
      "train loss: 0.00411667 | validation loss: 0.00572742\n",
      "Epoch 88/100\n",
      "train loss: 0.0083258 | validation loss: 0.00564663\n",
      "train loss: 0.00595138 | validation loss: 0.00567572\n",
      "train loss: 0.0065936 | validation loss: 0.005713\n",
      "train loss: 0.00672788 | validation loss: 0.00567329\n",
      "train loss: 0.00583047 | validation loss: 0.0056748\n",
      "train loss: 0.00645481 | validation loss: 0.00563984\n",
      "train loss: 0.00606168 | validation loss: 0.00573904\n",
      "Epoch 89/100\n",
      "train loss: 0.00509203 | validation loss: 0.00566712\n",
      "train loss: 0.00671806 | validation loss: 0.00566967\n",
      "train loss: 0.00380887 | validation loss: 0.00575765\n",
      "train loss: 0.00511373 | validation loss: 0.00563459\n",
      "train loss: 0.00353426 | validation loss: 0.00569987\n",
      "train loss: 0.00588739 | validation loss: 0.00562243\n",
      "train loss: 0.00544886 | validation loss: 0.00571707\n",
      "Epoch 90/100\n",
      "train loss: 0.00574957 | validation loss: 0.00567517\n",
      "train loss: 0.00589232 | validation loss: 0.00567646\n",
      "train loss: 0.00949632 | validation loss: 0.00566039\n",
      "train loss: 0.00643771 | validation loss: 0.00559203\n",
      "train loss: 0.00516069 | validation loss: 0.00569527\n",
      "train loss: 0.00460296 | validation loss: 0.00566031\n",
      "train loss: 0.00501071 | validation loss: 0.00567884\n",
      "Epoch 91/100\n",
      "train loss: 0.00560891 | validation loss: 0.00572347\n",
      "train loss: 0.00415019 | validation loss: 0.00570522\n",
      "train loss: 0.00597167 | validation loss: 0.0056183\n",
      "train loss: 0.00671145 | validation loss: 0.00565732\n",
      "train loss: 0.00583385 | validation loss: 0.00568627\n",
      "train loss: 0.00544682 | validation loss: 0.00557938\n",
      "train loss: 0.00631074 | validation loss: 0.00566723\n",
      "Epoch 92/100\n",
      "train loss: 0.00425615 | validation loss: 0.00569121\n",
      "train loss: 0.00461354 | validation loss: 0.0056974\n",
      "train loss: 0.00570453 | validation loss: 0.00562848\n",
      "train loss: 0.00504107 | validation loss: 0.00567336\n",
      "train loss: 0.00552952 | validation loss: 0.00560057\n",
      "train loss: 0.00595232 | validation loss: 0.00558624\n",
      "train loss: 0.00477101 | validation loss: 0.0056568\n",
      "Epoch 93/100\n",
      "train loss: 0.00625761 | validation loss: 0.00567293\n",
      "train loss: 0.00353098 | validation loss: 0.00565541\n",
      "train loss: 0.00737026 | validation loss: 0.00565687\n",
      "train loss: 0.00542802 | validation loss: 0.00565412\n",
      "train loss: 0.00425575 | validation loss: 0.00564021\n",
      "train loss: 0.0046963 | validation loss: 0.00562927\n",
      "train loss: 0.00526882 | validation loss: 0.00565854\n",
      "Epoch 94/100\n",
      "train loss: 0.00543394 | validation loss: 0.00570611\n",
      "train loss: 0.00396376 | validation loss: 0.00571782\n",
      "train loss: 0.0047395 | validation loss: 0.00570735\n",
      "train loss: 0.00377521 | validation loss: 0.0056803\n",
      "train loss: 0.00465007 | validation loss: 0.00557434\n",
      "train loss: 0.0056851 | validation loss: 0.00566954\n",
      "train loss: 0.00413445 | validation loss: 0.00567696\n",
      "Epoch 95/100\n",
      "train loss: 0.00382038 | validation loss: 0.0057085\n",
      "train loss: 0.00414832 | validation loss: 0.00562938\n",
      "train loss: 0.00683384 | validation loss: 0.00566153\n",
      "train loss: 0.00592741 | validation loss: 0.00567285\n",
      "train loss: 0.00362281 | validation loss: 0.00570202\n",
      "train loss: 0.00406168 | validation loss: 0.00567472\n",
      "train loss: 0.00458216 | validation loss: 0.00571821\n",
      "Epoch 96/100\n",
      "train loss: 0.00466743 | validation loss: 0.00564945\n",
      "train loss: 0.00554938 | validation loss: 0.0056562\n",
      "train loss: 0.00609928 | validation loss: 0.00565199\n",
      "train loss: 0.00507188 | validation loss: 0.00560284\n",
      "train loss: 0.00791947 | validation loss: 0.00571713\n",
      "train loss: 0.00437422 | validation loss: 0.00567781\n",
      "train loss: 0.00521829 | validation loss: 0.00566868\n",
      "Epoch 97/100\n",
      "train loss: 0.00456743 | validation loss: 0.00568739\n",
      "train loss: 0.00408034 | validation loss: 0.00566509\n",
      "train loss: 0.00465773 | validation loss: 0.00569148\n",
      "train loss: 0.00440113 | validation loss: 0.00566356\n",
      "train loss: 0.00539531 | validation loss: 0.00568166\n",
      "train loss: 0.00545359 | validation loss: 0.00566972\n",
      "train loss: 0.00722335 | validation loss: 0.00566603\n",
      "Epoch 98/100\n",
      "train loss: 0.00493482 | validation loss: 0.00563801\n",
      "train loss: 0.00482779 | validation loss: 0.00563668\n",
      "train loss: 0.00587383 | validation loss: 0.00560788\n",
      "train loss: 0.0058768 | validation loss: 0.00559721\n",
      "train loss: 0.00524257 | validation loss: 0.00557211\n",
      "train loss: 0.00511433 | validation loss: 0.00571815\n",
      "train loss: 0.00797841 | validation loss: 0.005585\n",
      "Epoch 99/100\n",
      "train loss: 0.00481141 | validation loss: 0.00564103\n",
      "train loss: 0.00419985 | validation loss: 0.00561741\n",
      "train loss: 0.00433601 | validation loss: 0.00564073\n",
      "train loss: 0.00649856 | validation loss: 0.00572123\n",
      "train loss: 0.00686125 | validation loss: 0.00568323\n",
      "train loss: 0.00775137 | validation loss: 0.00568065\n",
      "train loss: 0.00683699 | validation loss: 0.00563485\n",
      "Epoch 100/100\n",
      "train loss: 0.0068168 | validation loss: 0.00566781\n",
      "train loss: 0.00514899 | validation loss: 0.00562493\n",
      "train loss: 0.0049261 | validation loss: 0.00562431\n",
      "train loss: 0.0043492 | validation loss: 0.0056347\n",
      "train loss: 0.00549772 | validation loss: 0.00566069\n",
      "train loss: 0.00554027 | validation loss: 0.00564863\n",
      "train loss: 0.00334332 | validation loss: 0.00576307\n"
     ]
    }
   ],
   "source": [
    "ae_1.train_loop(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "train loss: 0.00457439 | validation loss: 0.00565311\n",
      "train loss: 0.00525392 | validation loss: 0.00563012\n",
      "train loss: 0.00495279 | validation loss: 0.00561453\n",
      "train loss: 0.00434748 | validation loss: 0.00570048\n",
      "train loss: 0.00528799 | validation loss: 0.00559775\n",
      "train loss: 0.00446374 | validation loss: 0.00561408\n",
      "train loss: 0.00430169 | validation loss: 0.00567095\n",
      "Epoch 2/50\n",
      "train loss: 0.00708935 | validation loss: 0.00566531\n",
      "train loss: 0.00435743 | validation loss: 0.00563964\n",
      "train loss: 0.00402434 | validation loss: 0.00560948\n",
      "train loss: 0.00571084 | validation loss: 0.00566217\n",
      "train loss: 0.00448714 | validation loss: 0.00567988\n",
      "train loss: 0.00674863 | validation loss: 0.00566965\n",
      "train loss: 0.0067034 | validation loss: 0.00565428\n",
      "Epoch 3/50\n",
      "train loss: 0.00452256 | validation loss: 0.0056806\n",
      "train loss: 0.00348999 | validation loss: 0.00563541\n",
      "train loss: 0.00503562 | validation loss: 0.00567615\n",
      "train loss: 0.00630369 | validation loss: 0.00562941\n",
      "train loss: 0.00584341 | validation loss: 0.00563011\n",
      "train loss: 0.00409835 | validation loss: 0.00567535\n",
      "train loss: 0.00547477 | validation loss: 0.00564039\n",
      "Epoch 4/50\n",
      "train loss: 0.00726542 | validation loss: 0.00565064\n",
      "train loss: 0.00593984 | validation loss: 0.0056393\n",
      "train loss: 0.00577093 | validation loss: 0.00560494\n",
      "train loss: 0.00451969 | validation loss: 0.00558443\n",
      "train loss: 0.00625579 | validation loss: 0.00561444\n",
      "train loss: 0.00560598 | validation loss: 0.00565336\n",
      "train loss: 0.00524078 | validation loss: 0.00571363\n",
      "Epoch 5/50\n",
      "train loss: 0.00553271 | validation loss: 0.00564745\n",
      "train loss: 0.00526027 | validation loss: 0.00565153\n",
      "train loss: 0.00456273 | validation loss: 0.005637\n",
      "train loss: 0.00603717 | validation loss: 0.00562144\n",
      "train loss: 0.00453333 | validation loss: 0.00561171\n",
      "train loss: 0.0050391 | validation loss: 0.0056216\n",
      "train loss: 0.00473235 | validation loss: 0.00569606\n",
      "Epoch 6/50\n",
      "train loss: 0.00783833 | validation loss: 0.00559533\n",
      "train loss: 0.0044401 | validation loss: 0.00564992\n",
      "train loss: 0.00472848 | validation loss: 0.00568558\n",
      "train loss: 0.00566699 | validation loss: 0.00571255\n",
      "train loss: 0.00590853 | validation loss: 0.00561622\n",
      "train loss: 0.00727194 | validation loss: 0.00582557\n",
      "train loss: 0.00555602 | validation loss: 0.00565615\n",
      "Epoch 7/50\n",
      "train loss: 0.00575915 | validation loss: 0.00563895\n",
      "train loss: 0.00381574 | validation loss: 0.00564366\n",
      "train loss: 0.00490933 | validation loss: 0.00569353\n",
      "train loss: 0.00601831 | validation loss: 0.00566178\n",
      "train loss: 0.00561999 | validation loss: 0.00561511\n",
      "train loss: 0.00568935 | validation loss: 0.00556331\n",
      "train loss: 0.00486305 | validation loss: 0.00559684\n",
      "Epoch 8/50\n",
      "train loss: 0.00480009 | validation loss: 0.00565206\n",
      "train loss: 0.00677614 | validation loss: 0.00565844\n",
      "train loss: 0.00507806 | validation loss: 0.00565608\n",
      "train loss: 0.00456259 | validation loss: 0.00569129\n",
      "train loss: 0.00476977 | validation loss: 0.00568595\n",
      "train loss: 0.00821225 | validation loss: 0.00567902\n",
      "train loss: 0.00558499 | validation loss: 0.00561708\n",
      "Epoch 9/50\n",
      "train loss: 0.00521821 | validation loss: 0.00572156\n",
      "train loss: 0.00692826 | validation loss: 0.00561717\n",
      "train loss: 0.00617438 | validation loss: 0.00564344\n",
      "train loss: 0.00503786 | validation loss: 0.00565689\n",
      "train loss: 0.00515747 | validation loss: 0.0056088\n",
      "train loss: 0.00458777 | validation loss: 0.00571787\n",
      "train loss: 0.00430225 | validation loss: 0.00567258\n",
      "Epoch 10/50\n",
      "train loss: 0.00467203 | validation loss: 0.00564687\n",
      "train loss: 0.00410956 | validation loss: 0.00562048\n",
      "train loss: 0.00485502 | validation loss: 0.005652\n",
      "train loss: 0.00419587 | validation loss: 0.00569009\n",
      "train loss: 0.00364047 | validation loss: 0.00568162\n",
      "train loss: 0.00637941 | validation loss: 0.00562984\n",
      "train loss: 0.00547902 | validation loss: 0.00561108\n",
      "Epoch 11/50\n",
      "train loss: 0.00823382 | validation loss: 0.00560766\n",
      "train loss: 0.00445489 | validation loss: 0.00561281\n",
      "train loss: 0.00569152 | validation loss: 0.00556893\n",
      "train loss: 0.00565026 | validation loss: 0.00562168\n",
      "train loss: 0.00619939 | validation loss: 0.00565524\n",
      "train loss: 0.00504187 | validation loss: 0.00561712\n",
      "train loss: 0.01143281 | validation loss: 0.00572845\n",
      "Epoch 12/50\n",
      "train loss: 0.00523637 | validation loss: 0.00564704\n",
      "train loss: 0.00911116 | validation loss: 0.00565761\n",
      "train loss: 0.00727238 | validation loss: 0.00562976\n",
      "train loss: 0.00764094 | validation loss: 0.0056873\n",
      "train loss: 0.00434352 | validation loss: 0.00574099\n",
      "train loss: 0.00402233 | validation loss: 0.00569084\n",
      "train loss: 0.00443759 | validation loss: 0.005648\n",
      "Epoch 13/50\n",
      "train loss: 0.00428645 | validation loss: 0.00561889\n",
      "train loss: 0.00390399 | validation loss: 0.00565438\n",
      "train loss: 0.00523358 | validation loss: 0.00564364\n",
      "train loss: 0.00503865 | validation loss: 0.00566197\n",
      "train loss: 0.00401016 | validation loss: 0.0056829\n",
      "train loss: 0.00392011 | validation loss: 0.005732\n",
      "train loss: 0.00485005 | validation loss: 0.00574983\n",
      "Epoch 14/50\n",
      "train loss: 0.00632171 | validation loss: 0.0056469\n",
      "train loss: 0.00439601 | validation loss: 0.0057068\n",
      "train loss: 0.00457143 | validation loss: 0.00565508\n",
      "train loss: 0.0040212 | validation loss: 0.00569678\n",
      "train loss: 0.00493121 | validation loss: 0.00565144\n",
      "train loss: 0.00519591 | validation loss: 0.00572433\n",
      "train loss: 0.00705668 | validation loss: 0.00565768\n",
      "Epoch 15/50\n",
      "train loss: 0.00448347 | validation loss: 0.00563384\n",
      "train loss: 0.00388453 | validation loss: 0.00559184\n",
      "train loss: 0.004568 | validation loss: 0.00568\n",
      "train loss: 0.00677839 | validation loss: 0.00567935\n",
      "train loss: 0.00326433 | validation loss: 0.00562778\n",
      "train loss: 0.00517341 | validation loss: 0.0056432\n",
      "train loss: 0.00527759 | validation loss: 0.00558174\n",
      "Epoch 16/50\n",
      "train loss: 0.00644849 | validation loss: 0.00570308\n",
      "train loss: 0.00508832 | validation loss: 0.00569913\n",
      "train loss: 0.00579364 | validation loss: 0.00570595\n",
      "train loss: 0.00485738 | validation loss: 0.00571498\n",
      "train loss: 0.00499888 | validation loss: 0.00560401\n",
      "train loss: 0.00481267 | validation loss: 0.00567064\n",
      "train loss: 0.00604313 | validation loss: 0.00563211\n",
      "Epoch 17/50\n",
      "train loss: 0.00712526 | validation loss: 0.00571038\n",
      "train loss: 0.00400035 | validation loss: 0.00567704\n",
      "train loss: 0.00432596 | validation loss: 0.00566864\n",
      "train loss: 0.00435562 | validation loss: 0.00566857\n",
      "train loss: 0.00459342 | validation loss: 0.00565869\n",
      "train loss: 0.00498639 | validation loss: 0.00563137\n",
      "train loss: 0.00938129 | validation loss: 0.00563435\n",
      "Epoch 18/50\n",
      "train loss: 0.00495296 | validation loss: 0.00560383\n",
      "train loss: 0.00441532 | validation loss: 0.00560906\n",
      "train loss: 0.00387936 | validation loss: 0.00562459\n",
      "train loss: 0.00792368 | validation loss: 0.00560656\n",
      "train loss: 0.00520671 | validation loss: 0.00575506\n",
      "train loss: 0.00618443 | validation loss: 0.00564668\n",
      "train loss: 0.00489802 | validation loss: 0.00563381\n",
      "Epoch 19/50\n",
      "train loss: 0.0060705 | validation loss: 0.00565185\n",
      "train loss: 0.00528365 | validation loss: 0.00565959\n",
      "train loss: 0.00610746 | validation loss: 0.0056343\n",
      "train loss: 0.00531713 | validation loss: 0.0056956\n",
      "train loss: 0.00525951 | validation loss: 0.00559414\n",
      "train loss: 0.00632077 | validation loss: 0.0056811\n",
      "train loss: 0.00669336 | validation loss: 0.00563765\n",
      "Epoch 20/50\n",
      "train loss: 0.00679355 | validation loss: 0.00561647\n",
      "train loss: 0.00481337 | validation loss: 0.00567199\n",
      "train loss: 0.00549944 | validation loss: 0.0056753\n",
      "train loss: 0.00397988 | validation loss: 0.00565266\n",
      "train loss: 0.00420423 | validation loss: 0.00563689\n",
      "train loss: 0.00402743 | validation loss: 0.00559503\n",
      "train loss: 0.00463359 | validation loss: 0.00566894\n",
      "Epoch 21/50\n",
      "train loss: 0.00611628 | validation loss: 0.00573217\n",
      "train loss: 0.00647134 | validation loss: 0.00566894\n",
      "train loss: 0.00561056 | validation loss: 0.00568742\n",
      "train loss: 0.00395253 | validation loss: 0.0056271\n",
      "train loss: 0.00552927 | validation loss: 0.00567591\n",
      "train loss: 0.00332345 | validation loss: 0.0057143\n",
      "train loss: 0.00766472 | validation loss: 0.00573641\n",
      "Epoch 22/50\n",
      "train loss: 0.00357978 | validation loss: 0.00572831\n",
      "train loss: 0.0044992 | validation loss: 0.00574049\n",
      "train loss: 0.00441074 | validation loss: 0.00568652\n",
      "train loss: 0.00551945 | validation loss: 0.00564664\n",
      "train loss: 0.00536801 | validation loss: 0.00564297\n",
      "train loss: 0.00440418 | validation loss: 0.00559378\n",
      "train loss: 0.00508579 | validation loss: 0.00561033\n",
      "Epoch 23/50\n",
      "train loss: 0.00494701 | validation loss: 0.00564988\n",
      "train loss: 0.00484373 | validation loss: 0.00564444\n",
      "train loss: 0.00377822 | validation loss: 0.00572672\n",
      "train loss: 0.00476756 | validation loss: 0.00565884\n",
      "train loss: 0.00643442 | validation loss: 0.0056723\n",
      "train loss: 0.0046119 | validation loss: 0.00561636\n",
      "train loss: 0.00506462 | validation loss: 0.0056189\n",
      "Epoch 24/50\n",
      "train loss: 0.00473859 | validation loss: 0.00563228\n",
      "train loss: 0.00409835 | validation loss: 0.00564514\n",
      "train loss: 0.00444217 | validation loss: 0.00572396\n",
      "train loss: 0.00513731 | validation loss: 0.00566224\n",
      "train loss: 0.00427114 | validation loss: 0.00562826\n",
      "train loss: 0.00447389 | validation loss: 0.00561126\n",
      "train loss: 0.00444595 | validation loss: 0.00576371\n",
      "Epoch 25/50\n",
      "train loss: 0.00616193 | validation loss: 0.00565759\n",
      "train loss: 0.00501063 | validation loss: 0.00580301\n",
      "train loss: 0.00569759 | validation loss: 0.00562194\n",
      "train loss: 0.00375613 | validation loss: 0.00564066\n",
      "train loss: 0.00499896 | validation loss: 0.00567242\n",
      "train loss: 0.00456011 | validation loss: 0.0057075\n",
      "train loss: 0.00614173 | validation loss: 0.00567116\n",
      "Epoch 26/50\n",
      "train loss: 0.00410048 | validation loss: 0.00572104\n",
      "train loss: 0.00559006 | validation loss: 0.0056794\n",
      "train loss: 0.00458057 | validation loss: 0.00564618\n",
      "train loss: 0.00620251 | validation loss: 0.00564303\n",
      "train loss: 0.00612648 | validation loss: 0.00567704\n",
      "train loss: 0.00483641 | validation loss: 0.00571709\n",
      "train loss: 0.00690882 | validation loss: 0.00563183\n",
      "Epoch 27/50\n",
      "train loss: 0.00517153 | validation loss: 0.0056201\n",
      "train loss: 0.00512725 | validation loss: 0.00569729\n",
      "train loss: 0.00481193 | validation loss: 0.00565874\n",
      "train loss: 0.00626914 | validation loss: 0.00571498\n",
      "train loss: 0.00522605 | validation loss: 0.00564639\n",
      "train loss: 0.00527086 | validation loss: 0.00563293\n",
      "train loss: 0.00508682 | validation loss: 0.00564931\n",
      "Epoch 28/50\n",
      "train loss: 0.00603522 | validation loss: 0.00579031\n",
      "train loss: 0.0042838 | validation loss: 0.00567454\n",
      "train loss: 0.00420505 | validation loss: 0.00567635\n",
      "train loss: 0.00543694 | validation loss: 0.00565323\n",
      "train loss: 0.00436033 | validation loss: 0.00564859\n",
      "train loss: 0.00640188 | validation loss: 0.00568763\n",
      "train loss: 0.00773983 | validation loss: 0.00562631\n",
      "Epoch 29/50\n",
      "train loss: 0.00454251 | validation loss: 0.0056713\n",
      "train loss: 0.00600243 | validation loss: 0.00570949\n",
      "train loss: 0.00416124 | validation loss: 0.00567393\n",
      "train loss: 0.00537116 | validation loss: 0.00567274\n",
      "train loss: 0.0049918 | validation loss: 0.00573635\n",
      "train loss: 0.0091076 | validation loss: 0.00569738\n",
      "train loss: 0.00419784 | validation loss: 0.00568971\n",
      "Epoch 30/50\n",
      "train loss: 0.00537533 | validation loss: 0.00572558\n",
      "train loss: 0.00381592 | validation loss: 0.00567828\n",
      "train loss: 0.0052451 | validation loss: 0.00567564\n",
      "train loss: 0.00628252 | validation loss: 0.00574993\n",
      "train loss: 0.00584153 | validation loss: 0.00566644\n",
      "train loss: 0.00730347 | validation loss: 0.00570645\n",
      "train loss: 0.00530205 | validation loss: 0.00564503\n",
      "Epoch 31/50\n",
      "train loss: 0.00431074 | validation loss: 0.0056941\n",
      "train loss: 0.0072045 | validation loss: 0.00573148\n",
      "train loss: 0.00444441 | validation loss: 0.00565436\n",
      "train loss: 0.00628376 | validation loss: 0.0056623\n",
      "train loss: 0.00404552 | validation loss: 0.00565641\n",
      "train loss: 0.00586065 | validation loss: 0.00566041\n",
      "train loss: 0.00477484 | validation loss: 0.00571135\n",
      "Epoch 32/50\n",
      "train loss: 0.00538551 | validation loss: 0.0056971\n",
      "train loss: 0.0047995 | validation loss: 0.00578078\n",
      "train loss: 0.00363426 | validation loss: 0.00567425\n",
      "train loss: 0.00612092 | validation loss: 0.00570657\n",
      "train loss: 0.00512581 | validation loss: 0.00570368\n",
      "train loss: 0.00539252 | validation loss: 0.00566395\n",
      "train loss: 0.00515844 | validation loss: 0.00566219\n",
      "Epoch 33/50\n",
      "train loss: 0.00479855 | validation loss: 0.00571598\n",
      "train loss: 0.00382318 | validation loss: 0.00567673\n",
      "train loss: 0.00509011 | validation loss: 0.00574725\n",
      "train loss: 0.00461255 | validation loss: 0.00569332\n",
      "train loss: 0.00539739 | validation loss: 0.00569114\n",
      "train loss: 0.00553582 | validation loss: 0.0056898\n",
      "train loss: 0.00621651 | validation loss: 0.00568187\n",
      "Epoch 34/50\n",
      "train loss: 0.00514331 | validation loss: 0.00567828\n",
      "train loss: 0.00679449 | validation loss: 0.0057428\n",
      "train loss: 0.00511683 | validation loss: 0.00569333\n",
      "train loss: 0.00619522 | validation loss: 0.00566936\n",
      "train loss: 0.00391091 | validation loss: 0.00569665\n",
      "train loss: 0.00386703 | validation loss: 0.0056731\n",
      "train loss: 0.00419802 | validation loss: 0.00563158\n",
      "Epoch 35/50\n",
      "train loss: 0.00545552 | validation loss: 0.00570031\n",
      "train loss: 0.00549843 | validation loss: 0.00564647\n",
      "train loss: 0.00444535 | validation loss: 0.00567772\n",
      "train loss: 0.00630182 | validation loss: 0.00570219\n",
      "train loss: 0.00376782 | validation loss: 0.0057412\n",
      "train loss: 0.00505932 | validation loss: 0.00573233\n",
      "train loss: 0.00539706 | validation loss: 0.0056937\n",
      "Epoch 36/50\n",
      "train loss: 0.00472774 | validation loss: 0.00577023\n",
      "train loss: 0.00628332 | validation loss: 0.00576916\n",
      "train loss: 0.00446583 | validation loss: 0.00583309\n",
      "train loss: 0.00492797 | validation loss: 0.00567954\n",
      "train loss: 0.00486033 | validation loss: 0.00574724\n",
      "train loss: 0.00357262 | validation loss: 0.00578568\n",
      "train loss: 0.00454656 | validation loss: 0.00571225\n",
      "Epoch 37/50\n",
      "train loss: 0.00488235 | validation loss: 0.00570626\n",
      "train loss: 0.0039926 | validation loss: 0.00572571\n",
      "train loss: 0.00502582 | validation loss: 0.00571941\n",
      "train loss: 0.00530472 | validation loss: 0.00578156\n",
      "train loss: 0.00404355 | validation loss: 0.00572021\n",
      "train loss: 0.00601066 | validation loss: 0.00569952\n",
      "train loss: 0.00581374 | validation loss: 0.00571589\n",
      "Epoch 38/50\n",
      "train loss: 0.00469373 | validation loss: 0.0057445\n",
      "train loss: 0.00454855 | validation loss: 0.00569758\n",
      "train loss: 0.00386056 | validation loss: 0.00581631\n",
      "train loss: 0.00507767 | validation loss: 0.00567325\n",
      "train loss: 0.00509062 | validation loss: 0.00572807\n",
      "train loss: 0.00536104 | validation loss: 0.00568664\n",
      "train loss: 0.00528633 | validation loss: 0.00562855\n",
      "Epoch 39/50\n",
      "train loss: 0.00545824 | validation loss: 0.00571092\n",
      "train loss: 0.00400061 | validation loss: 0.005754\n",
      "train loss: 0.00544654 | validation loss: 0.00584874\n",
      "train loss: 0.00455715 | validation loss: 0.00569031\n",
      "train loss: 0.00405859 | validation loss: 0.00573021\n",
      "train loss: 0.00514593 | validation loss: 0.00569013\n",
      "train loss: 0.00402426 | validation loss: 0.00569684\n",
      "Epoch 40/50\n",
      "train loss: 0.00580036 | validation loss: 0.00570876\n",
      "train loss: 0.00581286 | validation loss: 0.00572324\n",
      "train loss: 0.00601973 | validation loss: 0.005788\n",
      "train loss: 0.00495934 | validation loss: 0.00571795\n",
      "train loss: 0.00514256 | validation loss: 0.00572591\n",
      "train loss: 0.00561239 | validation loss: 0.00567792\n",
      "train loss: 0.00392692 | validation loss: 0.00574254\n",
      "Epoch 41/50\n",
      "train loss: 0.00556583 | validation loss: 0.00574487\n",
      "train loss: 0.00403627 | validation loss: 0.00579018\n",
      "train loss: 0.00524973 | validation loss: 0.00569432\n",
      "train loss: 0.005334 | validation loss: 0.00567308\n",
      "train loss: 0.00572245 | validation loss: 0.00583458\n",
      "train loss: 0.00469184 | validation loss: 0.00565843\n",
      "train loss: 0.00422404 | validation loss: 0.00564244\n",
      "Epoch 42/50\n",
      "train loss: 0.00353814 | validation loss: 0.00578166\n",
      "train loss: 0.00533949 | validation loss: 0.00582363\n",
      "train loss: 0.00558869 | validation loss: 0.00571616\n",
      "train loss: 0.00472921 | validation loss: 0.00566416\n",
      "train loss: 0.00441601 | validation loss: 0.00570914\n",
      "train loss: 0.00395094 | validation loss: 0.00577056\n",
      "train loss: 0.0045268 | validation loss: 0.00570951\n",
      "Epoch 43/50\n",
      "train loss: 0.00436101 | validation loss: 0.00575088\n",
      "train loss: 0.00813639 | validation loss: 0.00584943\n",
      "train loss: 0.00614058 | validation loss: 0.00577082\n",
      "train loss: 0.00633558 | validation loss: 0.00570803\n",
      "train loss: 0.00493295 | validation loss: 0.00573982\n",
      "train loss: 0.00481925 | validation loss: 0.00569727\n",
      "train loss: 0.00663078 | validation loss: 0.00566194\n",
      "Epoch 44/50\n",
      "train loss: 0.00420526 | validation loss: 0.00570794\n",
      "train loss: 0.00523864 | validation loss: 0.00570948\n",
      "train loss: 0.00530228 | validation loss: 0.0057547\n",
      "train loss: 0.00391632 | validation loss: 0.00571106\n",
      "train loss: 0.00548642 | validation loss: 0.00577454\n",
      "train loss: 0.00399184 | validation loss: 0.00584603\n",
      "train loss: 0.00496208 | validation loss: 0.0056926\n",
      "Epoch 45/50\n",
      "train loss: 0.00531012 | validation loss: 0.00571915\n",
      "train loss: 0.0042714 | validation loss: 0.00574014\n",
      "train loss: 0.00479242 | validation loss: 0.00570113\n",
      "train loss: 0.00473168 | validation loss: 0.00567244\n",
      "train loss: 0.00478877 | validation loss: 0.00570683\n",
      "train loss: 0.00634367 | validation loss: 0.00570874\n",
      "train loss: 0.00460599 | validation loss: 0.00577957\n",
      "Epoch 46/50\n",
      "train loss: 0.00430488 | validation loss: 0.00572903\n",
      "train loss: 0.00631707 | validation loss: 0.00573365\n",
      "train loss: 0.00382751 | validation loss: 0.00582131\n",
      "train loss: 0.00487646 | validation loss: 0.00570355\n",
      "train loss: 0.00494003 | validation loss: 0.00577633\n",
      "train loss: 0.00686045 | validation loss: 0.00572255\n",
      "train loss: 0.00359708 | validation loss: 0.00581234\n",
      "Epoch 47/50\n",
      "train loss: 0.00556195 | validation loss: 0.00575638\n",
      "train loss: 0.00414498 | validation loss: 0.0057423\n",
      "train loss: 0.00620718 | validation loss: 0.00578119\n",
      "train loss: 0.00508422 | validation loss: 0.00573002\n",
      "train loss: 0.00596569 | validation loss: 0.00572323\n",
      "train loss: 0.00443399 | validation loss: 0.00578623\n",
      "train loss: 0.00446612 | validation loss: 0.00580812\n",
      "Epoch 48/50\n",
      "train loss: 0.00490332 | validation loss: 0.00574384\n",
      "train loss: 0.00488738 | validation loss: 0.00577928\n",
      "train loss: 0.00404461 | validation loss: 0.00576763\n",
      "train loss: 0.00575513 | validation loss: 0.00573293\n",
      "train loss: 0.00510528 | validation loss: 0.00573441\n",
      "train loss: 0.00577406 | validation loss: 0.00573282\n",
      "train loss: 0.00371914 | validation loss: 0.00574002\n",
      "Epoch 49/50\n",
      "train loss: 0.00509906 | validation loss: 0.0057509\n",
      "train loss: 0.00446638 | validation loss: 0.00577081\n",
      "train loss: 0.00441759 | validation loss: 0.00575564\n",
      "train loss: 0.00378187 | validation loss: 0.00573722\n",
      "train loss: 0.00618206 | validation loss: 0.00575718\n",
      "train loss: 0.00565803 | validation loss: 0.00573407\n",
      "train loss: 0.00504895 | validation loss: 0.00575816\n",
      "Epoch 50/50\n",
      "train loss: 0.00518204 | validation loss: 0.00585665\n",
      "train loss: 0.00467264 | validation loss: 0.00584182\n",
      "train loss: 0.00656154 | validation loss: 0.00575848\n",
      "train loss: 0.00504773 | validation loss: 0.00573196\n",
      "train loss: 0.00500559 | validation loss: 0.00578131\n",
      "train loss: 0.00519958 | validation loss: 0.00573655\n",
      "train loss: 0.00455629 | validation loss: 0.00581802\n"
     ]
    }
   ],
   "source": [
    "ae_1.train_loop(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1 = pd.DataFrame(data=list(zip(ae_1.train_losses, ae_1.val_losses)), columns=['train_loss', 'validation_loss'])\n",
    "losses_1['epoch'] = (losses_1.index + 1) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1f5d40dd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 277.314375 \n",
       "L 392.14375 277.314375 \n",
       "L 392.14375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m3f0b958506\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"64.926505\" xlink:href=\"#m3f0b958506\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(61.745255 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.24059\" xlink:href=\"#m3f0b958506\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 50 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(123.87809 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.554675\" xlink:href=\"#m3f0b958506\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 100 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(186.010925 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"260.86876\" xlink:href=\"#m3f0b958506\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(251.32501 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"326.182845\" xlink:href=\"#m3f0b958506\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 200 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(316.639095 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <defs>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(202.315625 268.034687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"mfb859f30b1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mfb859f30b1\" y=\"232.726961\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.00 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 236.52618)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mfb859f30b1\" y=\"190.067705\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 193.866924)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mfb859f30b1\" y=\"147.408449\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 151.207668)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mfb859f30b1\" y=\"104.749194\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 108.548413)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mfb859f30b1\" y=\"62.089938\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 65.889157)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- MSE loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 24.515625 72.90625 \n",
       "L 43.109375 23.296875 \n",
       "L 61.8125 72.90625 \n",
       "L 76.515625 72.90625 \n",
       "L 76.515625 0 \n",
       "L 66.890625 0 \n",
       "L 66.890625 64.015625 \n",
       "L 48.09375 14.015625 \n",
       "L 38.1875 14.015625 \n",
       "L 19.390625 64.015625 \n",
       "L 19.390625 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-77\"/>\n",
       "      <path d=\"M 53.515625 70.515625 \n",
       "L 53.515625 60.890625 \n",
       "Q 47.90625 63.578125 42.921875 64.890625 \n",
       "Q 37.9375 66.21875 33.296875 66.21875 \n",
       "Q 25.25 66.21875 20.875 63.09375 \n",
       "Q 16.5 59.96875 16.5 54.203125 \n",
       "Q 16.5 49.359375 19.40625 46.890625 \n",
       "Q 22.3125 44.4375 30.421875 42.921875 \n",
       "L 36.375 41.703125 \n",
       "Q 47.40625 39.59375 52.65625 34.296875 \n",
       "Q 57.90625 29 57.90625 20.125 \n",
       "Q 57.90625 9.515625 50.796875 4.046875 \n",
       "Q 43.703125 -1.421875 29.984375 -1.421875 \n",
       "Q 24.8125 -1.421875 18.96875 -0.25 \n",
       "Q 13.140625 0.921875 6.890625 3.21875 \n",
       "L 6.890625 13.375 \n",
       "Q 12.890625 10.015625 18.65625 8.296875 \n",
       "Q 24.421875 6.59375 29.984375 6.59375 \n",
       "Q 38.421875 6.59375 43.015625 9.90625 \n",
       "Q 47.609375 13.234375 47.609375 19.390625 \n",
       "Q 47.609375 24.75 44.3125 27.78125 \n",
       "Q 41.015625 30.8125 33.5 32.328125 \n",
       "L 27.484375 33.5 \n",
       "Q 16.453125 35.6875 11.515625 40.375 \n",
       "Q 6.59375 45.0625 6.59375 53.421875 \n",
       "Q 6.59375 63.09375 13.40625 68.65625 \n",
       "Q 20.21875 74.21875 32.171875 74.21875 \n",
       "Q 37.3125 74.21875 42.625 73.28125 \n",
       "Q 47.953125 72.359375 53.515625 70.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-83\"/>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 55.90625 72.90625 \n",
       "L 55.90625 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.015625 \n",
       "L 54.390625 43.015625 \n",
       "L 54.390625 34.71875 \n",
       "L 19.671875 34.71875 \n",
       "L 19.671875 8.296875 \n",
       "L 56.78125 8.296875 \n",
       "L 56.78125 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-69\"/>\n",
       "      <path id=\"DejaVuSans-32\"/>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 152.932656)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n",
       "      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n",
       "      <use x=\"212.939453\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"244.726562\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"272.509766\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"333.691406\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"385.791016\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_11\">\n",
       "    <path clip-path=\"url(#p73f0f55659)\" d=\"M 65.361932 39.507596 \n",
       "L 65.797359 44.468333 \n",
       "L 66.232786 50.937139 \n",
       "L 66.668214 55.271306 \n",
       "L 67.103641 55.235365 \n",
       "L 67.539068 57.20164 \n",
       "L 68.409922 75.492733 \n",
       "L 68.84535 75.975717 \n",
       "L 69.280777 73.849812 \n",
       "L 70.587059 84.702216 \n",
       "L 71.022486 93.837401 \n",
       "L 71.457913 96.246104 \n",
       "L 71.89334 100.000906 \n",
       "L 72.328768 101.177053 \n",
       "L 72.764195 107.401875 \n",
       "L 73.199622 108.098229 \n",
       "L 73.635049 107.341664 \n",
       "L 74.070476 107.202478 \n",
       "L 74.505904 113.709284 \n",
       "L 74.941331 109.945582 \n",
       "L 75.376758 114.850405 \n",
       "L 75.812185 115.983288 \n",
       "L 76.247613 126.288282 \n",
       "L 76.68304 129.607992 \n",
       "L 77.118467 129.806983 \n",
       "L 77.553894 121.000078 \n",
       "L 77.989322 137.217405 \n",
       "L 78.424749 136.970001 \n",
       "L 78.860176 136.334939 \n",
       "L 79.295603 136.929973 \n",
       "L 79.731031 136.936851 \n",
       "L 80.166458 141.600188 \n",
       "L 80.601885 144.939109 \n",
       "L 81.037312 146.796467 \n",
       "L 81.472739 150.201663 \n",
       "L 81.908167 146.178187 \n",
       "L 82.343594 154.442863 \n",
       "L 82.779021 142.869442 \n",
       "L 83.214448 155.068816 \n",
       "L 83.649876 161.51702 \n",
       "L 84.085303 158.670829 \n",
       "L 84.52073 163.733775 \n",
       "L 85.391585 160.721304 \n",
       "L 85.827012 157.10394 \n",
       "L 86.262439 164.264422 \n",
       "L 86.697866 166.938213 \n",
       "L 87.133294 167.563257 \n",
       "L 88.439575 171.232587 \n",
       "L 88.875002 174.953407 \n",
       "L 89.31043 165.773705 \n",
       "L 89.745857 176.45899 \n",
       "L 90.181284 179.432781 \n",
       "L 90.616711 175.630868 \n",
       "L 91.052139 176.219653 \n",
       "L 91.487566 177.532323 \n",
       "L 91.922993 166.457803 \n",
       "L 92.35842 170.933713 \n",
       "L 92.793848 180.367559 \n",
       "L 93.229275 181.454378 \n",
       "L 93.664702 174.982546 \n",
       "L 94.100129 171.565266 \n",
       "L 94.535556 182.080448 \n",
       "L 94.970984 184.314567 \n",
       "L 95.406411 184.928096 \n",
       "L 95.841838 187.804606 \n",
       "L 96.277265 187.965199 \n",
       "L 96.712693 189.407447 \n",
       "L 97.14812 185.772825 \n",
       "L 98.018974 190.585479 \n",
       "L 98.454402 182.280682 \n",
       "L 98.889829 192.783818 \n",
       "L 99.325256 189.350021 \n",
       "L 99.760683 189.514104 \n",
       "L 100.196111 183.386943 \n",
       "L 100.631538 190.620403 \n",
       "L 101.066965 190.57622 \n",
       "L 101.502392 193.22344 \n",
       "L 101.937819 193.397017 \n",
       "L 102.373247 190.483695 \n",
       "L 102.808674 194.829168 \n",
       "L 103.244101 193.959654 \n",
       "L 103.679528 187.649425 \n",
       "L 104.114956 184.635508 \n",
       "L 104.550383 187.828911 \n",
       "L 104.98581 184.584721 \n",
       "L 105.421237 195.809384 \n",
       "L 105.856665 196.855691 \n",
       "L 106.292092 187.609194 \n",
       "L 106.727519 190.487706 \n",
       "L 107.162946 197.21593 \n",
       "L 107.598373 176.016969 \n",
       "L 108.033801 198.879437 \n",
       "L 108.469228 190.383666 \n",
       "L 108.904655 202.711306 \n",
       "L 109.340082 199.285413 \n",
       "L 109.77551 200.457726 \n",
       "L 110.210937 200.808236 \n",
       "L 110.646364 202.669043 \n",
       "L 111.081791 191.974811 \n",
       "L 111.517219 192.030238 \n",
       "L 111.952646 201.807278 \n",
       "L 112.388073 199.452035 \n",
       "L 112.8235 202.415524 \n",
       "L 113.258928 201.631295 \n",
       "L 113.694355 202.148494 \n",
       "L 114.129782 204.143091 \n",
       "L 114.565209 204.091017 \n",
       "L 115.000636 205.344776 \n",
       "L 115.436064 200.052158 \n",
       "L 115.871491 202.378792 \n",
       "L 116.306918 203.786298 \n",
       "L 116.742345 192.123962 \n",
       "L 117.177773 197.030594 \n",
       "L 117.6132 193.673916 \n",
       "L 118.048627 203.568993 \n",
       "L 118.484054 196.808757 \n",
       "L 118.919482 203.799342 \n",
       "L 119.354909 204.824945 \n",
       "L 119.790336 203.514166 \n",
       "L 120.66119 204.596164 \n",
       "L 121.096618 206.392388 \n",
       "L 121.532045 207.152231 \n",
       "L 121.967472 205.624968 \n",
       "L 122.402899 199.019295 \n",
       "L 122.838327 207.154013 \n",
       "L 123.273754 205.594643 \n",
       "L 123.709181 206.685244 \n",
       "L 124.144608 206.186047 \n",
       "L 124.580036 208.746016 \n",
       "L 125.015463 208.610518 \n",
       "L 125.45089 208.689255 \n",
       "L 125.886317 206.588657 \n",
       "L 126.757172 208.279409 \n",
       "L 127.192599 207.51824 \n",
       "L 127.628026 207.876732 \n",
       "L 128.063453 210.161566 \n",
       "L 128.498881 209.001034 \n",
       "L 128.934308 210.272137 \n",
       "L 129.369735 194.88464 \n",
       "L 129.805162 208.498072 \n",
       "L 130.24059 196.671845 \n",
       "L 130.676017 205.473932 \n",
       "L 131.111444 210.439441 \n",
       "L 131.546871 196.667459 \n",
       "L 131.982299 210.748178 \n",
       "L 132.417726 209.727685 \n",
       "L 132.853153 199.696442 \n",
       "L 133.28858 209.590391 \n",
       "L 133.724008 200.091563 \n",
       "L 134.159435 208.914333 \n",
       "L 134.594862 209.479458 \n",
       "L 135.030289 211.787515 \n",
       "L 135.465716 210.317077 \n",
       "L 135.901144 210.167512 \n",
       "L 136.336571 213.066566 \n",
       "L 136.771998 212.47963 \n",
       "L 137.207425 203.836649 \n",
       "L 137.642853 210.070014 \n",
       "L 138.07828 211.869025 \n",
       "L 138.949134 208.357892 \n",
       "L 139.384562 210.316491 \n",
       "L 139.819989 210.626635 \n",
       "L 140.255416 213.033125 \n",
       "L 140.690843 207.388385 \n",
       "L 141.12627 212.925278 \n",
       "L 141.561698 208.065942 \n",
       "L 141.997125 214.268325 \n",
       "L 142.432552 214.349701 \n",
       "L 142.867979 214.63196 \n",
       "L 143.303407 212.952742 \n",
       "L 143.738834 214.616327 \n",
       "L 144.174261 207.215863 \n",
       "L 145.045116 214.787634 \n",
       "L 145.480543 207.917624 \n",
       "L 145.91597 211.980575 \n",
       "L 146.351397 214.453458 \n",
       "L 146.786825 213.719431 \n",
       "L 147.222252 214.527199 \n",
       "L 147.657679 208.690328 \n",
       "L 148.093106 208.826133 \n",
       "L 148.528533 216.102048 \n",
       "L 148.963961 215.128164 \n",
       "L 149.399388 216.096217 \n",
       "L 149.834815 210.012187 \n",
       "L 150.270242 213.793052 \n",
       "L 150.70567 211.953718 \n",
       "L 151.141097 217.369324 \n",
       "L 151.576524 210.867598 \n",
       "L 152.011951 215.016539 \n",
       "L 152.447379 211.534571 \n",
       "L 152.882806 216.984221 \n",
       "L 153.318233 217.773323 \n",
       "L 153.75366 215.458967 \n",
       "L 154.189087 216.282879 \n",
       "L 154.624515 219.4749 \n",
       "L 155.059942 215.525383 \n",
       "L 155.495369 219.555375 \n",
       "L 155.930796 218.617437 \n",
       "L 156.366224 219.336756 \n",
       "L 156.801651 214.918211 \n",
       "L 157.237078 219.83575 \n",
       "L 157.672505 216.757703 \n",
       "L 158.107933 219.630385 \n",
       "L 158.54336 216.566559 \n",
       "L 158.978787 216.728314 \n",
       "L 159.414214 221.639313 \n",
       "L 159.849642 220.021927 \n",
       "L 160.285069 217.535397 \n",
       "L 160.720496 219.372654 \n",
       "L 161.155923 218.808944 \n",
       "L 161.59135 221.795834 \n",
       "L 162.026778 220.495298 \n",
       "L 162.462205 219.5461 \n",
       "L 162.897632 222.283291 \n",
       "L 163.333059 222.986881 \n",
       "L 163.768487 218.999252 \n",
       "L 164.203914 222.142348 \n",
       "L 164.639341 222.789462 \n",
       "L 165.074768 219.460898 \n",
       "L 165.510196 222.5824 \n",
       "L 165.945623 223.324355 \n",
       "L 166.38105 217.263904 \n",
       "L 166.816477 222.8588 \n",
       "L 167.251905 219.174327 \n",
       "L 168.122759 223.218325 \n",
       "L 168.993613 219.738295 \n",
       "L 169.429041 223.332576 \n",
       "L 169.864468 223.287791 \n",
       "L 170.299895 219.823692 \n",
       "L 170.735322 223.812544 \n",
       "L 171.17075 224.983893 \n",
       "L 172.041604 223.275199 \n",
       "L 172.477031 224.548212 \n",
       "L 172.912459 223.373443 \n",
       "L 173.347886 220.8845 \n",
       "L 173.783313 223.66946 \n",
       "L 174.21874 223.97565 \n",
       "L 174.654167 224.836901 \n",
       "L 175.089595 220.463929 \n",
       "L 175.525022 223.340238 \n",
       "L 175.960449 222.594113 \n",
       "L 176.395876 224.823304 \n",
       "L 176.831304 223.056115 \n",
       "L 177.266731 225.899449 \n",
       "L 177.702158 224.491042 \n",
       "L 178.137585 225.595884 \n",
       "L 178.573013 221.378839 \n",
       "L 179.00844 224.058154 \n",
       "L 179.443867 222.808862 \n",
       "L 179.879294 225.705781 \n",
       "L 180.750149 225.534254 \n",
       "L 181.185576 225.182521 \n",
       "L 181.621003 223.891175 \n",
       "L 182.05643 225.387616 \n",
       "L 182.491858 222.29448 \n",
       "L 182.927285 224.504175 \n",
       "L 183.362712 224.153117 \n",
       "L 183.798139 225.509963 \n",
       "L 184.233567 225.737697 \n",
       "L 184.668994 223.047032 \n",
       "L 185.104421 226.365528 \n",
       "L 185.975276 222.914553 \n",
       "L 186.410703 224.500901 \n",
       "L 186.84613 225.130651 \n",
       "L 187.281557 221.537174 \n",
       "L 187.716984 223.209253 \n",
       "L 188.152412 222.688161 \n",
       "L 188.587839 226.629983 \n",
       "L 189.023266 224.709439 \n",
       "L 189.458693 225.079403 \n",
       "L 189.894121 225.948772 \n",
       "L 190.329548 222.557521 \n",
       "L 190.764975 225.919544 \n",
       "L 191.200402 226.38475 \n",
       "L 192.071257 224.096972 \n",
       "L 192.506684 224.070602 \n",
       "L 192.942111 227.071059 \n",
       "L 193.377539 224.838887 \n",
       "L 193.812966 226.771038 \n",
       "L 194.248393 226.196932 \n",
       "L 194.68382 225.08821 \n",
       "L 195.119247 226.353166 \n",
       "L 195.554675 223.566632 \n",
       "L 195.990102 226.725419 \n",
       "L 196.425529 224.461885 \n",
       "L 196.860956 227.046769 \n",
       "L 197.296384 221.455387 \n",
       "L 197.731811 225.985685 \n",
       "L 198.167238 225.973659 \n",
       "L 198.602665 225.590821 \n",
       "L 199.038093 226.448264 \n",
       "L 199.47352 223.163207 \n",
       "L 199.908947 226.589946 \n",
       "L 200.344374 225.885342 \n",
       "L 200.779802 223.87095 \n",
       "L 201.215229 226.654405 \n",
       "L 201.650656 224.530234 \n",
       "L 202.086083 226.86279 \n",
       "L 202.52151 225.147157 \n",
       "L 202.956938 225.713177 \n",
       "L 203.392365 227.317433 \n",
       "L 203.827792 227.76088 \n",
       "L 204.263219 224.596472 \n",
       "L 204.698647 227.259489 \n",
       "L 205.569501 225.677829 \n",
       "L 206.004928 226.034715 \n",
       "L 206.440356 222.635443 \n",
       "L 206.875783 226.066729 \n",
       "L 207.31121 225.279191 \n",
       "L 207.746637 228.144114 \n",
       "L 208.182064 226.042891 \n",
       "L 208.617492 225.024227 \n",
       "L 209.488346 227.714773 \n",
       "L 209.923773 226.512001 \n",
       "L 210.359201 224.775759 \n",
       "L 210.794628 227.588726 \n",
       "L 211.665482 226.959605 \n",
       "L 212.10091 227.75877 \n",
       "L 212.971764 227.224162 \n",
       "L 213.407191 225.982346 \n",
       "L 213.842619 226.934228 \n",
       "L 214.278046 224.536148 \n",
       "L 214.713473 227.159067 \n",
       "L 215.1489 227.889885 \n",
       "L 215.584327 228.029447 \n",
       "L 216.019755 228.33375 \n",
       "L 216.455182 227.075098 \n",
       "L 216.890609 227.305539 \n",
       "L 217.326036 225.998793 \n",
       "L 217.761464 225.688207 \n",
       "L 218.196891 227.1743 \n",
       "L 218.632318 227.866881 \n",
       "L 219.067745 226.490741 \n",
       "L 219.503173 225.965845 \n",
       "L 219.9386 225.12997 \n",
       "L 220.374027 225.517281 \n",
       "L 220.809454 226.815543 \n",
       "L 221.244881 223.320025 \n",
       "L 221.680309 226.872925 \n",
       "L 222.115736 225.159545 \n",
       "L 222.551163 228.941131 \n",
       "L 222.98659 225.741827 \n",
       "L 223.422018 225.722457 \n",
       "L 223.857445 227.691556 \n",
       "L 224.292872 227.457946 \n",
       "L 224.728299 227.99028 \n",
       "L 225.163727 223.707825 \n",
       "L 225.599154 227.058937 \n",
       "L 226.034581 227.084509 \n",
       "L 226.470008 227.225353 \n",
       "L 226.905436 228.831878 \n",
       "L 227.340863 226.008676 \n",
       "L 227.77629 227.806356 \n",
       "L 228.211717 225.397731 \n",
       "L 228.647144 225.111136 \n",
       "L 229.082572 227.396472 \n",
       "L 229.517999 227.634556 \n",
       "L 229.953426 227.245062 \n",
       "L 230.388853 228.768479 \n",
       "L 230.824281 229.218689 \n",
       "L 231.259708 223.63072 \n",
       "L 231.695135 227.976867 \n",
       "L 232.130562 228.148433 \n",
       "L 232.56599 228.520044 \n",
       "L 233.001417 226.40784 \n",
       "L 233.436844 227.322813 \n",
       "L 233.872271 225.324222 \n",
       "L 234.307698 227.986444 \n",
       "L 234.743126 226.521177 \n",
       "L 235.61398 227.249564 \n",
       "L 236.049407 229.04914 \n",
       "L 236.484835 227.652038 \n",
       "L 236.920262 227.405679 \n",
       "L 237.355689 229.384503 \n",
       "L 237.791116 228.975814 \n",
       "L 238.226544 227.529894 \n",
       "L 238.661971 227.712195 \n",
       "L 239.097398 227.087272 \n",
       "L 239.532825 227.000918 \n",
       "L 239.968253 227.884061 \n",
       "L 240.839107 228.076408 \n",
       "L 241.274534 229.283869 \n",
       "L 241.709961 226.539997 \n",
       "L 242.145389 226.898675 \n",
       "L 242.580816 228.506892 \n",
       "L 243.016243 228.351776 \n",
       "L 243.45167 229.194592 \n",
       "L 243.887098 228.550955 \n",
       "L 244.322525 229.183639 \n",
       "L 244.757952 227.929632 \n",
       "L 245.193379 226.248778 \n",
       "L 246.064234 228.213183 \n",
       "L 246.499661 227.232958 \n",
       "L 246.935088 223.939156 \n",
       "L 247.370516 228.355717 \n",
       "L 247.805943 227.422328 \n",
       "L 248.24137 228.024953 \n",
       "L 248.676797 229.475583 \n",
       "L 249.112224 225.188189 \n",
       "L 249.547652 228.479404 \n",
       "L 250.418506 228.862058 \n",
       "L 250.853933 228.045471 \n",
       "L 251.289361 228.544196 \n",
       "L 251.724788 226.277181 \n",
       "L 252.160215 226.148133 \n",
       "L 252.595642 228.708677 \n",
       "L 253.03107 226.340754 \n",
       "L 253.466497 225.508737 \n",
       "L 253.901924 228.430806 \n",
       "L 254.337351 229.138853 \n",
       "L 255.208206 226.583394 \n",
       "L 255.643633 228.75392 \n",
       "L 256.07906 229.036149 \n",
       "L 256.514487 226.718216 \n",
       "L 256.949915 227.064018 \n",
       "L 257.385342 228.873416 \n",
       "L 257.820769 228.325446 \n",
       "L 258.256196 229.459549 \n",
       "L 258.691624 225.929733 \n",
       "L 259.127051 228.778095 \n",
       "L 259.562478 227.068117 \n",
       "L 259.997905 227.728001 \n",
       "L 260.433333 226.98989 \n",
       "L 260.86876 229.185858 \n",
       "L 261.304187 228.173718 \n",
       "L 261.739614 224.86827 \n",
       "L 262.175041 228.711299 \n",
       "L 262.610469 228.009686 \n",
       "L 263.045896 226.466473 \n",
       "L 263.481323 227.607744 \n",
       "L 264.352178 228.15455 \n",
       "L 264.787605 227.832837 \n",
       "L 265.223032 227.859147 \n",
       "L 265.658459 227.534648 \n",
       "L 266.093887 227.067931 \n",
       "L 266.529314 226.879004 \n",
       "L 266.964741 228.357201 \n",
       "L 267.400168 227.0222 \n",
       "L 267.835595 227.031541 \n",
       "L 268.271023 228.236701 \n",
       "L 268.70645 228.502552 \n",
       "L 269.141877 225.802951 \n",
       "L 269.577304 228.917377 \n",
       "L 270.012732 227.998316 \n",
       "L 270.448159 226.524762 \n",
       "L 270.883586 228.136381 \n",
       "L 271.319013 228.503751 \n",
       "L 271.754441 227.177219 \n",
       "L 272.189868 228.436257 \n",
       "L 272.625295 227.994862 \n",
       "L 273.060722 229.003458 \n",
       "L 273.49615 228.820076 \n",
       "L 273.931577 229.214759 \n",
       "L 274.367004 226.410567 \n",
       "L 274.802431 228.901926 \n",
       "L 275.237858 227.444717 \n",
       "L 275.673286 229.873679 \n",
       "L 276.108713 224.213359 \n",
       "L 276.54414 229.210134 \n",
       "L 276.979567 226.068743 \n",
       "L 277.414995 228.149726 \n",
       "L 277.850422 226.913986 \n",
       "L 278.285849 227.196262 \n",
       "L 278.721276 226.399472 \n",
       "L 279.156704 226.989778 \n",
       "L 279.592131 228.932487 \n",
       "L 280.027558 227.379285 \n",
       "L 280.462985 229.544119 \n",
       "L 280.898413 227.48543 \n",
       "L 281.769267 228.108035 \n",
       "L 282.204694 226.752872 \n",
       "L 283.075549 228.583165 \n",
       "L 283.510976 227.643083 \n",
       "L 283.946403 226.260549 \n",
       "L 284.38183 227.614371 \n",
       "L 284.817258 227.660402 \n",
       "L 285.252685 229.0568 \n",
       "L 285.688112 228.462286 \n",
       "L 286.123539 228.19074 \n",
       "L 286.558967 228.718623 \n",
       "L 287.429821 227.109138 \n",
       "L 287.865248 229.15282 \n",
       "L 288.300675 225.933135 \n",
       "L 288.736103 228.69227 \n",
       "L 289.17153 228.685241 \n",
       "L 289.606957 226.732596 \n",
       "L 290.042384 225.390955 \n",
       "L 290.477812 228.794796 \n",
       "L 290.913239 227.410102 \n",
       "L 291.348666 229.650035 \n",
       "L 292.219521 226.314958 \n",
       "L 292.654948 227.841381 \n",
       "L 293.090375 227.531773 \n",
       "L 293.525802 228.848515 \n",
       "L 293.96123 227.714462 \n",
       "L 294.396657 229.181551 \n",
       "L 294.832084 228.15071 \n",
       "L 295.267511 229.633664 \n",
       "L 295.702938 227.322519 \n",
       "L 296.138366 228.727236 \n",
       "L 297.00922 228.433865 \n",
       "L 297.444647 228.468642 \n",
       "L 297.880075 227.507566 \n",
       "L 298.315502 228.347466 \n",
       "L 298.750929 227.617741 \n",
       "L 299.186356 227.758721 \n",
       "L 299.621784 227.352807 \n",
       "L 300.057211 226.186203 \n",
       "L 300.492638 228.151033 \n",
       "L 300.928065 226.974426 \n",
       "L 301.363492 227.164608 \n",
       "L 301.79892 228.942614 \n",
       "L 302.234347 228.325789 \n",
       "L 302.669774 229.143166 \n",
       "L 303.105201 228.313377 \n",
       "L 303.540629 228.923115 \n",
       "L 303.976056 227.155407 \n",
       "L 304.411483 226.750099 \n",
       "L 304.84691 228.283353 \n",
       "L 305.282338 227.271165 \n",
       "L 305.717765 228.81119 \n",
       "L 306.153192 228.652859 \n",
       "L 306.588619 229.028207 \n",
       "L 307.459474 227.688564 \n",
       "L 307.894901 228.704042 \n",
       "L 308.330328 227.616998 \n",
       "L 308.765755 228.039444 \n",
       "L 309.201183 227.284123 \n",
       "L 309.63661 228.179398 \n",
       "L 310.072037 228.569265 \n",
       "L 310.507464 227.640688 \n",
       "L 310.942892 228.836349 \n",
       "L 311.378319 227.954769 \n",
       "L 311.813746 228.238811 \n",
       "L 312.249173 228.840721 \n",
       "L 312.684601 229.017181 \n",
       "L 313.120028 227.363048 \n",
       "L 313.555455 228.925279 \n",
       "L 313.990882 228.578102 \n",
       "L 314.42631 226.994646 \n",
       "L 314.861737 228.75154 \n",
       "L 315.297164 229.734446 \n",
       "L 315.732591 225.349807 \n",
       "L 316.168018 228.543179 \n",
       "L 316.603446 228.342821 \n",
       "L 317.038873 229.097416 \n",
       "L 318.345155 228.932942 \n",
       "L 318.780582 229.496457 \n",
       "L 319.216009 228.530316 \n",
       "L 319.651436 227.961497 \n",
       "L 320.086864 227.984463 \n",
       "L 320.522291 228.737777 \n",
       "L 320.957718 227.322894 \n",
       "L 321.393145 228.585501 \n",
       "L 321.828572 228.372874 \n",
       "L 322.264 227.910732 \n",
       "L 322.699427 223.914015 \n",
       "L 323.134854 228.452756 \n",
       "L 323.570281 226.241264 \n",
       "L 324.005709 228.335562 \n",
       "L 324.441136 228.768205 \n",
       "L 324.876563 228.656422 \n",
       "L 325.31199 229.474889 \n",
       "L 325.747418 226.523394 \n",
       "L 326.182845 228.451514 \n",
       "L 326.618272 227.444145 \n",
       "L 327.053699 228.425774 \n",
       "L 327.489127 226.597097 \n",
       "L 327.924554 228.021391 \n",
       "L 328.359981 227.675557 \n",
       "L 328.795408 228.316517 \n",
       "L 329.230835 226.620289 \n",
       "L 329.666263 228.785119 \n",
       "L 330.10169 229.214683 \n",
       "L 330.537117 225.62351 \n",
       "L 330.972544 227.649334 \n",
       "L 331.407972 227.101402 \n",
       "L 331.843399 226.986838 \n",
       "L 332.278826 227.752491 \n",
       "L 332.714253 227.21981 \n",
       "L 333.149681 227.555225 \n",
       "L 333.585108 228.38252 \n",
       "L 334.020535 226.995212 \n",
       "L 334.455962 229.477286 \n",
       "L 334.891389 228.364004 \n",
       "L 335.326817 229.711583 \n",
       "L 335.762244 227.703926 \n",
       "L 336.197671 228.078076 \n",
       "L 336.633098 227.821517 \n",
       "L 337.068526 227.699723 \n",
       "L 337.503953 224.624838 \n",
       "L 337.93938 227.234401 \n",
       "L 338.374807 228.323937 \n",
       "L 338.810235 228.799785 \n",
       "L 339.245662 228.451901 \n",
       "L 339.681089 227.941519 \n",
       "L 340.116516 229.186084 \n",
       "L 340.551944 227.63202 \n",
       "L 340.987371 227.000855 \n",
       "L 341.422798 227.749607 \n",
       "L 341.858225 228.079813 \n",
       "L 342.293652 227.342728 \n",
       "L 342.72908 229.095678 \n",
       "L 343.164507 228.79076 \n",
       "L 343.599934 227.85994 \n",
       "L 344.035361 228.425998 \n",
       "L 344.906216 227.648528 \n",
       "L 345.341643 228.656409 \n",
       "L 345.77707 227.388064 \n",
       "L 346.212498 229.714384 \n",
       "L 346.647925 226.438765 \n",
       "L 347.083352 228.095859 \n",
       "L 347.518779 229.096015 \n",
       "L 348.825061 228.090801 \n",
       "L 349.260488 229.345139 \n",
       "L 349.695915 228.683291 \n",
       "L 350.131343 229.506007 \n",
       "L 351.002197 227.876516 \n",
       "L 351.437624 229.199512 \n",
       "L 351.873052 229.467469 \n",
       "L 352.308479 229.187676 \n",
       "L 352.743906 226.896427 \n",
       "L 353.179333 227.669779 \n",
       "L 353.614761 229.636033 \n",
       "L 354.485615 228.817531 \n",
       "L 354.921042 228.74478 \n",
       "L 355.356469 227.992313 \n",
       "L 355.791897 227.52315 \n",
       "L 356.227324 228.399708 \n",
       "L 356.662751 225.970189 \n",
       "L 357.098178 228.994941 \n",
       "L 357.533606 228.274798 \n",
       "L 358.40446 229.245679 \n",
       "L 358.839887 228.753052 \n",
       "L 359.275315 228.971984 \n",
       "L 359.710742 228.12376 \n",
       "L 360.146169 228.074043 \n",
       "L 360.581596 226.564109 \n",
       "L 361.017024 228.516643 \n",
       "L 361.452451 228.607966 \n",
       "L 361.887878 227.715494 \n",
       "L 362.323305 227.712963 \n",
       "L 362.758732 228.254077 \n",
       "L 363.19416 228.363495 \n",
       "L 363.629587 225.919902 \n",
       "L 364.065014 228.621934 \n",
       "L 364.500441 229.143711 \n",
       "L 364.935869 229.02754 \n",
       "L 365.371296 227.182487 \n",
       "L 365.806723 226.873042 \n",
       "L 366.24215 226.11361 \n",
       "L 366.677578 226.893743 \n",
       "L 367.113005 226.910972 \n",
       "L 367.548432 228.333918 \n",
       "L 367.983859 228.524085 \n",
       "L 368.419286 229.016291 \n",
       "L 368.854714 228.036392 \n",
       "L 369.290141 228.000086 \n",
       "L 369.725568 229.874489 \n",
       "L 369.725568 229.874489 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path clip-path=\"url(#p73f0f55659)\" d=\"M 65.361932 32.201761 \n",
       "L 65.797359 48.487969 \n",
       "L 66.232786 59.846355 \n",
       "L 66.668214 64.430583 \n",
       "L 67.103641 61.069961 \n",
       "L 67.539068 62.50358 \n",
       "L 67.974495 67.797626 \n",
       "L 68.409922 82.81915 \n",
       "L 68.84535 84.044447 \n",
       "L 69.716204 95.189362 \n",
       "L 70.151631 95.28231 \n",
       "L 70.587059 84.967545 \n",
       "L 71.022486 98.594125 \n",
       "L 71.457913 105.573796 \n",
       "L 71.89334 108.914343 \n",
       "L 72.328768 108.026169 \n",
       "L 72.764195 115.148078 \n",
       "L 73.199622 116.979323 \n",
       "L 73.635049 120.460876 \n",
       "L 74.070476 119.686678 \n",
       "L 74.505904 126.411113 \n",
       "L 74.941331 115.395685 \n",
       "L 75.376758 122.553738 \n",
       "L 75.812185 126.66078 \n",
       "L 76.247613 133.736145 \n",
       "L 76.68304 135.384831 \n",
       "L 77.118467 139.553526 \n",
       "L 77.553894 140.595305 \n",
       "L 77.989322 137.599527 \n",
       "L 78.424749 145.202627 \n",
       "L 78.860176 148.545317 \n",
       "L 79.295603 146.999838 \n",
       "L 79.731031 149.44466 \n",
       "L 80.166458 149.365468 \n",
       "L 80.601885 155.925175 \n",
       "L 81.037312 153.407512 \n",
       "L 81.472739 154.035569 \n",
       "L 81.908167 157.725692 \n",
       "L 82.343594 157.331079 \n",
       "L 82.779021 158.753612 \n",
       "L 83.214448 162.977757 \n",
       "L 83.649876 161.369531 \n",
       "L 84.52073 169.263382 \n",
       "L 84.956157 166.711297 \n",
       "L 85.391585 166.75669 \n",
       "L 85.827012 169.98733 \n",
       "L 86.262439 171.648393 \n",
       "L 86.697866 176.293474 \n",
       "L 87.133294 174.139631 \n",
       "L 87.568721 174.546023 \n",
       "L 88.004148 174.219116 \n",
       "L 88.439575 175.76544 \n",
       "L 88.875002 178.882504 \n",
       "L 89.31043 177.941804 \n",
       "L 89.745857 180.500373 \n",
       "L 90.181284 179.381666 \n",
       "L 90.616711 180.644979 \n",
       "L 91.052139 181.525039 \n",
       "L 91.487566 181.513689 \n",
       "L 91.922993 184.738802 \n",
       "L 92.35842 185.507657 \n",
       "L 92.793848 185.088358 \n",
       "L 93.229275 186.146788 \n",
       "L 93.664702 188.465909 \n",
       "L 94.535556 187.022376 \n",
       "L 94.970984 188.766664 \n",
       "L 95.406411 189.803306 \n",
       "L 95.841838 195.029061 \n",
       "L 96.277265 191.880112 \n",
       "L 96.712693 193.513933 \n",
       "L 97.14812 193.303068 \n",
       "L 97.583547 192.53624 \n",
       "L 98.018974 195.139659 \n",
       "L 98.454402 191.765738 \n",
       "L 98.889829 195.054266 \n",
       "L 99.325256 189.910589 \n",
       "L 100.196111 196.793685 \n",
       "L 100.631538 196.285452 \n",
       "L 101.066965 193.275858 \n",
       "L 101.502392 197.592334 \n",
       "L 101.937819 198.937468 \n",
       "L 102.373247 199.882919 \n",
       "L 102.808674 195.335404 \n",
       "L 103.244101 198.635711 \n",
       "L 103.679528 200.216256 \n",
       "L 104.114956 200.920845 \n",
       "L 104.550383 200.288386 \n",
       "L 104.98581 196.821451 \n",
       "L 105.421237 200.068777 \n",
       "L 105.856665 200.766704 \n",
       "L 106.292092 201.750789 \n",
       "L 106.727519 203.289701 \n",
       "L 107.162946 202.925832 \n",
       "L 107.598373 202.897869 \n",
       "L 108.033801 203.458555 \n",
       "L 108.469228 203.335968 \n",
       "L 108.904655 206.124083 \n",
       "L 109.340082 205.082517 \n",
       "L 109.77551 204.410455 \n",
       "L 110.210937 205.835242 \n",
       "L 110.646364 206.184866 \n",
       "L 111.081791 204.604875 \n",
       "L 111.517219 204.057581 \n",
       "L 111.952646 205.468894 \n",
       "L 112.388073 207.375041 \n",
       "L 112.8235 208.519087 \n",
       "L 113.258928 204.100911 \n",
       "L 113.694355 206.521552 \n",
       "L 114.129782 206.237042 \n",
       "L 114.565209 207.463042 \n",
       "L 115.000636 206.642326 \n",
       "L 115.436064 206.1207 \n",
       "L 115.871491 208.625467 \n",
       "L 116.306918 204.697976 \n",
       "L 116.742345 208.104729 \n",
       "L 117.177773 209.134809 \n",
       "L 117.6132 207.154198 \n",
       "L 118.048627 208.731698 \n",
       "L 118.484054 208.44664 \n",
       "L 118.919482 207.857564 \n",
       "L 119.354909 209.222923 \n",
       "L 119.790336 208.49657 \n",
       "L 120.225763 208.860633 \n",
       "L 120.66119 209.661399 \n",
       "L 121.096618 211.074738 \n",
       "L 121.532045 207.409715 \n",
       "L 121.967472 208.830104 \n",
       "L 122.402899 210.746533 \n",
       "L 122.838327 211.320003 \n",
       "L 123.273754 210.584559 \n",
       "L 124.144608 209.912346 \n",
       "L 124.580036 212.084477 \n",
       "L 125.015463 212.540745 \n",
       "L 125.45089 212.655451 \n",
       "L 126.321745 212.21356 \n",
       "L 126.757172 212.399725 \n",
       "L 127.192599 210.750193 \n",
       "L 127.628026 213.044373 \n",
       "L 128.063453 214.687665 \n",
       "L 128.498881 213.390589 \n",
       "L 128.934308 213.36961 \n",
       "L 129.369735 214.429451 \n",
       "L 129.805162 210.785442 \n",
       "L 130.24059 210.788606 \n",
       "L 130.676017 211.840625 \n",
       "L 131.111444 213.658879 \n",
       "L 131.546871 212.9631 \n",
       "L 131.982299 212.972596 \n",
       "L 132.417726 209.528963 \n",
       "L 132.853153 210.978007 \n",
       "L 133.28858 213.397565 \n",
       "L 133.724008 209.288092 \n",
       "L 134.159435 215.657579 \n",
       "L 134.594862 211.888116 \n",
       "L 135.030289 211.687862 \n",
       "L 135.465716 213.420028 \n",
       "L 135.901144 213.511513 \n",
       "L 136.336571 215.325048 \n",
       "L 136.771998 214.883627 \n",
       "L 137.207425 214.841873 \n",
       "L 137.642853 213.488023 \n",
       "L 138.07828 213.445937 \n",
       "L 138.513707 216.558538 \n",
       "L 138.949134 213.701919 \n",
       "L 139.384562 214.231076 \n",
       "L 139.819989 214.503241 \n",
       "L 140.255416 214.339978 \n",
       "L 140.690843 214.400478 \n",
       "L 141.12627 214.669522 \n",
       "L 141.561698 215.594966 \n",
       "L 141.997125 217.830662 \n",
       "L 142.432552 215.367091 \n",
       "L 142.867979 216.675834 \n",
       "L 143.303407 215.68925 \n",
       "L 143.738834 215.493727 \n",
       "L 144.174261 215.602066 \n",
       "L 145.045116 217.888287 \n",
       "L 145.480543 216.425386 \n",
       "L 145.91597 215.691834 \n",
       "L 146.351397 218.748055 \n",
       "L 146.786825 217.702592 \n",
       "L 147.222252 216.299065 \n",
       "L 147.657679 217.027016 \n",
       "L 148.093106 216.039758 \n",
       "L 148.528533 218.905262 \n",
       "L 148.963961 217.513222 \n",
       "L 149.399388 218.277994 \n",
       "L 149.834815 217.724824 \n",
       "L 150.70567 217.41638 \n",
       "L 151.141097 218.864102 \n",
       "L 151.576524 218.663105 \n",
       "L 152.011951 217.11428 \n",
       "L 152.447379 219.289955 \n",
       "L 152.882806 218.833864 \n",
       "L 153.318233 217.508117 \n",
       "L 154.189087 219.637645 \n",
       "L 154.624515 219.86485 \n",
       "L 155.059942 219.00456 \n",
       "L 155.495369 218.491305 \n",
       "L 155.930796 220.298797 \n",
       "L 156.366224 219.233407 \n",
       "L 156.801651 219.228318 \n",
       "L 157.237078 220.227328 \n",
       "L 158.107933 219.683593 \n",
       "L 158.54336 220.751332 \n",
       "L 158.978787 220.197041 \n",
       "L 159.414214 220.23334 \n",
       "L 159.849642 221.26183 \n",
       "L 160.285069 221.057426 \n",
       "L 160.720496 221.210095 \n",
       "L 161.155923 220.823409 \n",
       "L 162.026778 221.380614 \n",
       "L 162.897632 221.250171 \n",
       "L 163.333059 221.402856 \n",
       "L 163.768487 221.360979 \n",
       "L 164.203914 222.170971 \n",
       "L 164.639341 221.824443 \n",
       "L 165.074768 221.908657 \n",
       "L 165.510196 221.732312 \n",
       "L 165.945623 221.833634 \n",
       "L 166.38105 222.679544 \n",
       "L 167.251905 222.361491 \n",
       "L 167.687332 222.595613 \n",
       "L 168.122759 222.483046 \n",
       "L 168.558186 222.636343 \n",
       "L 168.993613 222.953111 \n",
       "L 169.429041 222.83162 \n",
       "L 169.864468 223.300799 \n",
       "L 170.299895 223.236776 \n",
       "L 170.735322 222.923855 \n",
       "L 171.606177 223.343747 \n",
       "L 172.477031 223.344906 \n",
       "L 172.912459 223.808304 \n",
       "L 173.347886 223.808944 \n",
       "L 174.21874 223.478938 \n",
       "L 174.654167 223.571491 \n",
       "L 175.089595 224.180041 \n",
       "L 175.960449 224.164784 \n",
       "L 176.395876 224.040305 \n",
       "L 177.266731 224.379149 \n",
       "L 177.702158 224.320631 \n",
       "L 178.137585 223.863904 \n",
       "L 178.573013 224.14641 \n",
       "L 179.00844 224.140261 \n",
       "L 179.443867 224.563525 \n",
       "L 180.314722 224.663504 \n",
       "L 180.750149 224.572687 \n",
       "L 181.185576 224.688201 \n",
       "L 181.621003 224.5755 \n",
       "L 182.05643 224.588393 \n",
       "L 182.491858 224.915882 \n",
       "L 182.927285 224.965627 \n",
       "L 183.362712 224.57632 \n",
       "L 183.798139 225.07182 \n",
       "L 184.233567 224.877139 \n",
       "L 184.668994 225.179229 \n",
       "L 189.023266 225.389333 \n",
       "L 189.458693 225.158035 \n",
       "L 189.894121 225.431637 \n",
       "L 190.329548 225.31544 \n",
       "L 190.764975 225.512812 \n",
       "L 191.200402 225.416591 \n",
       "L 192.071257 225.584482 \n",
       "L 193.377539 225.59317 \n",
       "L 194.68382 225.775099 \n",
       "L 195.990102 225.735379 \n",
       "L 196.425529 225.882657 \n",
       "L 196.860956 225.71451 \n",
       "L 197.296384 225.920201 \n",
       "L 199.038093 226.024129 \n",
       "L 199.47352 225.875321 \n",
       "L 200.344374 226.03573 \n",
       "L 200.779802 226.099093 \n",
       "L 202.086083 226.017921 \n",
       "L 202.52151 226.19915 \n",
       "L 202.956938 226.072869 \n",
       "L 204.698647 226.376381 \n",
       "L 205.134074 226.123406 \n",
       "L 205.569501 226.315472 \n",
       "L 206.875783 226.391482 \n",
       "L 208.182064 226.219399 \n",
       "L 208.617492 226.430753 \n",
       "L 209.052919 226.249157 \n",
       "L 210.359201 226.544732 \n",
       "L 211.230055 226.561893 \n",
       "L 211.665482 226.558331 \n",
       "L 212.10091 226.411393 \n",
       "L 212.971764 226.617377 \n",
       "L 213.407191 226.535442 \n",
       "L 213.842619 226.658815 \n",
       "L 214.278046 226.625879 \n",
       "L 214.713473 226.476661 \n",
       "L 215.584327 226.722059 \n",
       "L 216.019755 226.676207 \n",
       "L 216.890609 226.779679 \n",
       "L 217.326036 226.66608 \n",
       "L 218.196891 226.709944 \n",
       "L 221.244881 226.733508 \n",
       "L 222.551163 226.81213 \n",
       "L 223.422018 226.899714 \n",
       "L 224.728299 226.853237 \n",
       "L 225.599154 226.939473 \n",
       "L 226.905436 226.96222 \n",
       "L 229.953426 227.075038 \n",
       "L 230.388853 226.865279 \n",
       "L 230.824281 227.070516 \n",
       "L 231.259708 227.120406 \n",
       "L 232.130562 226.997519 \n",
       "L 233.001417 227.060439 \n",
       "L 233.436844 227.250494 \n",
       "L 234.307698 227.341328 \n",
       "L 234.743126 227.463649 \n",
       "L 235.61398 227.38142 \n",
       "L 236.049407 227.294623 \n",
       "L 236.920262 227.4949 \n",
       "L 237.355689 227.39383 \n",
       "L 238.226544 227.494985 \n",
       "L 243.016243 227.595784 \n",
       "L 244.757952 227.565047 \n",
       "L 245.628807 227.596371 \n",
       "L 246.064234 227.512641 \n",
       "L 246.935088 227.630755 \n",
       "L 249.112224 227.635764 \n",
       "L 252.595642 227.729779 \n",
       "L 253.466497 227.679991 \n",
       "L 307.894901 227.855351 \n",
       "L 308.330328 227.783903 \n",
       "L 309.201183 227.855745 \n",
       "L 310.942892 227.916271 \n",
       "L 312.684601 227.875387 \n",
       "L 313.990882 227.879281 \n",
       "L 318.780582 227.899907 \n",
       "L 319.216009 227.801609 \n",
       "L 319.651436 227.890286 \n",
       "L 320.957718 227.878496 \n",
       "L 321.393145 227.924999 \n",
       "L 322.264 227.835796 \n",
       "L 329.666263 227.915636 \n",
       "L 330.10169 227.840411 \n",
       "L 330.972544 227.884521 \n",
       "L 332.278826 227.885308 \n",
       "L 332.714253 227.915131 \n",
       "L 333.149681 227.830498 \n",
       "L 334.020535 227.88968 \n",
       "L 334.455962 227.814618 \n",
       "L 334.891389 227.919616 \n",
       "L 335.326817 227.863918 \n",
       "L 335.762244 227.929984 \n",
       "L 336.197671 227.849242 \n",
       "L 337.93938 227.955924 \n",
       "L 338.374807 227.867841 \n",
       "L 339.245662 227.881863 \n",
       "L 340.116516 227.859354 \n",
       "L 340.987371 227.90022 \n",
       "L 341.422798 227.875517 \n",
       "L 341.858225 227.966714 \n",
       "L 342.72908 227.871308 \n",
       "L 344.470789 227.948637 \n",
       "L 346.647925 227.900606 \n",
       "L 362.758732 227.972919 \n",
       "L 363.19416 227.848319 \n",
       "L 363.629587 227.96192 \n",
       "L 365.371296 227.845693 \n",
       "L 367.548432 227.927852 \n",
       "L 369.290141 227.907637 \n",
       "L 369.725568 227.809998 \n",
       "L 369.725568 227.809998 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_13\">\n",
       "    <!-- Autoencoder loss over time -->\n",
       "    <defs>\n",
       "     <path d=\"M 34.1875 63.1875 \n",
       "L 20.796875 26.90625 \n",
       "L 47.609375 26.90625 \n",
       "z\n",
       "M 28.609375 72.90625 \n",
       "L 39.796875 72.90625 \n",
       "L 67.578125 0 \n",
       "L 57.328125 0 \n",
       "L 50.6875 18.703125 \n",
       "L 17.828125 18.703125 \n",
       "L 11.1875 0 \n",
       "L 0.78125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-65\"/>\n",
       "     <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "     <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "     <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "     <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "     <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "     <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(134.935 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "     <use x=\"68.408203\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"131.787109\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"170.996094\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"232.177734\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"293.701172\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"357.080078\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"412.060547\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"473.242188\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"536.71875\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"598.242188\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"639.355469\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"671.142578\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"698.925781\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"760.107422\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"812.207031\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"864.306641\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"896.09375\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"957.275391\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "     <use x=\"1016.455078\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"1077.978516\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"1119.091797\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"1150.878906\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"1190.087891\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"1217.871094\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"1315.283203\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 274.215625 59.674375 \n",
       "L 377.94375 59.674375 \n",
       "Q 379.94375 59.674375 379.94375 57.674375 \n",
       "L 379.94375 29.318125 \n",
       "Q 379.94375 27.318125 377.94375 27.318125 \n",
       "L 274.215625 27.318125 \n",
       "Q 272.215625 27.318125 272.215625 29.318125 \n",
       "L 272.215625 57.674375 \n",
       "Q 272.215625 59.674375 274.215625 59.674375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_13\">\n",
       "     <path d=\"M 276.215625 35.416562 \n",
       "L 296.215625 35.416562 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_14\"/>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- train loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(304.215625 38.916562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\">\n",
       "     <path d=\"M 276.215625 50.094687 \n",
       "L 296.215625 50.094687 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\"/>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- validation loss -->\n",
       "     <g transform=\"translate(304.215625 53.594687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"492.333984\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"524.121094\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"551.904297\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"613.085938\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"665.185547\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p73f0f55659\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses_1['epoch'], losses_1['train_loss'], label='train loss')\n",
    "ax.plot(losses_1['epoch'], losses_1['validation_loss'], label='validation loss')\n",
    "ax.set_ylabel('MSE loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_title('Autoencoder loss over time')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_01 = ae_1.get_encoded_representations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('autoencoderhy_embeddings.pkl', 'wb') as fh:\n",
    "    pickle.dump(encoded_01, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1.to_csv('hyb_loss_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_2 = pd.DataFrame(data=list(zip(ae_1.train_losses, ae_1.val_losses)), columns=['train_loss', 'validation_loss'])\n",
    "losses_2['epoch'] = (losses_2.index + 1) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1f5dd7f50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 277.314375 \n",
       "L 392.14375 277.314375 \n",
       "L 392.14375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m809b7a7ea9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.071785\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(61.890535 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.593754\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 50 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(102.231254 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.115723\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 100 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(142.571973 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.637692\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(186.093942 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.159661\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 200 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(229.615911 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"282.68163\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(273.13788 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"326.203599\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 300 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(316.659849 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m809b7a7ea9\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 350 -->\n",
       "      <g transform=\"translate(360.181818 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- epoch -->\n",
       "     <defs>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(202.315625 268.034687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"mb63ee462fb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mb63ee462fb\" y=\"232.658615\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.00 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 236.457834)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mb63ee462fb\" y=\"190.013899\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 193.813118)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mb63ee462fb\" y=\"147.369183\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 151.168402)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mb63ee462fb\" y=\"104.724467\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 108.523686)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mb63ee462fb\" y=\"62.079751\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 65.87897)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- MSE loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 24.515625 72.90625 \n",
       "L 43.109375 23.296875 \n",
       "L 61.8125 72.90625 \n",
       "L 76.515625 72.90625 \n",
       "L 76.515625 0 \n",
       "L 66.890625 0 \n",
       "L 66.890625 64.015625 \n",
       "L 48.09375 14.015625 \n",
       "L 38.1875 14.015625 \n",
       "L 19.390625 64.015625 \n",
       "L 19.390625 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-77\"/>\n",
       "      <path d=\"M 53.515625 70.515625 \n",
       "L 53.515625 60.890625 \n",
       "Q 47.90625 63.578125 42.921875 64.890625 \n",
       "Q 37.9375 66.21875 33.296875 66.21875 \n",
       "Q 25.25 66.21875 20.875 63.09375 \n",
       "Q 16.5 59.96875 16.5 54.203125 \n",
       "Q 16.5 49.359375 19.40625 46.890625 \n",
       "Q 22.3125 44.4375 30.421875 42.921875 \n",
       "L 36.375 41.703125 \n",
       "Q 47.40625 39.59375 52.65625 34.296875 \n",
       "Q 57.90625 29 57.90625 20.125 \n",
       "Q 57.90625 9.515625 50.796875 4.046875 \n",
       "Q 43.703125 -1.421875 29.984375 -1.421875 \n",
       "Q 24.8125 -1.421875 18.96875 -0.25 \n",
       "Q 13.140625 0.921875 6.890625 3.21875 \n",
       "L 6.890625 13.375 \n",
       "Q 12.890625 10.015625 18.65625 8.296875 \n",
       "Q 24.421875 6.59375 29.984375 6.59375 \n",
       "Q 38.421875 6.59375 43.015625 9.90625 \n",
       "Q 47.609375 13.234375 47.609375 19.390625 \n",
       "Q 47.609375 24.75 44.3125 27.78125 \n",
       "Q 41.015625 30.8125 33.5 32.328125 \n",
       "L 27.484375 33.5 \n",
       "Q 16.453125 35.6875 11.515625 40.375 \n",
       "Q 6.59375 45.0625 6.59375 53.421875 \n",
       "Q 6.59375 63.09375 13.40625 68.65625 \n",
       "Q 20.21875 74.21875 32.171875 74.21875 \n",
       "Q 37.3125 74.21875 42.625 73.28125 \n",
       "Q 47.953125 72.359375 53.515625 70.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-83\"/>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 55.90625 72.90625 \n",
       "L 55.90625 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.015625 \n",
       "L 54.390625 43.015625 \n",
       "L 54.390625 34.71875 \n",
       "L 19.671875 34.71875 \n",
       "L 19.671875 8.296875 \n",
       "L 56.78125 8.296875 \n",
       "L 56.78125 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-69\"/>\n",
       "      <path id=\"DejaVuSans-32\"/>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 152.932656)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n",
       "      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n",
       "      <use x=\"212.939453\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"244.726562\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"272.509766\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"333.691406\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"385.791016\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path clip-path=\"url(#p2fb0fe701f)\" d=\"M 65.361932 39.505106 \n",
       "L 66.232371 55.263443 \n",
       "L 66.522518 55.227515 \n",
       "L 66.812664 57.193119 \n",
       "L 67.392957 75.477978 \n",
       "L 67.683103 75.960797 \n",
       "L 67.97325 73.835617 \n",
       "L 68.843689 84.684322 \n",
       "L 69.133836 93.816393 \n",
       "L 69.423982 96.224275 \n",
       "L 69.714129 99.977798 \n",
       "L 70.004275 101.153544 \n",
       "L 70.294422 107.376245 \n",
       "L 70.584568 108.072361 \n",
       "L 70.874715 107.316054 \n",
       "L 71.164861 107.176915 \n",
       "L 71.455007 113.681504 \n",
       "L 71.745154 109.919085 \n",
       "L 72.0353 114.822236 \n",
       "L 72.325447 115.954733 \n",
       "L 72.615593 126.256214 \n",
       "L 72.90574 129.574793 \n",
       "L 73.195886 129.773716 \n",
       "L 73.486033 120.969813 \n",
       "L 73.776179 137.181612 \n",
       "L 74.066326 136.934293 \n",
       "L 74.356472 136.299447 \n",
       "L 74.646619 136.894278 \n",
       "L 74.936765 136.901154 \n",
       "L 75.517058 144.900684 \n",
       "L 75.807204 146.75741 \n",
       "L 76.097351 150.161445 \n",
       "L 76.387497 146.13934 \n",
       "L 76.677644 154.401199 \n",
       "L 76.96779 142.831723 \n",
       "L 77.257937 155.026939 \n",
       "L 77.548083 161.472945 \n",
       "L 77.83823 158.627724 \n",
       "L 78.128376 163.688945 \n",
       "L 78.708669 160.677501 \n",
       "L 78.998815 157.06137 \n",
       "L 79.288962 164.219411 \n",
       "L 79.579108 166.89229 \n",
       "L 79.869255 167.517121 \n",
       "L 80.739694 171.185201 \n",
       "L 81.029841 174.904752 \n",
       "L 81.319987 165.72818 \n",
       "L 81.610134 176.409823 \n",
       "L 81.90028 179.3826 \n",
       "L 82.190426 175.581983 \n",
       "L 82.480573 176.170567 \n",
       "L 82.770719 177.48279 \n",
       "L 83.060866 166.412045 \n",
       "L 83.351012 170.886429 \n",
       "L 83.641159 180.317059 \n",
       "L 83.931305 181.403508 \n",
       "L 84.221452 174.933882 \n",
       "L 84.511598 171.517767 \n",
       "L 84.801745 182.029365 \n",
       "L 85.091891 184.262722 \n",
       "L 85.382038 184.876042 \n",
       "L 85.672184 187.751571 \n",
       "L 85.96233 187.91211 \n",
       "L 86.252477 189.353867 \n",
       "L 86.542623 185.720483 \n",
       "L 87.122916 190.531497 \n",
       "L 87.413063 182.22953 \n",
       "L 87.703209 192.729087 \n",
       "L 87.993356 189.29646 \n",
       "L 88.283502 189.460487 \n",
       "L 88.573649 183.335414 \n",
       "L 88.863795 190.566409 \n",
       "L 89.153942 190.522241 \n",
       "L 89.444088 193.168559 \n",
       "L 89.734234 193.342076 \n",
       "L 90.024381 190.429747 \n",
       "L 90.314527 194.773739 \n",
       "L 90.604674 193.904521 \n",
       "L 90.89482 187.596444 \n",
       "L 91.184967 184.583554 \n",
       "L 91.475113 187.775868 \n",
       "L 91.76526 184.532784 \n",
       "L 92.055406 195.753621 \n",
       "L 92.345553 196.799572 \n",
       "L 92.635699 187.556226 \n",
       "L 92.925846 190.433757 \n",
       "L 93.215992 197.159688 \n",
       "L 93.506138 175.967952 \n",
       "L 93.796285 198.822628 \n",
       "L 94.086431 190.329752 \n",
       "L 94.376578 202.653191 \n",
       "L 94.666724 199.228465 \n",
       "L 94.956871 200.400379 \n",
       "L 95.247017 200.750769 \n",
       "L 95.537164 202.610942 \n",
       "L 95.82731 191.920355 \n",
       "L 96.117457 191.975764 \n",
       "L 96.407603 201.749471 \n",
       "L 96.697749 199.395031 \n",
       "L 96.987896 202.35751 \n",
       "L 97.278042 201.573548 \n",
       "L 97.568189 202.090571 \n",
       "L 97.858335 204.084488 \n",
       "L 98.148482 204.032432 \n",
       "L 98.438628 205.285763 \n",
       "L 98.728775 199.994949 \n",
       "L 99.018921 202.32079 \n",
       "L 99.309068 203.727816 \n",
       "L 99.599214 192.069455 \n",
       "L 99.889361 196.974415 \n",
       "L 100.179507 193.618881 \n",
       "L 100.469653 203.510586 \n",
       "L 100.7598 196.752653 \n",
       "L 101.049946 203.740856 \n",
       "L 101.340093 204.76611 \n",
       "L 101.630239 203.455778 \n",
       "L 102.210532 204.537406 \n",
       "L 102.500679 206.333018 \n",
       "L 102.790825 207.092602 \n",
       "L 103.080972 205.565859 \n",
       "L 103.371118 198.962438 \n",
       "L 103.661265 207.094383 \n",
       "L 103.951411 205.535545 \n",
       "L 104.241557 206.625774 \n",
       "L 104.531704 206.126748 \n",
       "L 104.82185 208.685844 \n",
       "L 105.111997 208.550392 \n",
       "L 105.402143 208.629103 \n",
       "L 105.69229 206.52922 \n",
       "L 106.272583 208.219396 \n",
       "L 106.562729 207.458487 \n",
       "L 106.852876 207.816857 \n",
       "L 107.143022 210.100912 \n",
       "L 107.433168 208.940775 \n",
       "L 107.723315 210.211445 \n",
       "L 108.013461 194.829192 \n",
       "L 108.303608 208.437985 \n",
       "L 108.593754 196.615789 \n",
       "L 108.883901 205.414875 \n",
       "L 109.174047 210.378692 \n",
       "L 109.464194 196.611404 \n",
       "L 109.75434 210.687323 \n",
       "L 110.044487 209.667178 \n",
       "L 110.334633 199.639354 \n",
       "L 110.62478 209.529931 \n",
       "L 110.914926 200.034341 \n",
       "L 111.205072 208.854103 \n",
       "L 111.495219 209.419036 \n",
       "L 111.785365 211.726306 \n",
       "L 112.075512 210.25637 \n",
       "L 112.365658 210.106855 \n",
       "L 112.655805 213.004921 \n",
       "L 112.945951 212.418186 \n",
       "L 113.236098 203.778151 \n",
       "L 113.526244 210.00939 \n",
       "L 113.816391 211.807789 \n",
       "L 114.396684 208.297852 \n",
       "L 114.68683 210.255784 \n",
       "L 114.976976 210.565823 \n",
       "L 115.267123 212.971491 \n",
       "L 115.557269 207.328676 \n",
       "L 115.847416 212.863682 \n",
       "L 116.137562 208.006002 \n",
       "L 116.427709 214.206271 \n",
       "L 116.717855 214.287619 \n",
       "L 117.008002 214.569782 \n",
       "L 117.298148 212.891136 \n",
       "L 117.588295 214.554154 \n",
       "L 117.878441 207.156213 \n",
       "L 118.458734 214.725403 \n",
       "L 118.74888 207.857734 \n",
       "L 119.039027 211.9193 \n",
       "L 119.329173 214.391341 \n",
       "L 119.61932 213.657564 \n",
       "L 119.909466 214.465057 \n",
       "L 120.199613 208.630175 \n",
       "L 120.489759 208.765934 \n",
       "L 120.779906 216.039369 \n",
       "L 121.070052 215.065817 \n",
       "L 121.360199 216.03354 \n",
       "L 121.650345 209.951583 \n",
       "L 121.940491 213.73116 \n",
       "L 122.230638 211.892452 \n",
       "L 122.520784 217.306213 \n",
       "L 122.810931 210.806703 \n",
       "L 123.101077 214.954229 \n",
       "L 123.391224 211.473449 \n",
       "L 123.68137 216.921241 \n",
       "L 123.971517 217.710074 \n",
       "L 124.261663 215.396507 \n",
       "L 124.55181 216.220138 \n",
       "L 124.841956 219.411071 \n",
       "L 125.132103 215.462901 \n",
       "L 125.422249 219.491519 \n",
       "L 125.712395 218.553901 \n",
       "L 126.002542 219.272974 \n",
       "L 126.292688 214.855936 \n",
       "L 126.582835 219.771798 \n",
       "L 126.872981 216.6948 \n",
       "L 127.163128 219.566503 \n",
       "L 127.453274 216.503722 \n",
       "L 127.743421 216.665421 \n",
       "L 128.033567 221.574747 \n",
       "L 128.323714 219.957912 \n",
       "L 128.61386 217.472229 \n",
       "L 128.904007 219.30886 \n",
       "L 129.194153 218.745343 \n",
       "L 129.484299 221.731214 \n",
       "L 130.064592 219.482247 \n",
       "L 130.354739 222.218505 \n",
       "L 130.644885 222.921855 \n",
       "L 130.935032 218.935585 \n",
       "L 131.225178 222.07761 \n",
       "L 131.515325 222.724503 \n",
       "L 131.805471 219.397074 \n",
       "L 132.095618 222.517512 \n",
       "L 132.385764 223.259214 \n",
       "L 132.67591 217.200829 \n",
       "L 132.966057 222.793818 \n",
       "L 133.256203 219.110601 \n",
       "L 133.836496 223.153221 \n",
       "L 134.416789 219.674377 \n",
       "L 134.706936 223.267432 \n",
       "L 134.997082 223.222663 \n",
       "L 135.287229 219.759745 \n",
       "L 135.577375 223.747237 \n",
       "L 135.867522 224.918186 \n",
       "L 136.447814 223.210075 \n",
       "L 136.737961 224.482655 \n",
       "L 137.028107 223.308285 \n",
       "L 137.318254 220.820191 \n",
       "L 137.6084 223.604201 \n",
       "L 137.898547 223.910288 \n",
       "L 138.188693 224.771244 \n",
       "L 138.47884 220.399763 \n",
       "L 138.768986 223.275092 \n",
       "L 139.059133 222.529221 \n",
       "L 139.349279 224.757653 \n",
       "L 139.639426 222.991066 \n",
       "L 139.929572 225.833431 \n",
       "L 140.219718 224.425504 \n",
       "L 140.509865 225.529969 \n",
       "L 140.800011 221.314361 \n",
       "L 141.090158 223.992763 \n",
       "L 141.380304 222.743897 \n",
       "L 141.670451 225.639828 \n",
       "L 142.250744 225.46836 \n",
       "L 142.54089 225.116747 \n",
       "L 142.831037 223.825841 \n",
       "L 143.121183 225.321772 \n",
       "L 143.41133 222.229691 \n",
       "L 143.701476 224.438632 \n",
       "L 143.991622 224.087693 \n",
       "L 144.281769 225.444077 \n",
       "L 144.571915 225.671734 \n",
       "L 144.862062 222.981986 \n",
       "L 145.152208 226.299351 \n",
       "L 145.732501 222.849552 \n",
       "L 146.022648 224.435359 \n",
       "L 146.312794 225.064894 \n",
       "L 146.602941 221.472642 \n",
       "L 146.893087 223.144152 \n",
       "L 147.183233 222.623237 \n",
       "L 147.47338 226.563715 \n",
       "L 147.763526 224.643826 \n",
       "L 148.053673 225.013664 \n",
       "L 148.343819 225.882737 \n",
       "L 148.633966 222.492641 \n",
       "L 148.924112 225.853519 \n",
       "L 149.214259 226.318566 \n",
       "L 149.794552 224.031568 \n",
       "L 150.084698 224.005207 \n",
       "L 150.374845 227.004641 \n",
       "L 150.664991 224.77323 \n",
       "L 150.955137 226.704723 \n",
       "L 151.245284 226.130812 \n",
       "L 151.53543 225.022468 \n",
       "L 151.825577 226.286993 \n",
       "L 152.115723 223.501409 \n",
       "L 152.40587 226.659119 \n",
       "L 152.696016 224.396357 \n",
       "L 152.986163 226.98036 \n",
       "L 153.276309 221.390883 \n",
       "L 153.566456 225.919637 \n",
       "L 153.856602 225.907615 \n",
       "L 154.146749 225.524908 \n",
       "L 154.436895 226.382058 \n",
       "L 154.727041 223.098121 \n",
       "L 155.017188 226.523692 \n",
       "L 155.307334 225.819328 \n",
       "L 155.597481 223.805623 \n",
       "L 155.887627 226.58813 \n",
       "L 156.177774 224.464682 \n",
       "L 156.46792 226.796443 \n",
       "L 156.758067 225.081395 \n",
       "L 157.048213 225.647222 \n",
       "L 157.33836 227.250931 \n",
       "L 157.628506 227.694227 \n",
       "L 157.918653 224.530898 \n",
       "L 158.208799 227.193007 \n",
       "L 158.789092 225.611887 \n",
       "L 159.079238 225.968651 \n",
       "L 159.369385 222.570537 \n",
       "L 159.659531 226.000654 \n",
       "L 159.949678 225.213384 \n",
       "L 160.239824 228.077331 \n",
       "L 160.529971 225.976824 \n",
       "L 160.820117 224.958507 \n",
       "L 161.40041 227.648136 \n",
       "L 161.690556 226.445774 \n",
       "L 161.980703 224.710123 \n",
       "L 162.270849 227.522132 \n",
       "L 162.851142 226.893226 \n",
       "L 163.141289 227.692118 \n",
       "L 163.721582 227.157692 \n",
       "L 164.011728 225.916299 \n",
       "L 164.301875 226.867857 \n",
       "L 164.592021 224.470594 \n",
       "L 164.882168 227.092619 \n",
       "L 165.172314 227.823188 \n",
       "L 165.46246 227.962702 \n",
       "L 165.752607 228.266901 \n",
       "L 166.042753 227.008679 \n",
       "L 166.3329 227.239041 \n",
       "L 166.623046 225.932741 \n",
       "L 166.913193 225.62226 \n",
       "L 167.203339 227.107847 \n",
       "L 167.493486 227.800192 \n",
       "L 167.783632 226.424521 \n",
       "L 168.073779 225.899804 \n",
       "L 168.363925 225.064214 \n",
       "L 168.654072 225.451393 \n",
       "L 168.944218 226.749213 \n",
       "L 169.234364 223.254886 \n",
       "L 169.524511 226.806575 \n",
       "L 169.814657 225.093779 \n",
       "L 170.104804 228.874075 \n",
       "L 170.39495 225.675863 \n",
       "L 170.685097 225.656499 \n",
       "L 170.975243 227.624927 \n",
       "L 171.26539 227.391396 \n",
       "L 171.555536 227.923549 \n",
       "L 171.845683 223.642553 \n",
       "L 172.135829 226.992524 \n",
       "L 172.425975 227.018086 \n",
       "L 172.716122 227.158883 \n",
       "L 173.006268 228.76486 \n",
       "L 173.296415 225.942621 \n",
       "L 173.586561 227.739688 \n",
       "L 173.876708 225.331884 \n",
       "L 174.166854 225.045386 \n",
       "L 174.457001 227.329944 \n",
       "L 174.747147 227.567946 \n",
       "L 175.037294 227.178585 \n",
       "L 175.32744 228.701483 \n",
       "L 175.617587 229.151539 \n",
       "L 175.907733 223.565475 \n",
       "L 176.197879 227.91014 \n",
       "L 176.488026 228.081648 \n",
       "L 176.778172 228.453133 \n",
       "L 177.068319 226.341649 \n",
       "L 177.358465 227.256309 \n",
       "L 177.648612 225.2584 \n",
       "L 177.938758 227.919715 \n",
       "L 178.228905 226.454947 \n",
       "L 178.809198 227.183086 \n",
       "L 179.099344 228.982049 \n",
       "L 179.389491 227.585422 \n",
       "L 179.679637 227.339147 \n",
       "L 179.969783 229.317297 \n",
       "L 180.25993 228.908747 \n",
       "L 180.550076 227.46332 \n",
       "L 180.840223 227.645559 \n",
       "L 181.130369 227.020849 \n",
       "L 181.420516 226.934524 \n",
       "L 181.710662 227.817367 \n",
       "L 182.290955 228.009648 \n",
       "L 182.581102 229.216697 \n",
       "L 182.871248 226.47376 \n",
       "L 183.161395 226.832316 \n",
       "L 183.451541 228.439985 \n",
       "L 183.741687 228.284922 \n",
       "L 184.031834 229.12745 \n",
       "L 184.32198 228.484033 \n",
       "L 184.612127 229.116501 \n",
       "L 185.19242 226.182641 \n",
       "L 185.772713 228.146376 \n",
       "L 186.062859 227.166485 \n",
       "L 186.353006 223.873806 \n",
       "L 186.643152 228.288861 \n",
       "L 186.933298 227.35579 \n",
       "L 187.223445 227.958211 \n",
       "L 187.513591 229.408346 \n",
       "L 187.803738 225.122413 \n",
       "L 188.093884 228.412506 \n",
       "L 188.674177 228.79503 \n",
       "L 188.964324 227.978721 \n",
       "L 189.25447 228.477276 \n",
       "L 189.544617 226.211034 \n",
       "L 189.834763 226.08203 \n",
       "L 190.12491 228.641701 \n",
       "L 190.415056 226.274585 \n",
       "L 190.705202 225.442851 \n",
       "L 190.995349 228.363925 \n",
       "L 191.285495 229.07173 \n",
       "L 191.865788 226.517143 \n",
       "L 192.155935 228.686929 \n",
       "L 192.446081 228.969061 \n",
       "L 192.736228 226.651918 \n",
       "L 193.026374 226.997603 \n",
       "L 193.316521 228.806384 \n",
       "L 193.606667 228.258601 \n",
       "L 193.896814 229.392318 \n",
       "L 194.18696 225.863704 \n",
       "L 194.477106 228.711095 \n",
       "L 194.767253 227.0017 \n",
       "L 195.057399 227.661359 \n",
       "L 195.347546 226.923499 \n",
       "L 195.637692 229.118719 \n",
       "L 195.927839 228.106924 \n",
       "L 196.217985 224.802603 \n",
       "L 196.508132 228.644322 \n",
       "L 196.798278 227.942948 \n",
       "L 197.088425 226.400261 \n",
       "L 197.378571 227.541143 \n",
       "L 197.958864 228.087763 \n",
       "L 198.24901 227.76616 \n",
       "L 198.539157 227.792461 \n",
       "L 199.409596 226.812652 \n",
       "L 199.699743 228.290345 \n",
       "L 199.989889 226.955799 \n",
       "L 200.280036 226.965137 \n",
       "L 200.570182 228.169886 \n",
       "L 200.860329 228.435646 \n",
       "L 201.150475 225.736966 \n",
       "L 201.440621 228.85033 \n",
       "L 201.730768 227.931582 \n",
       "L 202.020914 226.458531 \n",
       "L 202.311061 228.0696 \n",
       "L 202.601207 228.436845 \n",
       "L 202.891354 227.110765 \n",
       "L 203.1815 228.369374 \n",
       "L 203.471647 227.928129 \n",
       "L 203.761793 228.936382 \n",
       "L 204.05194 228.753062 \n",
       "L 204.342086 229.14761 \n",
       "L 204.632233 226.344374 \n",
       "L 204.922379 228.834885 \n",
       "L 205.212525 227.378172 \n",
       "L 205.502672 229.806306 \n",
       "L 205.792818 224.147916 \n",
       "L 206.082965 229.142987 \n",
       "L 206.373111 226.002667 \n",
       "L 206.663258 228.08294 \n",
       "L 206.953404 226.847622 \n",
       "L 207.243551 227.129801 \n",
       "L 207.533697 226.333283 \n",
       "L 207.823844 226.923388 \n",
       "L 208.11399 228.865435 \n",
       "L 208.404137 227.312762 \n",
       "L 208.694283 229.476859 \n",
       "L 208.984429 227.418871 \n",
       "L 209.564722 228.041264 \n",
       "L 209.854869 226.686563 \n",
       "L 210.435162 228.516232 \n",
       "L 210.725308 227.57647 \n",
       "L 211.015455 226.194408 \n",
       "L 211.305601 227.547768 \n",
       "L 211.595748 227.593784 \n",
       "L 211.885894 228.989705 \n",
       "L 212.17604 228.395395 \n",
       "L 212.466187 228.123941 \n",
       "L 212.756333 228.651644 \n",
       "L 213.336626 227.042707 \n",
       "L 213.626773 229.085693 \n",
       "L 213.916919 225.867105 \n",
       "L 214.207066 228.625299 \n",
       "L 214.497212 228.618273 \n",
       "L 215.077505 225.32511 \n",
       "L 215.367652 228.727791 \n",
       "L 215.657798 227.343569 \n",
       "L 215.947944 229.582739 \n",
       "L 216.528237 226.248798 \n",
       "L 216.818384 227.774701 \n",
       "L 217.10853 227.465199 \n",
       "L 217.398677 228.781492 \n",
       "L 217.688823 227.647825 \n",
       "L 217.97897 229.114414 \n",
       "L 218.269116 228.083924 \n",
       "L 218.559263 229.566373 \n",
       "L 218.849409 227.256016 \n",
       "L 219.139556 228.660254 \n",
       "L 219.719848 228.366983 \n",
       "L 220.009995 228.401748 \n",
       "L 220.300141 227.441 \n",
       "L 220.590288 228.280613 \n",
       "L 220.880434 227.551137 \n",
       "L 221.170581 227.692069 \n",
       "L 221.460727 227.286293 \n",
       "L 221.750874 226.120087 \n",
       "L 222.04102 228.084247 \n",
       "L 222.331167 226.908041 \n",
       "L 222.621313 227.098159 \n",
       "L 222.91146 228.875559 \n",
       "L 223.201606 228.258944 \n",
       "L 223.491752 229.076042 \n",
       "L 223.781899 228.246536 \n",
       "L 224.072045 228.856066 \n",
       "L 224.362192 227.088961 \n",
       "L 224.652338 226.683791 \n",
       "L 224.942485 228.216523 \n",
       "L 225.232631 227.204679 \n",
       "L 225.522778 228.744179 \n",
       "L 225.812924 228.585902 \n",
       "L 226.103071 228.961122 \n",
       "L 226.683363 227.621936 \n",
       "L 226.97351 228.637068 \n",
       "L 227.263656 227.550394 \n",
       "L 227.553803 227.972696 \n",
       "L 227.843949 227.217633 \n",
       "L 228.134096 228.112603 \n",
       "L 228.424242 228.502337 \n",
       "L 228.714389 227.574076 \n",
       "L 229.004535 228.76933 \n",
       "L 229.294682 227.88805 \n",
       "L 229.584828 228.171995 \n",
       "L 229.874975 228.7737 \n",
       "L 230.165121 228.9501 \n",
       "L 230.455267 227.296531 \n",
       "L 230.745414 228.85823 \n",
       "L 231.03556 228.51117 \n",
       "L 231.325707 226.928254 \n",
       "L 231.615853 228.684549 \n",
       "L 231.906 229.667121 \n",
       "L 232.196146 225.283975 \n",
       "L 232.486293 228.47626 \n",
       "L 232.776439 228.27597 \n",
       "L 233.066586 229.030308 \n",
       "L 233.937025 228.865889 \n",
       "L 234.227171 229.429212 \n",
       "L 234.517318 228.463401 \n",
       "L 234.807464 227.894775 \n",
       "L 235.097611 227.917734 \n",
       "L 235.387757 228.670791 \n",
       "L 235.677904 227.256391 \n",
       "L 235.96805 228.518567 \n",
       "L 236.258197 228.306013 \n",
       "L 236.548343 227.844028 \n",
       "L 236.83849 223.848673 \n",
       "L 237.128636 228.385867 \n",
       "L 237.418782 226.175129 \n",
       "L 237.708929 228.268714 \n",
       "L 237.999075 228.701209 \n",
       "L 238.289222 228.589464 \n",
       "L 238.579368 229.407652 \n",
       "L 238.869515 226.457163 \n",
       "L 239.159661 228.384626 \n",
       "L 239.449808 227.3776 \n",
       "L 239.739954 228.358895 \n",
       "L 240.030101 226.530841 \n",
       "L 240.320247 227.954649 \n",
       "L 240.610394 227.608933 \n",
       "L 240.90054 228.249675 \n",
       "L 241.190686 226.554024 \n",
       "L 241.480833 228.718118 \n",
       "L 241.770979 229.147535 \n",
       "L 242.061126 225.557586 \n",
       "L 242.351272 227.582719 \n",
       "L 242.641419 227.034974 \n",
       "L 242.931565 226.920449 \n",
       "L 243.221712 227.685841 \n",
       "L 243.511858 227.153341 \n",
       "L 243.802005 227.488642 \n",
       "L 244.092151 228.315656 \n",
       "L 244.382298 226.92882 \n",
       "L 244.672444 229.410048 \n",
       "L 244.96259 228.297146 \n",
       "L 245.252737 229.644265 \n",
       "L 245.542883 227.637292 \n",
       "L 245.83303 228.011315 \n",
       "L 246.413323 227.633091 \n",
       "L 246.703469 224.559254 \n",
       "L 246.993616 227.167928 \n",
       "L 247.283762 228.257092 \n",
       "L 247.573909 228.732778 \n",
       "L 248.154202 227.874805 \n",
       "L 248.444348 229.118945 \n",
       "L 248.734494 227.565411 \n",
       "L 249.024641 226.934461 \n",
       "L 249.314787 227.682959 \n",
       "L 249.604934 228.013052 \n",
       "L 249.89508 227.276218 \n",
       "L 250.185227 229.028571 \n",
       "L 250.475373 228.723756 \n",
       "L 250.76552 227.793253 \n",
       "L 251.055666 228.359118 \n",
       "L 251.635959 227.581914 \n",
       "L 251.926105 228.589451 \n",
       "L 252.216252 227.321538 \n",
       "L 252.506398 229.647065 \n",
       "L 252.796545 226.372563 \n",
       "L 253.086691 228.029092 \n",
       "L 253.376838 229.028907 \n",
       "L 254.247277 228.024035 \n",
       "L 254.537424 229.277946 \n",
       "L 254.82757 228.616324 \n",
       "L 255.117717 229.43876 \n",
       "L 255.698009 227.809824 \n",
       "L 255.988156 229.132368 \n",
       "L 256.278302 229.400234 \n",
       "L 256.568449 229.120537 \n",
       "L 256.858595 226.830069 \n",
       "L 257.148742 227.603158 \n",
       "L 257.438888 229.568741 \n",
       "L 258.019181 228.750518 \n",
       "L 258.309328 228.677792 \n",
       "L 258.889621 227.456578 \n",
       "L 259.179767 228.332838 \n",
       "L 259.469913 225.904147 \n",
       "L 259.76006 228.927867 \n",
       "L 260.050206 228.20797 \n",
       "L 260.630499 229.17852 \n",
       "L 260.920646 228.686061 \n",
       "L 261.210792 228.904919 \n",
       "L 261.500939 228.056983 \n",
       "L 261.791085 228.007283 \n",
       "L 262.081232 226.497864 \n",
       "L 262.371378 228.449733 \n",
       "L 262.661525 228.541025 \n",
       "L 262.951671 227.648856 \n",
       "L 263.241817 227.646326 \n",
       "L 263.531964 228.187256 \n",
       "L 263.82211 228.296636 \n",
       "L 264.112257 225.853877 \n",
       "L 264.402403 228.554988 \n",
       "L 264.69255 229.076587 \n",
       "L 264.982696 228.960455 \n",
       "L 265.272843 227.116031 \n",
       "L 265.562989 226.806692 \n",
       "L 265.853136 226.047519 \n",
       "L 266.143282 226.827386 \n",
       "L 266.433428 226.844609 \n",
       "L 266.723575 228.26707 \n",
       "L 267.013721 228.457172 \n",
       "L 267.303868 228.949211 \n",
       "L 267.594014 227.969645 \n",
       "L 267.884161 227.933352 \n",
       "L 268.174307 229.807115 \n",
       "L 268.464454 228.757147 \n",
       "L 268.7546 228.177578 \n",
       "L 269.044747 228.434409 \n",
       "L 269.334893 228.950675 \n",
       "L 269.62504 228.148515 \n",
       "L 269.915186 228.851513 \n",
       "L 270.205332 228.989726 \n",
       "L 270.495479 226.612153 \n",
       "L 270.785625 228.942184 \n",
       "L 271.075772 229.226276 \n",
       "L 271.365918 227.787876 \n",
       "L 271.656065 228.831563 \n",
       "L 271.946211 226.902744 \n",
       "L 272.236358 226.941325 \n",
       "L 272.526504 228.801354 \n",
       "L 272.816651 229.682025 \n",
       "L 273.396944 227.282231 \n",
       "L 273.68709 227.674806 \n",
       "L 273.977236 229.163159 \n",
       "L 274.557529 226.461984 \n",
       "L 274.847676 227.592555 \n",
       "L 275.137822 227.736625 \n",
       "L 275.427969 228.803797 \n",
       "L 275.718115 227.323088 \n",
       "L 276.008262 227.877304 \n",
       "L 276.298408 228.188781 \n",
       "L 276.588555 227.939795 \n",
       "L 276.878701 228.172162 \n",
       "L 277.168847 228.767092 \n",
       "L 277.458994 227.509549 \n",
       "L 277.74914 228.792161 \n",
       "L 278.039287 228.360796 \n",
       "L 278.329433 228.622417 \n",
       "L 278.61958 225.97335 \n",
       "L 278.909726 228.871679 \n",
       "L 279.199873 228.62572 \n",
       "L 279.490019 227.825274 \n",
       "L 279.780166 227.619266 \n",
       "L 280.070312 226.456416 \n",
       "L 280.360459 227.919921 \n",
       "L 280.650605 227.746671 \n",
       "L 280.940751 229.404192 \n",
       "L 281.521044 227.525632 \n",
       "L 281.811191 227.865354 \n",
       "L 282.101337 227.806204 \n",
       "L 282.391484 228.510947 \n",
       "L 282.68163 228.564648 \n",
       "L 282.971777 226.879288 \n",
       "L 283.261923 228.32757 \n",
       "L 283.55207 228.767211 \n",
       "L 283.842216 228.590508 \n",
       "L 284.132363 225.654436 \n",
       "L 284.422509 227.895205 \n",
       "L 284.712655 228.208034 \n",
       "L 285.002802 226.749545 \n",
       "L 285.292948 227.392523 \n",
       "L 285.583095 228.36185 \n",
       "L 285.873241 228.259838 \n",
       "L 286.163388 228.745734 \n",
       "L 286.453534 228.989251 \n",
       "L 286.743681 228.673867 \n",
       "L 287.033827 229.153593 \n",
       "L 287.323974 228.5178 \n",
       "L 287.904267 229.553676 \n",
       "L 288.194413 227.217655 \n",
       "L 288.484559 227.985593 \n",
       "L 288.774706 225.636039 \n",
       "L 289.064852 228.859062 \n",
       "L 289.354999 227.804348 \n",
       "L 289.645145 227.839542 \n",
       "L 289.935292 227.371194 \n",
       "L 290.225438 228.358432 \n",
       "L 290.515585 222.907635 \n",
       "L 290.805731 228.192546 \n",
       "L 291.095878 224.887762 \n",
       "L 291.386024 226.456047 \n",
       "L 291.67617 226.141702 \n",
       "L 291.966317 228.954055 \n",
       "L 292.256463 229.227992 \n",
       "L 292.54661 228.873821 \n",
       "L 292.836756 229.002724 \n",
       "L 293.126903 229.328922 \n",
       "L 293.417049 228.194923 \n",
       "L 293.707196 228.361184 \n",
       "L 293.997342 229.238374 \n",
       "L 294.287489 229.315175 \n",
       "L 294.577635 228.522036 \n",
       "L 294.867782 227.266865 \n",
       "L 295.157928 228.909286 \n",
       "L 295.448074 228.75967 \n",
       "L 295.738221 229.22896 \n",
       "L 296.028367 228.452812 \n",
       "L 296.318514 228.227054 \n",
       "L 296.60866 226.640015 \n",
       "L 296.898807 228.834685 \n",
       "L 297.188953 229.345524 \n",
       "L 297.4791 228.762592 \n",
       "L 297.769246 226.877363 \n",
       "L 298.059393 229.874489 \n",
       "L 298.349539 228.246244 \n",
       "L 298.639686 228.157389 \n",
       "L 298.929832 227.158733 \n",
       "L 299.219978 228.318819 \n",
       "L 299.510125 227.717253 \n",
       "L 299.800271 228.515787 \n",
       "L 300.090418 228.395096 \n",
       "L 300.380564 228.553913 \n",
       "L 300.960857 226.581521 \n",
       "L 301.251004 229.24674 \n",
       "L 301.54115 228.96903 \n",
       "L 301.831297 228.943731 \n",
       "L 302.41159 228.405751 \n",
       "L 302.701736 224.657365 \n",
       "L 302.991882 228.434265 \n",
       "L 303.572175 229.349936 \n",
       "L 303.862322 225.90055 \n",
       "L 304.152468 228.217841 \n",
       "L 304.442615 227.383954 \n",
       "L 304.732761 228.481124 \n",
       "L 305.022908 227.481119 \n",
       "L 305.313054 228.152218 \n",
       "L 305.603201 227.449595 \n",
       "L 305.893347 228.123666 \n",
       "L 306.183493 228.17281 \n",
       "L 306.47364 227.267665 \n",
       "L 306.763786 226.949888 \n",
       "L 307.053933 226.864439 \n",
       "L 307.344079 228.553323 \n",
       "L 307.634226 227.968178 \n",
       "L 307.924372 229.2642 \n",
       "L 308.214519 229.072854 \n",
       "L 308.504665 229.223644 \n",
       "L 308.794812 228.706655 \n",
       "L 309.084958 227.442075 \n",
       "L 309.375105 227.139248 \n",
       "L 309.665251 227.873397 \n",
       "L 309.955397 229.287522 \n",
       "L 310.245544 227.94273 \n",
       "L 310.53569 229.824066 \n",
       "L 310.825837 226.121423 \n",
       "L 311.115983 229.605439 \n",
       "L 311.40613 228.821272 \n",
       "L 311.696276 228.896718 \n",
       "L 311.986423 227.951107 \n",
       "L 312.276569 228.08027 \n",
       "L 312.566716 228.902317 \n",
       "L 312.856862 228.320972 \n",
       "L 313.437155 228.527426 \n",
       "L 313.727301 229.436191 \n",
       "L 314.017448 228.592391 \n",
       "L 314.307594 227.170733 \n",
       "L 314.597741 228.725153 \n",
       "L 314.887887 228.339033 \n",
       "L 315.178034 228.617099 \n",
       "L 315.46818 229.163159 \n",
       "L 315.758327 228.869916 \n",
       "L 316.048473 228.277036 \n",
       "L 316.33862 229.015783 \n",
       "L 316.628766 228.842857 \n",
       "L 316.918912 228.866686 \n",
       "L 317.209059 227.403136 \n",
       "L 317.499205 228.38508 \n",
       "L 317.789352 227.799177 \n",
       "L 318.079498 229.455034 \n",
       "L 318.369645 228.395034 \n",
       "L 318.659791 228.76932 \n",
       "L 318.949938 227.420371 \n",
       "L 319.240084 229.161342 \n",
       "L 319.530231 227.890889 \n",
       "L 319.820377 228.751872 \n",
       "L 320.110524 227.36853 \n",
       "L 320.40067 227.433374 \n",
       "L 320.690816 228.533671 \n",
       "L 320.980963 226.766122 \n",
       "L 321.271109 228.247846 \n",
       "L 321.561256 228.28561 \n",
       "L 321.851402 228.554549 \n",
       "L 322.141549 227.311698 \n",
       "L 322.431695 228.201347 \n",
       "L 322.721842 228.163132 \n",
       "L 323.011988 228.320095 \n",
       "L 323.302135 227.511213 \n",
       "L 323.592281 229.004985 \n",
       "L 323.882428 229.072153 \n",
       "L 324.172574 228.021481 \n",
       "L 324.46272 228.939711 \n",
       "L 325.043013 226.057355 \n",
       "L 325.33316 228.784333 \n",
       "L 325.623306 227.539175 \n",
       "L 325.913453 229.10952 \n",
       "L 326.203599 228.077586 \n",
       "L 326.493746 228.40114 \n",
       "L 326.783892 224.890794 \n",
       "L 327.074039 229.078299 \n",
       "L 327.364185 228.07403 \n",
       "L 327.654332 229.404042 \n",
       "L 328.234624 227.300291 \n",
       "L 328.524771 227.676408 \n",
       "L 328.814917 226.429525 \n",
       "L 329.105064 228.136524 \n",
       "L 329.39521 228.982007 \n",
       "L 329.685357 226.513936 \n",
       "L 329.975503 228.867999 \n",
       "L 330.26565 227.299235 \n",
       "L 330.555796 229.208214 \n",
       "L 330.845943 227.660102 \n",
       "L 331.136089 228.586183 \n",
       "L 331.426235 228.065345 \n",
       "L 331.716382 228.565153 \n",
       "L 332.006528 229.558974 \n",
       "L 332.296675 227.438114 \n",
       "L 332.586821 228.286841 \n",
       "L 332.876968 228.059362 \n",
       "L 333.457261 228.565959 \n",
       "L 333.747407 229.397847 \n",
       "L 334.037554 228.317288 \n",
       "L 334.3277 228.724599 \n",
       "L 334.617847 228.055214 \n",
       "L 334.907993 227.937148 \n",
       "L 335.198139 227.356593 \n",
       "L 335.488286 228.271915 \n",
       "L 335.778432 226.863632 \n",
       "L 336.068579 228.294499 \n",
       "L 336.358725 227.374743 \n",
       "L 336.648872 229.323025 \n",
       "L 336.939018 229.360448 \n",
       "L 337.229165 229.078151 \n",
       "L 337.519311 228.005637 \n",
       "L 337.809458 227.969037 \n",
       "L 338.099604 228.867203 \n",
       "L 338.389751 227.283832 \n",
       "L 338.679897 229.445066 \n",
       "L 338.970043 228.343548 \n",
       "L 339.26019 228.055491 \n",
       "L 339.550336 228.626349 \n",
       "L 339.840483 227.299604 \n",
       "L 340.130629 228.849732 \n",
       "L 340.420776 228.455575 \n",
       "L 340.710922 228.513266 \n",
       "L 341.001069 229.61155 \n",
       "L 341.291215 228.780884 \n",
       "L 341.581362 228.494488 \n",
       "L 341.871508 229.253347 \n",
       "L 342.161654 228.37212 \n",
       "L 342.451801 228.134253 \n",
       "L 342.741947 229.209894 \n",
       "L 343.032094 227.532155 \n",
       "L 343.32224 227.700106 \n",
       "L 343.612387 228.655359 \n",
       "L 343.902533 228.77918 \n",
       "L 344.19268 229.365965 \n",
       "L 344.482826 228.327896 \n",
       "L 344.772973 228.316853 \n",
       "L 345.063119 228.086213 \n",
       "L 345.353266 228.149931 \n",
       "L 345.643412 228.003314 \n",
       "L 345.933558 229.246521 \n",
       "L 346.223705 228.013291 \n",
       "L 346.513851 228.771846 \n",
       "L 346.803998 229.197066 \n",
       "L 347.094144 228.269679 \n",
       "L 347.384291 229.226345 \n",
       "L 347.674437 227.711521 \n",
       "L 347.964584 227.700863 \n",
       "L 348.25473 227.524425 \n",
       "L 348.544877 228.428819 \n",
       "L 348.835023 228.272559 \n",
       "L 349.12517 227.871837 \n",
       "L 349.415316 229.309365 \n",
       "L 349.705462 227.911547 \n",
       "L 349.995609 229.216106 \n",
       "L 350.285755 228.181148 \n",
       "L 350.575902 228.109278 \n",
       "L 350.866048 227.777968 \n",
       "L 351.156195 228.656976 \n",
       "L 351.736488 229.640953 \n",
       "L 352.026634 228.104594 \n",
       "L 352.316781 227.892055 \n",
       "L 352.606927 228.625103 \n",
       "L 353.18722 229.288883 \n",
       "L 353.477366 228.797735 \n",
       "L 353.767513 228.939133 \n",
       "L 354.057659 225.719139 \n",
       "L 354.347806 227.421349 \n",
       "L 354.637952 227.255033 \n",
       "L 354.928099 228.451327 \n",
       "L 355.218245 228.548303 \n",
       "L 355.508392 227.003263 \n",
       "L 355.798538 229.071974 \n",
       "L 356.088685 228.190606 \n",
       "L 356.378831 228.136327 \n",
       "L 356.668977 229.31841 \n",
       "L 356.959124 227.979281 \n",
       "L 357.24927 229.253997 \n",
       "L 357.539417 228.42649 \n",
       "L 357.829563 228.129648 \n",
       "L 358.11971 229.015562 \n",
       "L 358.409856 228.571186 \n",
       "L 358.990149 228.5743 \n",
       "L 359.280296 227.248131 \n",
       "L 359.570442 228.73019 \n",
       "L 359.860589 228.987005 \n",
       "L 360.150735 227.270826 \n",
       "L 360.440881 229.394153 \n",
       "L 360.731028 228.49951 \n",
       "L 361.021174 228.445289 \n",
       "L 361.311321 226.807381 \n",
       "L 361.601467 229.590683 \n",
       "L 361.891614 227.914864 \n",
       "L 362.18176 229.123387 \n",
       "L 362.471907 227.36455 \n",
       "L 362.762053 228.322312 \n",
       "L 363.0522 227.570511 \n",
       "L 363.342346 228.876893 \n",
       "L 363.632493 228.84949 \n",
       "L 363.922639 228.476604 \n",
       "L 364.212785 228.490193 \n",
       "L 364.502932 229.208992 \n",
       "L 364.793078 227.750098 \n",
       "L 365.083225 228.304351 \n",
       "L 365.373371 227.733955 \n",
       "L 365.663518 229.486583 \n",
       "L 365.953664 228.309655 \n",
       "L 366.243811 228.849263 \n",
       "L 366.533957 228.890881 \n",
       "L 366.824104 229.433084 \n",
       "L 367.11425 227.385972 \n",
       "L 367.694543 228.352396 \n",
       "L 367.984689 228.238883 \n",
       "L 368.274836 228.673349 \n",
       "L 368.564982 227.062319 \n",
       "L 368.855129 228.353435 \n",
       "L 369.145275 228.389372 \n",
       "L 369.435422 228.223919 \n",
       "L 369.725568 228.772581 \n",
       "L 369.725568 228.772581 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path clip-path=\"url(#p2fb0fe701f)\" d=\"M 65.361932 32.201761 \n",
       "L 65.942225 59.836933 \n",
       "L 66.232371 64.419599 \n",
       "L 66.522518 61.060122 \n",
       "L 66.812664 62.493252 \n",
       "L 67.102811 67.785494 \n",
       "L 67.392957 82.801898 \n",
       "L 67.683103 84.026778 \n",
       "L 68.263396 95.167893 \n",
       "L 68.553543 95.26081 \n",
       "L 68.843689 84.949561 \n",
       "L 69.133836 98.571496 \n",
       "L 69.423982 105.548788 \n",
       "L 69.714129 108.888197 \n",
       "L 70.004275 108.000326 \n",
       "L 70.294422 115.119807 \n",
       "L 70.584568 116.950428 \n",
       "L 70.874715 120.430794 \n",
       "L 71.164861 119.65686 \n",
       "L 71.455007 126.379003 \n",
       "L 71.745154 115.36733 \n",
       "L 72.0353 122.522943 \n",
       "L 72.325447 126.628585 \n",
       "L 72.615593 133.701538 \n",
       "L 72.90574 135.349663 \n",
       "L 73.195886 139.516938 \n",
       "L 73.486033 140.558361 \n",
       "L 73.776179 137.563604 \n",
       "L 74.066326 145.164112 \n",
       "L 74.356472 148.505663 \n",
       "L 74.646619 146.960711 \n",
       "L 74.936765 149.404699 \n",
       "L 75.226911 149.325534 \n",
       "L 75.517058 155.883006 \n",
       "L 75.807204 153.366201 \n",
       "L 76.097351 153.994044 \n",
       "L 76.387497 157.682909 \n",
       "L 76.677644 157.288431 \n",
       "L 76.96779 158.710479 \n",
       "L 77.257937 162.933185 \n",
       "L 77.548083 161.325507 \n",
       "L 78.128376 169.216667 \n",
       "L 78.418523 166.665452 \n",
       "L 78.708669 166.710829 \n",
       "L 78.998815 169.940368 \n",
       "L 79.288962 171.600865 \n",
       "L 79.579108 176.244363 \n",
       "L 79.869255 174.091254 \n",
       "L 80.159401 174.497508 \n",
       "L 80.449548 174.170712 \n",
       "L 80.739694 175.716509 \n",
       "L 81.029841 178.832511 \n",
       "L 81.319987 177.892132 \n",
       "L 81.610134 180.449828 \n",
       "L 81.90028 179.331503 \n",
       "L 82.480573 181.474145 \n",
       "L 82.770719 181.462799 \n",
       "L 83.060866 184.686813 \n",
       "L 83.351012 185.455406 \n",
       "L 83.641159 185.03625 \n",
       "L 83.931305 186.094318 \n",
       "L 84.221452 188.412649 \n",
       "L 84.801745 186.969608 \n",
       "L 85.091891 188.713302 \n",
       "L 85.382038 189.74959 \n",
       "L 85.672184 194.973564 \n",
       "L 85.96233 191.825688 \n",
       "L 86.252477 193.458952 \n",
       "L 86.542623 193.248159 \n",
       "L 86.83277 192.481593 \n",
       "L 87.122916 195.084124 \n",
       "L 87.413063 191.711354 \n",
       "L 87.703209 194.99876 \n",
       "L 87.993356 189.856836 \n",
       "L 88.573649 196.737587 \n",
       "L 88.863795 196.229527 \n",
       "L 89.153942 193.220958 \n",
       "L 89.444088 197.535964 \n",
       "L 90.024381 199.825768 \n",
       "L 90.314527 195.279803 \n",
       "L 90.604674 198.578985 \n",
       "L 90.89482 200.158992 \n",
       "L 91.184967 200.86334 \n",
       "L 91.475113 200.231097 \n",
       "L 91.76526 196.765343 \n",
       "L 92.055406 200.011563 \n",
       "L 92.635699 201.693001 \n",
       "L 92.925846 203.231389 \n",
       "L 93.215992 202.867644 \n",
       "L 93.506138 202.839691 \n",
       "L 93.796285 203.400185 \n",
       "L 94.086431 203.27764 \n",
       "L 94.376578 206.064805 \n",
       "L 94.956871 204.351761 \n",
       "L 95.247017 205.776062 \n",
       "L 95.537164 206.125567 \n",
       "L 95.82731 204.546115 \n",
       "L 96.117457 203.999007 \n",
       "L 96.987896 208.458993 \n",
       "L 97.278042 204.042322 \n",
       "L 97.568189 206.462138 \n",
       "L 97.858335 206.177726 \n",
       "L 98.148482 207.403307 \n",
       "L 98.728775 206.061423 \n",
       "L 99.018921 208.565336 \n",
       "L 99.309068 204.639184 \n",
       "L 99.599214 208.044776 \n",
       "L 99.889361 209.074505 \n",
       "L 100.179507 207.094569 \n",
       "L 100.469653 208.67153 \n",
       "L 100.7598 208.38657 \n",
       "L 101.049946 207.797695 \n",
       "L 101.340093 209.162588 \n",
       "L 101.630239 208.436483 \n",
       "L 101.920386 208.800422 \n",
       "L 102.210532 209.600915 \n",
       "L 102.500679 211.013773 \n",
       "L 102.790825 207.349998 \n",
       "L 103.661265 211.258954 \n",
       "L 103.951411 210.52376 \n",
       "L 104.531704 209.851777 \n",
       "L 104.82185 212.023167 \n",
       "L 105.111997 212.47928 \n",
       "L 105.402143 212.593946 \n",
       "L 105.982436 212.152206 \n",
       "L 106.272583 212.338307 \n",
       "L 106.562729 210.689338 \n",
       "L 107.143022 214.625468 \n",
       "L 107.433168 213.328834 \n",
       "L 107.723315 213.307862 \n",
       "L 108.013461 214.367341 \n",
       "L 108.303608 210.724575 \n",
       "L 108.593754 210.727738 \n",
       "L 108.883901 211.779399 \n",
       "L 109.174047 213.597032 \n",
       "L 109.464194 212.901491 \n",
       "L 109.75434 212.910983 \n",
       "L 110.044487 209.468524 \n",
       "L 110.334633 210.917074 \n",
       "L 110.62478 213.335808 \n",
       "L 110.914926 209.227735 \n",
       "L 111.205072 215.595052 \n",
       "L 111.495219 211.826873 \n",
       "L 111.785365 211.626687 \n",
       "L 112.075512 213.358263 \n",
       "L 112.365658 213.449716 \n",
       "L 112.655805 215.262634 \n",
       "L 112.945951 214.821364 \n",
       "L 113.236098 214.779624 \n",
       "L 113.526244 213.426235 \n",
       "L 113.816391 213.384163 \n",
       "L 114.106537 216.495704 \n",
       "L 114.396684 213.640058 \n",
       "L 114.68683 214.169035 \n",
       "L 114.976976 214.441107 \n",
       "L 115.267123 214.2779 \n",
       "L 115.557269 214.338379 \n",
       "L 115.847416 214.607331 \n",
       "L 116.137562 215.532459 \n",
       "L 116.427709 217.767394 \n",
       "L 116.717855 215.304663 \n",
       "L 117.008002 216.61296 \n",
       "L 117.298148 215.626712 \n",
       "L 117.588295 215.431255 \n",
       "L 117.878441 215.539557 \n",
       "L 118.458734 217.824999 \n",
       "L 118.74888 216.362596 \n",
       "L 119.039027 215.629295 \n",
       "L 119.329173 218.684474 \n",
       "L 119.909466 216.236319 \n",
       "L 120.199613 216.964022 \n",
       "L 120.489759 215.9771 \n",
       "L 120.779906 218.841627 \n",
       "L 121.070052 217.450061 \n",
       "L 121.360199 218.214574 \n",
       "L 121.650345 217.661592 \n",
       "L 122.230638 217.353253 \n",
       "L 122.520784 218.800481 \n",
       "L 122.810931 218.599553 \n",
       "L 123.101077 217.051256 \n",
       "L 123.391224 219.226189 \n",
       "L 123.68137 218.770254 \n",
       "L 123.971517 217.444959 \n",
       "L 124.55181 219.573761 \n",
       "L 124.841956 219.800888 \n",
       "L 125.422249 218.427812 \n",
       "L 125.712395 220.234687 \n",
       "L 126.002542 219.16966 \n",
       "L 126.292688 219.164574 \n",
       "L 126.582835 220.163243 \n",
       "L 127.163128 219.619693 \n",
       "L 127.453274 220.687068 \n",
       "L 127.743421 220.132966 \n",
       "L 128.033567 220.169253 \n",
       "L 128.323714 221.197392 \n",
       "L 128.61386 220.993058 \n",
       "L 128.904007 221.145675 \n",
       "L 129.194153 220.75912 \n",
       "L 129.774446 221.316136 \n",
       "L 130.354739 221.185737 \n",
       "L 130.644885 221.33837 \n",
       "L 130.935032 221.296508 \n",
       "L 131.225178 222.106223 \n",
       "L 131.515325 221.759814 \n",
       "L 131.805471 221.843999 \n",
       "L 132.095618 221.667714 \n",
       "L 132.385764 221.769001 \n",
       "L 132.67591 222.614624 \n",
       "L 133.256203 222.296679 \n",
       "L 133.54635 222.530721 \n",
       "L 133.836496 222.418192 \n",
       "L 134.126643 222.571436 \n",
       "L 134.416789 222.888097 \n",
       "L 134.706936 222.766648 \n",
       "L 134.997082 223.235666 \n",
       "L 135.287229 223.171666 \n",
       "L 135.577375 222.858851 \n",
       "L 136.157668 223.278599 \n",
       "L 136.737961 223.279758 \n",
       "L 137.028107 223.742998 \n",
       "L 137.318254 223.743639 \n",
       "L 137.898547 223.413744 \n",
       "L 138.188693 223.506266 \n",
       "L 138.47884 224.114608 \n",
       "L 139.059133 224.099356 \n",
       "L 139.349279 223.97492 \n",
       "L 139.929572 224.313649 \n",
       "L 140.219718 224.25515 \n",
       "L 140.509865 223.798579 \n",
       "L 140.800011 224.080989 \n",
       "L 141.090158 224.074842 \n",
       "L 141.380304 224.497962 \n",
       "L 141.960597 224.597906 \n",
       "L 142.250744 224.507121 \n",
       "L 142.54089 224.622595 \n",
       "L 142.831037 224.509933 \n",
       "L 143.121183 224.522821 \n",
       "L 143.41133 224.850199 \n",
       "L 143.701476 224.899927 \n",
       "L 143.991622 224.510753 \n",
       "L 144.281769 225.006084 \n",
       "L 144.571915 224.811469 \n",
       "L 144.862062 225.113456 \n",
       "L 147.763526 225.323488 \n",
       "L 148.053673 225.09227 \n",
       "L 148.343819 225.365779 \n",
       "L 148.633966 225.249621 \n",
       "L 148.924112 225.446925 \n",
       "L 149.214259 225.350737 \n",
       "L 149.794552 225.518571 \n",
       "L 150.664991 225.527256 \n",
       "L 151.53543 225.709123 \n",
       "L 152.40587 225.669417 \n",
       "L 152.696016 225.816645 \n",
       "L 152.986163 225.648555 \n",
       "L 153.276309 225.854176 \n",
       "L 154.436895 225.958069 \n",
       "L 154.727041 225.809311 \n",
       "L 155.307334 225.969665 \n",
       "L 155.597481 226.033007 \n",
       "L 156.46792 225.951863 \n",
       "L 156.758067 226.133029 \n",
       "L 157.048213 226.006792 \n",
       "L 158.208799 226.3102 \n",
       "L 158.498945 226.057311 \n",
       "L 158.789092 226.249312 \n",
       "L 159.659531 226.325296 \n",
       "L 160.529971 226.153271 \n",
       "L 160.820117 226.364554 \n",
       "L 161.110264 226.183019 \n",
       "L 161.980703 226.478494 \n",
       "L 162.560996 226.495649 \n",
       "L 162.851142 226.492089 \n",
       "L 163.141289 226.3452 \n",
       "L 163.721582 226.551114 \n",
       "L 164.011728 226.469207 \n",
       "L 164.301875 226.592537 \n",
       "L 164.592021 226.559613 \n",
       "L 164.882168 226.410446 \n",
       "L 165.46246 226.65576 \n",
       "L 166.042753 226.675966 \n",
       "L 166.3329 226.71336 \n",
       "L 166.623046 226.599801 \n",
       "L 167.203339 226.643649 \n",
       "L 169.234364 226.667205 \n",
       "L 170.104804 226.745801 \n",
       "L 170.685097 226.833355 \n",
       "L 171.555536 226.786894 \n",
       "L 172.135829 226.8731 \n",
       "L 173.006268 226.89584 \n",
       "L 175.037294 227.008619 \n",
       "L 175.32744 226.798931 \n",
       "L 175.617587 227.004098 \n",
       "L 175.907733 227.053971 \n",
       "L 176.488026 226.931127 \n",
       "L 177.068319 226.994025 \n",
       "L 177.358465 227.184015 \n",
       "L 177.938758 227.274818 \n",
       "L 178.519051 227.399758 \n",
       "L 179.099344 227.228129 \n",
       "L 179.679637 227.428338 \n",
       "L 179.969783 227.327302 \n",
       "L 180.550076 227.428422 \n",
       "L 183.741687 227.529187 \n",
       "L 184.902273 227.498461 \n",
       "L 185.482566 227.529775 \n",
       "L 185.772713 227.446073 \n",
       "L 186.353006 227.564146 \n",
       "L 187.803738 227.569154 \n",
       "L 190.12491 227.663137 \n",
       "L 190.705202 227.613365 \n",
       "L 226.97351 227.788666 \n",
       "L 227.263656 227.717242 \n",
       "L 227.843949 227.789059 \n",
       "L 229.004535 227.849565 \n",
       "L 230.165121 227.808695 \n",
       "L 231.03556 227.812588 \n",
       "L 234.227171 227.833207 \n",
       "L 234.517318 227.734943 \n",
       "L 234.807464 227.823589 \n",
       "L 235.677904 227.811804 \n",
       "L 235.96805 227.85829 \n",
       "L 236.548343 227.769118 \n",
       "L 241.480833 227.84893 \n",
       "L 241.770979 227.773731 \n",
       "L 242.351272 227.817826 \n",
       "L 243.221712 227.818613 \n",
       "L 243.511858 227.848425 \n",
       "L 243.802005 227.763821 \n",
       "L 244.382298 227.822983 \n",
       "L 244.672444 227.747947 \n",
       "L 244.96259 227.85291 \n",
       "L 245.252737 227.79723 \n",
       "L 245.542883 227.863273 \n",
       "L 245.83303 227.782559 \n",
       "L 246.993616 227.889205 \n",
       "L 247.283762 227.801152 \n",
       "L 247.864055 227.815169 \n",
       "L 248.444348 227.792668 \n",
       "L 249.024641 227.833519 \n",
       "L 249.314787 227.808825 \n",
       "L 249.604934 227.899991 \n",
       "L 250.185227 227.804618 \n",
       "L 251.345813 227.88192 \n",
       "L 252.796545 227.833906 \n",
       "L 263.531964 227.906194 \n",
       "L 263.82211 227.781637 \n",
       "L 264.112257 227.895199 \n",
       "L 265.272843 227.779012 \n",
       "L 266.723575 227.861142 \n",
       "L 267.884161 227.840934 \n",
       "L 268.174307 227.743328 \n",
       "L 268.7546 227.856722 \n",
       "L 271.075772 227.874318 \n",
       "L 271.946211 227.823003 \n",
       "L 275.427969 227.89569 \n",
       "L 277.168847 227.850853 \n",
       "L 278.909726 227.839833 \n",
       "L 279.490019 227.786417 \n",
       "L 279.780166 227.868574 \n",
       "L 280.070312 227.690023 \n",
       "L 280.360459 227.834519 \n",
       "L 282.391484 227.885104 \n",
       "L 283.55207 227.804549 \n",
       "L 290.225438 227.867809 \n",
       "L 290.515585 227.772854 \n",
       "L 291.095878 227.833276 \n",
       "L 291.67617 227.807952 \n",
       "L 292.256463 227.804929 \n",
       "L 293.417049 227.845184 \n",
       "L 295.738221 227.799861 \n",
       "L 297.188953 227.889364 \n",
       "L 297.769246 227.814731 \n",
       "L 298.639686 227.897985 \n",
       "L 299.219978 227.797858 \n",
       "L 302.121443 227.832349 \n",
       "L 303.862322 227.876816 \n",
       "L 304.152468 227.750154 \n",
       "L 304.732761 227.853574 \n",
       "L 306.763786 227.850293 \n",
       "L 308.504665 227.886649 \n",
       "L 309.084958 227.769676 \n",
       "L 309.955397 227.859293 \n",
       "L 311.40613 227.762583 \n",
       "L 312.566716 227.887715 \n",
       "L 316.048473 227.829325 \n",
       "L 316.628766 227.8728 \n",
       "L 316.918912 227.742778 \n",
       "L 317.209059 227.833292 \n",
       "L 317.499205 227.709257 \n",
       "L 317.789352 227.863698 \n",
       "L 320.690816 227.782538 \n",
       "L 321.271109 227.865268 \n",
       "L 321.851402 227.832312 \n",
       "L 322.431695 227.842838 \n",
       "L 323.011988 227.840352 \n",
       "L 323.302135 227.720097 \n",
       "L 323.882428 227.817289 \n",
       "L 325.33316 227.821594 \n",
       "L 326.493746 227.766112 \n",
       "L 327.654332 227.815639 \n",
       "L 331.426235 227.79959 \n",
       "L 331.716382 227.728222 \n",
       "L 332.006528 227.819079 \n",
       "L 332.876968 227.827861 \n",
       "L 339.26019 227.802494 \n",
       "L 340.130629 227.683603 \n",
       "L 340.420776 227.81457 \n",
       "L 341.001069 227.724038 \n",
       "L 341.581362 227.791782 \n",
       "L 343.612387 227.759164 \n",
       "L 343.902533 227.79918 \n",
       "L 344.19268 227.697921 \n",
       "L 344.482826 227.819936 \n",
       "L 345.063119 227.808513 \n",
       "L 345.353266 227.858061 \n",
       "L 346.223705 227.670259 \n",
       "L 346.513851 227.805382 \n",
       "L 347.384291 227.799816 \n",
       "L 350.285755 227.801961 \n",
       "L 350.575902 227.820082 \n",
       "L 350.866048 227.682332 \n",
       "L 351.156195 227.832572 \n",
       "L 351.446341 227.846211 \n",
       "L 352.026634 227.691674 \n",
       "L 352.606927 227.827682 \n",
       "L 354.057659 227.669672 \n",
       "L 354.928099 227.763154 \n",
       "L 355.798538 227.790348 \n",
       "L 356.959124 227.733547 \n",
       "L 357.24927 227.672568 \n",
       "L 357.539417 227.803433 \n",
       "L 358.990149 227.791294 \n",
       "L 369.725568 227.696456 \n",
       "L 369.725568 227.696456 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_16\">\n",
       "    <!-- Autoencoder loss over time -->\n",
       "    <defs>\n",
       "     <path d=\"M 34.1875 63.1875 \n",
       "L 20.796875 26.90625 \n",
       "L 47.609375 26.90625 \n",
       "z\n",
       "M 28.609375 72.90625 \n",
       "L 39.796875 72.90625 \n",
       "L 67.578125 0 \n",
       "L 57.328125 0 \n",
       "L 50.6875 18.703125 \n",
       "L 17.828125 18.703125 \n",
       "L 11.1875 0 \n",
       "L 0.78125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-65\"/>\n",
       "     <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "     <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "     <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "     <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "     <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "     <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(134.935 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "     <use x=\"68.408203\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"131.787109\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"170.996094\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"232.177734\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"293.701172\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"357.080078\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"412.060547\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"473.242188\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"536.71875\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"598.242188\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"639.355469\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"671.142578\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"698.925781\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"760.107422\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"812.207031\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"864.306641\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"896.09375\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"957.275391\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "     <use x=\"1016.455078\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"1077.978516\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"1119.091797\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"1150.878906\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"1190.087891\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"1217.871094\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"1315.283203\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 274.215625 59.674375 \n",
       "L 377.94375 59.674375 \n",
       "Q 379.94375 59.674375 379.94375 57.674375 \n",
       "L 379.94375 29.318125 \n",
       "Q 379.94375 27.318125 377.94375 27.318125 \n",
       "L 274.215625 27.318125 \n",
       "Q 272.215625 27.318125 272.215625 29.318125 \n",
       "L 272.215625 57.674375 \n",
       "Q 272.215625 59.674375 274.215625 59.674375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <path d=\"M 276.215625 35.416562 \n",
       "L 296.215625 35.416562 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\"/>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- train loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(304.215625 38.916562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 276.215625 50.094687 \n",
       "L 296.215625 50.094687 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\"/>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- validation loss -->\n",
       "     <g transform=\"translate(304.215625 53.594687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"492.333984\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"524.121094\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"551.904297\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"613.085938\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"665.185547\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2fb0fe701f\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses_2['epoch'], losses_2['train_loss'], label='train loss')\n",
    "ax.plot(losses_2['epoch'], losses_2['validation_loss'], label='validation loss')\n",
    "ax.set_ylabel('MSE loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_title('Autoencoder loss over time')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_02 = ae_1.get_encoded_representations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('autoencoderhy2_embeddings.pkl', 'wb') as fh:\n",
    "    pickle.dump(encoded_02, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_5 = AutoEncoder(df_new, validation_perc=0.1, lr=1e-3, intermediate_size=1000,intermediate_size2=500, encoded_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "train loss: 0.22687487 | validation loss: 0.23347482\n",
      "train loss: 0.2138636 | validation loss: 0.21444488\n",
      "train loss: 0.21414584 | validation loss: 0.21093114\n",
      "train loss: 0.20620148 | validation loss: 0.20762575\n",
      "train loss: 0.20368007 | validation loss: 0.20072559\n",
      "train loss: 0.20976926 | validation loss: 0.19581823\n",
      "train loss: 0.19720303 | validation loss: 0.18664081\n",
      "Epoch 2/100\n",
      "train loss: 0.19225802 | validation loss: 0.19477651\n",
      "train loss: 0.1924013 | validation loss: 0.17648667\n",
      "train loss: 0.18388727 | validation loss: 0.17624198\n",
      "train loss: 0.17764394 | validation loss: 0.1665626\n",
      "train loss: 0.17515899 | validation loss: 0.16473073\n",
      "train loss: 0.16890708 | validation loss: 0.15848196\n",
      "train loss: 0.16731933 | validation loss: 0.15974346\n",
      "Epoch 3/100\n",
      "train loss: 0.16312948 | validation loss: 0.15736099\n",
      "train loss: 0.15469769 | validation loss: 0.14351682\n",
      "train loss: 0.15270461 | validation loss: 0.14721374\n",
      "train loss: 0.15544726 | validation loss: 0.14849043\n",
      "train loss: 0.14904456 | validation loss: 0.14867903\n",
      "train loss: 0.14727546 | validation loss: 0.13608147\n",
      "train loss: 0.14919345 | validation loss: 0.12873933\n",
      "Epoch 4/100\n",
      "train loss: 0.14563932 | validation loss: 0.14114931\n",
      "train loss: 0.13153683 | validation loss: 0.1230399\n",
      "train loss: 0.13322262 | validation loss: 0.12003411\n",
      "train loss: 0.13179097 | validation loss: 0.12155921\n",
      "train loss: 0.13232507 | validation loss: 0.11766852\n",
      "train loss: 0.12001949 | validation loss: 0.11632989\n",
      "train loss: 0.11967247 | validation loss: 0.11713137\n",
      "Epoch 5/100\n",
      "train loss: 0.12472117 | validation loss: 0.10751344\n",
      "train loss: 0.10970023 | validation loss: 0.1081797\n",
      "train loss: 0.11223326 | validation loss: 0.10241748\n",
      "train loss: 0.10845185 | validation loss: 0.09789925\n",
      "train loss: 0.10802581 | validation loss: 0.09838369\n",
      "train loss: 0.10647362 | validation loss: 0.09934294\n",
      "train loss: 0.10755597 | validation loss: 0.10173391\n",
      "Epoch 6/100\n",
      "train loss: 0.10314438 | validation loss: 0.10183104\n",
      "train loss: 0.10367228 | validation loss: 0.0885703\n",
      "train loss: 0.10731791 | validation loss: 0.09058428\n",
      "train loss: 0.09456263 | validation loss: 0.08445006\n",
      "train loss: 0.10036032 | validation loss: 0.08616778\n",
      "train loss: 0.09616478 | validation loss: 0.08496788\n",
      "train loss: 0.09762574 | validation loss: 0.08191885\n",
      "Epoch 7/100\n",
      "train loss: 0.08794843 | validation loss: 0.0861696\n",
      "train loss: 0.09542007 | validation loss: 0.07850204\n",
      "train loss: 0.08474234 | validation loss: 0.07680365\n",
      "train loss: 0.07916011 | validation loss: 0.07468633\n",
      "train loss: 0.08263376 | validation loss: 0.07693004\n",
      "train loss: 0.07946546 | validation loss: 0.07446258\n",
      "train loss: 0.07818383 | validation loss: 0.07144213\n",
      "Epoch 8/100\n",
      "train loss: 0.07749396 | validation loss: 0.07532734\n",
      "train loss: 0.07442079 | validation loss: 0.07147963\n",
      "train loss: 0.07596643 | validation loss: 0.06991018\n",
      "train loss: 0.07042833 | validation loss: 0.06444108\n",
      "train loss: 0.07154646 | validation loss: 0.06495013\n",
      "train loss: 0.07084868 | validation loss: 0.06407304\n",
      "train loss: 0.07922861 | validation loss: 0.0595866\n",
      "Epoch 9/100\n",
      "train loss: 0.07472655 | validation loss: 0.06194406\n",
      "train loss: 0.07449324 | validation loss: 0.05898144\n",
      "train loss: 0.06432073 | validation loss: 0.05781976\n",
      "train loss: 0.06626269 | validation loss: 0.05795369\n",
      "train loss: 0.06995197 | validation loss: 0.06422349\n",
      "train loss: 0.06837828 | validation loss: 0.06040715\n",
      "train loss: 0.06233799 | validation loss: 0.05789731\n",
      "Epoch 10/100\n",
      "train loss: 0.06187452 | validation loss: 0.05524205\n",
      "train loss: 0.0684661 | validation loss: 0.05499811\n",
      "train loss: 0.0605156 | validation loss: 0.05358831\n",
      "train loss: 0.05817939 | validation loss: 0.05432444\n",
      "train loss: 0.05823256 | validation loss: 0.05795522\n",
      "train loss: 0.05754571 | validation loss: 0.05164517\n",
      "train loss: 0.05900104 | validation loss: 0.04835625\n",
      "Epoch 11/100\n",
      "train loss: 0.05520068 | validation loss: 0.05775664\n",
      "train loss: 0.06313328 | validation loss: 0.04850177\n",
      "train loss: 0.06123368 | validation loss: 0.04789398\n",
      "train loss: 0.06355662 | validation loss: 0.04599702\n",
      "train loss: 0.05131324 | validation loss: 0.04641105\n",
      "train loss: 0.05345171 | validation loss: 0.04723016\n",
      "train loss: 0.05182692 | validation loss: 0.04407614\n",
      "Epoch 12/100\n",
      "train loss: 0.04993087 | validation loss: 0.04534745\n",
      "train loss: 0.0592113 | validation loss: 0.04784597\n",
      "train loss: 0.05733315 | validation loss: 0.04323395\n",
      "train loss: 0.04739264 | validation loss: 0.04450694\n",
      "train loss: 0.04715588 | validation loss: 0.0491263\n",
      "train loss: 0.05849959 | validation loss: 0.04726345\n",
      "train loss: 0.05864414 | validation loss: 0.04118907\n",
      "Epoch 13/100\n",
      "train loss: 0.04549222 | validation loss: 0.04788391\n",
      "train loss: 0.05671427 | validation loss: 0.04319769\n",
      "train loss: 0.0475005 | validation loss: 0.0424564\n",
      "train loss: 0.04104669 | validation loss: 0.03894817\n",
      "train loss: 0.04622997 | validation loss: 0.04056266\n",
      "train loss: 0.04116766 | validation loss: 0.03970252\n",
      "train loss: 0.04372276 | validation loss: 0.04434313\n",
      "Epoch 14/100\n",
      "train loss: 0.04155979 | validation loss: 0.04217135\n",
      "train loss: 0.04406222 | validation loss: 0.0401282\n",
      "train loss: 0.04213633 | validation loss: 0.03782904\n",
      "train loss: 0.04331364 | validation loss: 0.03961735\n",
      "train loss: 0.04795168 | validation loss: 0.0396045\n",
      "train loss: 0.04247107 | validation loss: 0.04036721\n",
      "train loss: 0.04935786 | validation loss: 0.03446595\n",
      "Epoch 15/100\n",
      "train loss: 0.03955458 | validation loss: 0.03900255\n",
      "train loss: 0.03992522 | validation loss: 0.03884181\n",
      "train loss: 0.05169648 | validation loss: 0.03409086\n",
      "train loss: 0.04834665 | validation loss: 0.03489891\n",
      "train loss: 0.04120995 | validation loss: 0.03562464\n",
      "train loss: 0.05023725 | validation loss: 0.0331993\n",
      "train loss: 0.03901744 | validation loss: 0.03452695\n",
      "Epoch 16/100\n",
      "train loss: 0.03781945 | validation loss: 0.03515266\n",
      "train loss: 0.04269565 | validation loss: 0.03492635\n",
      "train loss: 0.03775811 | validation loss: 0.03240527\n",
      "train loss: 0.03612059 | validation loss: 0.03413997\n",
      "train loss: 0.03499647 | validation loss: 0.0320885\n",
      "train loss: 0.03874994 | validation loss: 0.03256239\n",
      "train loss: 0.03456574 | validation loss: 0.03297713\n",
      "Epoch 17/100\n",
      "train loss: 0.03774834 | validation loss: 0.03296059\n",
      "train loss: 0.04682258 | validation loss: 0.02988845\n",
      "train loss: 0.03504774 | validation loss: 0.03266432\n",
      "train loss: 0.03422716 | validation loss: 0.03241027\n",
      "train loss: 0.03503041 | validation loss: 0.03044296\n",
      "train loss: 0.0335377 | validation loss: 0.03032361\n",
      "train loss: 0.03364391 | validation loss: 0.02972255\n",
      "Epoch 18/100\n",
      "train loss: 0.03191706 | validation loss: 0.03138205\n",
      "train loss: 0.03429213 | validation loss: 0.02885215\n",
      "train loss: 0.04528932 | validation loss: 0.02880719\n",
      "train loss: 0.03424971 | validation loss: 0.0287401\n",
      "train loss: 0.04234593 | validation loss: 0.02878507\n",
      "train loss: 0.02802428 | validation loss: 0.02855556\n",
      "train loss: 0.03398046 | validation loss: 0.02844454\n",
      "Epoch 19/100\n",
      "train loss: 0.02866853 | validation loss: 0.03112321\n",
      "train loss: 0.0316799 | validation loss: 0.03030689\n",
      "train loss: 0.03159427 | validation loss: 0.02856673\n",
      "train loss: 0.04270836 | validation loss: 0.02790908\n",
      "train loss: 0.03385486 | validation loss: 0.02833522\n",
      "train loss: 0.03247504 | validation loss: 0.03006824\n",
      "train loss: 0.03158491 | validation loss: 0.03079491\n",
      "Epoch 20/100\n",
      "train loss: 0.02822153 | validation loss: 0.02690263\n",
      "train loss: 0.03115824 | validation loss: 0.02586334\n",
      "train loss: 0.03002933 | validation loss: 0.03138509\n",
      "train loss: 0.02642167 | validation loss: 0.03355704\n",
      "train loss: 0.03053585 | validation loss: 0.03262082\n",
      "train loss: 0.03001993 | validation loss: 0.03009419\n",
      "train loss: 0.02956869 | validation loss: 0.02613708\n",
      "Epoch 21/100\n",
      "train loss: 0.04096053 | validation loss: 0.02742535\n",
      "train loss: 0.04000393 | validation loss: 0.03247864\n",
      "train loss: 0.03441737 | validation loss: 0.02443207\n",
      "train loss: 0.02967596 | validation loss: 0.02961631\n",
      "train loss: 0.02917182 | validation loss: 0.02473422\n",
      "train loss: 0.03015025 | validation loss: 0.02817697\n",
      "train loss: 0.02693278 | validation loss: 0.02409275\n",
      "Epoch 22/100\n",
      "train loss: 0.0292484 | validation loss: 0.02659961\n",
      "train loss: 0.03028532 | validation loss: 0.02808765\n",
      "train loss: 0.02399976 | validation loss: 0.02346649\n",
      "train loss: 0.02790912 | validation loss: 0.01923604\n",
      "train loss: 0.0258437 | validation loss: 0.02716372\n",
      "train loss: 0.02494401 | validation loss: 0.02361585\n",
      "train loss: 0.03792211 | validation loss: 0.02360164\n",
      "Epoch 23/100\n",
      "train loss: 0.02891218 | validation loss: 0.02271882\n",
      "train loss: 0.02667824 | validation loss: 0.02131954\n",
      "train loss: 0.02576315 | validation loss: 0.02070741\n",
      "train loss: 0.02848495 | validation loss: 0.02529893\n",
      "train loss: 0.03255901 | validation loss: 0.02166872\n",
      "train loss: 0.02574932 | validation loss: 0.0246666\n",
      "train loss: 0.02672513 | validation loss: 0.02206315\n",
      "Epoch 24/100\n",
      "train loss: 0.02363508 | validation loss: 0.01921987\n",
      "train loss: 0.0246835 | validation loss: 0.02367325\n",
      "train loss: 0.02881937 | validation loss: 0.02287393\n",
      "train loss: 0.02334996 | validation loss: 0.0223783\n",
      "train loss: 0.02395191 | validation loss: 0.02279404\n",
      "train loss: 0.02691949 | validation loss: 0.01990401\n",
      "train loss: 0.02314015 | validation loss: 0.02366475\n",
      "Epoch 25/100\n",
      "train loss: 0.02481925 | validation loss: 0.02275557\n",
      "train loss: 0.02377394 | validation loss: 0.02017615\n",
      "train loss: 0.02402214 | validation loss: 0.02014309\n",
      "train loss: 0.02640864 | validation loss: 0.02015815\n",
      "train loss: 0.03090304 | validation loss: 0.02233978\n",
      "train loss: 0.02530848 | validation loss: 0.02137253\n",
      "train loss: 0.02419718 | validation loss: 0.01868908\n",
      "Epoch 26/100\n",
      "train loss: 0.02164691 | validation loss: 0.02027619\n",
      "train loss: 0.02425739 | validation loss: 0.02303787\n",
      "train loss: 0.0265996 | validation loss: 0.02239172\n",
      "train loss: 0.02236655 | validation loss: 0.01766071\n",
      "train loss: 0.0312732 | validation loss: 0.01802115\n",
      "train loss: 0.02190129 | validation loss: 0.01960346\n",
      "train loss: 0.02450062 | validation loss: 0.01948418\n",
      "Epoch 27/100\n",
      "train loss: 0.02123284 | validation loss: 0.01955968\n",
      "train loss: 0.02225394 | validation loss: 0.01622356\n",
      "train loss: 0.02400322 | validation loss: 0.01633237\n",
      "train loss: 0.02185127 | validation loss: 0.01736582\n",
      "train loss: 0.02358783 | validation loss: 0.01856448\n",
      "train loss: 0.03078468 | validation loss: 0.01888005\n",
      "train loss: 0.0207033 | validation loss: 0.02136708\n",
      "Epoch 28/100\n",
      "train loss: 0.0201993 | validation loss: 0.01849889\n",
      "train loss: 0.02175744 | validation loss: 0.01823663\n",
      "train loss: 0.02115914 | validation loss: 0.0158011\n",
      "train loss: 0.02201803 | validation loss: 0.01772284\n",
      "train loss: 0.02110272 | validation loss: 0.01565558\n",
      "train loss: 0.02392456 | validation loss: 0.01784034\n",
      "train loss: 0.02029781 | validation loss: 0.01565094\n",
      "Epoch 29/100\n",
      "train loss: 0.02414267 | validation loss: 0.01766525\n",
      "train loss: 0.02316414 | validation loss: 0.0196554\n",
      "train loss: 0.02489719 | validation loss: 0.0197818\n",
      "train loss: 0.01819096 | validation loss: 0.0184499\n",
      "train loss: 0.01652112 | validation loss: 0.01968976\n",
      "train loss: 0.01701716 | validation loss: 0.01812271\n",
      "train loss: 0.01850617 | validation loss: 0.01663254\n",
      "Epoch 30/100\n",
      "train loss: 0.02122263 | validation loss: 0.01768063\n",
      "train loss: 0.02208129 | validation loss: 0.01687487\n",
      "train loss: 0.01782048 | validation loss: 0.01709819\n",
      "train loss: 0.0163786 | validation loss: 0.01593876\n",
      "train loss: 0.01531328 | validation loss: 0.0157665\n",
      "train loss: 0.01756454 | validation loss: 0.01479858\n",
      "train loss: 0.0161407 | validation loss: 0.01587459\n",
      "Epoch 31/100\n",
      "train loss: 0.01886147 | validation loss: 0.01572849\n",
      "train loss: 0.02026615 | validation loss: 0.01443494\n",
      "train loss: 0.01789876 | validation loss: 0.01545184\n",
      "train loss: 0.01487186 | validation loss: 0.0155042\n",
      "train loss: 0.01578256 | validation loss: 0.01536356\n",
      "train loss: 0.01430112 | validation loss: 0.01572246\n",
      "train loss: 0.01700289 | validation loss: 0.01559873\n",
      "Epoch 32/100\n",
      "train loss: 0.01770402 | validation loss: 0.01371227\n",
      "train loss: 0.01396792 | validation loss: 0.01421868\n",
      "train loss: 0.01373943 | validation loss: 0.0136571\n",
      "train loss: 0.01924184 | validation loss: 0.01342842\n",
      "train loss: 0.01417398 | validation loss: 0.01443072\n",
      "train loss: 0.01385105 | validation loss: 0.0136121\n",
      "train loss: 0.01880294 | validation loss: 0.01342716\n",
      "Epoch 33/100\n",
      "train loss: 0.01485341 | validation loss: 0.01425845\n",
      "train loss: 0.01505875 | validation loss: 0.01300876\n",
      "train loss: 0.01245937 | validation loss: 0.01309484\n",
      "train loss: 0.01511653 | validation loss: 0.01272334\n",
      "train loss: 0.01595703 | validation loss: 0.01272583\n",
      "train loss: 0.01624245 | validation loss: 0.01252666\n",
      "train loss: 0.0187348 | validation loss: 0.01227635\n",
      "Epoch 34/100\n",
      "train loss: 0.01303434 | validation loss: 0.01265188\n",
      "train loss: 0.01282442 | validation loss: 0.0125636\n",
      "train loss: 0.01196126 | validation loss: 0.0119966\n",
      "train loss: 0.01422875 | validation loss: 0.01209088\n",
      "train loss: 0.01530855 | validation loss: 0.01163805\n",
      "train loss: 0.01556856 | validation loss: 0.01131113\n",
      "train loss: 0.01343843 | validation loss: 0.01229028\n",
      "Epoch 35/100\n",
      "train loss: 0.0117981 | validation loss: 0.01189607\n",
      "train loss: 0.01154052 | validation loss: 0.01245634\n",
      "train loss: 0.01288608 | validation loss: 0.01214499\n",
      "train loss: 0.01177881 | validation loss: 0.01088618\n",
      "train loss: 0.01086181 | validation loss: 0.01190806\n",
      "train loss: 0.01063424 | validation loss: 0.01163572\n",
      "train loss: 0.0150083 | validation loss: 0.0113496\n",
      "Epoch 36/100\n",
      "train loss: 0.01063598 | validation loss: 0.01117234\n",
      "train loss: 0.01305467 | validation loss: 0.0117538\n",
      "train loss: 0.01136343 | validation loss: 0.01141434\n",
      "train loss: 0.01268843 | validation loss: 0.01094834\n",
      "train loss: 0.01492041 | validation loss: 0.01100857\n",
      "train loss: 0.01197447 | validation loss: 0.01095784\n",
      "train loss: 0.01118138 | validation loss: 0.01150602\n",
      "Epoch 37/100\n",
      "train loss: 0.01084261 | validation loss: 0.01092884\n",
      "train loss: 0.01061248 | validation loss: 0.01056173\n",
      "train loss: 0.0131151 | validation loss: 0.01112712\n",
      "train loss: 0.01112576 | validation loss: 0.01085857\n",
      "train loss: 0.00932684 | validation loss: 0.0108186\n",
      "train loss: 0.01253164 | validation loss: 0.01051092\n",
      "train loss: 0.01020947 | validation loss: 0.01007774\n",
      "Epoch 38/100\n",
      "train loss: 0.01074049 | validation loss: 0.01015559\n",
      "train loss: 0.01097642 | validation loss: 0.0099611\n",
      "train loss: 0.01028965 | validation loss: 0.0103305\n",
      "train loss: 0.01009038 | validation loss: 0.01042533\n",
      "train loss: 0.0118241 | validation loss: 0.01030117\n",
      "train loss: 0.00859297 | validation loss: 0.0103527\n",
      "train loss: 0.00921888 | validation loss: 0.01039556\n",
      "Epoch 39/100\n",
      "train loss: 0.01043894 | validation loss: 0.00976281\n",
      "train loss: 0.00831317 | validation loss: 0.01035795\n",
      "train loss: 0.0098282 | validation loss: 0.0095598\n",
      "train loss: 0.00992414 | validation loss: 0.01001875\n",
      "train loss: 0.01228087 | validation loss: 0.01010552\n",
      "train loss: 0.01036017 | validation loss: 0.01006929\n",
      "train loss: 0.00817113 | validation loss: 0.01008872\n",
      "Epoch 40/100\n",
      "train loss: 0.01273023 | validation loss: 0.00941594\n",
      "train loss: 0.01052998 | validation loss: 0.00938086\n",
      "train loss: 0.00936552 | validation loss: 0.00922138\n",
      "train loss: 0.01173867 | validation loss: 0.00940895\n",
      "train loss: 0.00811072 | validation loss: 0.00961062\n",
      "train loss: 0.00886995 | validation loss: 0.00985549\n",
      "train loss: 0.00890038 | validation loss: 0.00921171\n",
      "Epoch 41/100\n",
      "train loss: 0.00848038 | validation loss: 0.00915361\n",
      "train loss: 0.01077867 | validation loss: 0.00915265\n",
      "train loss: 0.00873853 | validation loss: 0.00934586\n",
      "train loss: 0.00739897 | validation loss: 0.00915726\n",
      "train loss: 0.01348434 | validation loss: 0.00910106\n",
      "train loss: 0.01015401 | validation loss: 0.00938903\n",
      "train loss: 0.01037342 | validation loss: 0.00903144\n",
      "Epoch 42/100\n",
      "train loss: 0.00942921 | validation loss: 0.009293\n",
      "train loss: 0.011768 | validation loss: 0.00961184\n",
      "train loss: 0.0093134 | validation loss: 0.00894153\n",
      "train loss: 0.00752924 | validation loss: 0.0093302\n",
      "train loss: 0.0098692 | validation loss: 0.00920469\n",
      "train loss: 0.00730929 | validation loss: 0.00884811\n",
      "train loss: 0.00904361 | validation loss: 0.008891\n",
      "Epoch 43/100\n",
      "train loss: 0.01196413 | validation loss: 0.00858878\n",
      "train loss: 0.01087764 | validation loss: 0.00858739\n",
      "train loss: 0.00798217 | validation loss: 0.00901466\n",
      "train loss: 0.00711087 | validation loss: 0.00969172\n",
      "train loss: 0.0081644 | validation loss: 0.00909781\n",
      "train loss: 0.00709639 | validation loss: 0.00974647\n",
      "train loss: 0.01180803 | validation loss: 0.00844615\n",
      "Epoch 44/100\n",
      "train loss: 0.00802108 | validation loss: 0.00867525\n",
      "train loss: 0.00828052 | validation loss: 0.0086534\n",
      "train loss: 0.00856838 | validation loss: 0.009211\n",
      "train loss: 0.00919399 | validation loss: 0.00850519\n",
      "train loss: 0.00940383 | validation loss: 0.00846676\n",
      "train loss: 0.01044443 | validation loss: 0.00909758\n",
      "train loss: 0.01009984 | validation loss: 0.00899066\n",
      "Epoch 45/100\n",
      "train loss: 0.0101005 | validation loss: 0.00860215\n",
      "train loss: 0.01043617 | validation loss: 0.00807697\n",
      "train loss: 0.00831207 | validation loss: 0.00853197\n",
      "train loss: 0.00836455 | validation loss: 0.00864441\n",
      "train loss: 0.00838782 | validation loss: 0.00926726\n",
      "train loss: 0.00740336 | validation loss: 0.0086324\n",
      "train loss: 0.00837101 | validation loss: 0.00837024\n",
      "Epoch 46/100\n",
      "train loss: 0.00892104 | validation loss: 0.00806148\n",
      "train loss: 0.00898522 | validation loss: 0.00865036\n",
      "train loss: 0.00886302 | validation loss: 0.00841648\n",
      "train loss: 0.00851383 | validation loss: 0.0086002\n",
      "train loss: 0.01067732 | validation loss: 0.00828296\n",
      "train loss: 0.00790448 | validation loss: 0.00834341\n",
      "train loss: 0.01027446 | validation loss: 0.00846569\n",
      "Epoch 47/100\n",
      "train loss: 0.00626116 | validation loss: 0.00856206\n",
      "train loss: 0.01181899 | validation loss: 0.00805592\n",
      "train loss: 0.00865651 | validation loss: 0.00823271\n",
      "train loss: 0.00949552 | validation loss: 0.00792657\n",
      "train loss: 0.00790447 | validation loss: 0.00805798\n",
      "train loss: 0.00712206 | validation loss: 0.00784083\n",
      "train loss: 0.00730746 | validation loss: 0.00781119\n",
      "Epoch 48/100\n",
      "train loss: 0.01264205 | validation loss: 0.00798979\n",
      "train loss: 0.0059915 | validation loss: 0.0080791\n",
      "train loss: 0.00976647 | validation loss: 0.00845417\n",
      "train loss: 0.01006369 | validation loss: 0.00805799\n",
      "train loss: 0.01354333 | validation loss: 0.007803\n",
      "train loss: 0.00776265 | validation loss: 0.00776287\n",
      "train loss: 0.00834007 | validation loss: 0.00821639\n",
      "Epoch 49/100\n",
      "train loss: 0.00741613 | validation loss: 0.00798161\n",
      "train loss: 0.00776015 | validation loss: 0.00773984\n",
      "train loss: 0.00971313 | validation loss: 0.00758045\n",
      "train loss: 0.00563284 | validation loss: 0.00804156\n",
      "train loss: 0.01029583 | validation loss: 0.0077724\n",
      "train loss: 0.00706836 | validation loss: 0.00756137\n",
      "train loss: 0.00675666 | validation loss: 0.00796118\n",
      "Epoch 50/100\n",
      "train loss: 0.00841098 | validation loss: 0.0078683\n",
      "train loss: 0.01011215 | validation loss: 0.00789198\n",
      "train loss: 0.01038882 | validation loss: 0.00790182\n",
      "train loss: 0.00631454 | validation loss: 0.00780748\n",
      "train loss: 0.00756385 | validation loss: 0.00772313\n",
      "train loss: 0.00795452 | validation loss: 0.00769852\n",
      "train loss: 0.00683609 | validation loss: 0.0076732\n",
      "Epoch 51/100\n",
      "train loss: 0.00695248 | validation loss: 0.00755769\n",
      "train loss: 0.00753699 | validation loss: 0.00760681\n",
      "train loss: 0.00842062 | validation loss: 0.00743472\n",
      "train loss: 0.00764824 | validation loss: 0.00782112\n",
      "train loss: 0.00869376 | validation loss: 0.00754326\n",
      "train loss: 0.01027025 | validation loss: 0.0075114\n",
      "train loss: 0.00675868 | validation loss: 0.00749049\n",
      "Epoch 52/100\n",
      "train loss: 0.00686174 | validation loss: 0.00755887\n",
      "train loss: 0.00557597 | validation loss: 0.00743111\n",
      "train loss: 0.00882914 | validation loss: 0.00766289\n",
      "train loss: 0.00757012 | validation loss: 0.00782109\n",
      "train loss: 0.00576389 | validation loss: 0.00743004\n",
      "train loss: 0.00918756 | validation loss: 0.00730901\n",
      "train loss: 0.00747945 | validation loss: 0.00732414\n",
      "Epoch 53/100\n",
      "train loss: 0.0080947 | validation loss: 0.00741423\n",
      "train loss: 0.00875413 | validation loss: 0.00716908\n",
      "train loss: 0.00651416 | validation loss: 0.0073933\n",
      "train loss: 0.00518646 | validation loss: 0.00747132\n",
      "train loss: 0.00711505 | validation loss: 0.0072721\n",
      "train loss: 0.00762353 | validation loss: 0.00726264\n",
      "train loss: 0.00632731 | validation loss: 0.00733316\n",
      "Epoch 54/100\n",
      "train loss: 0.00577402 | validation loss: 0.00747657\n",
      "train loss: 0.00911521 | validation loss: 0.00745223\n",
      "train loss: 0.00852103 | validation loss: 0.0072545\n",
      "train loss: 0.00897328 | validation loss: 0.00720518\n",
      "train loss: 0.01175117 | validation loss: 0.00730174\n",
      "train loss: 0.00652522 | validation loss: 0.007067\n",
      "train loss: 0.00678756 | validation loss: 0.00743124\n",
      "Epoch 55/100\n",
      "train loss: 0.00711319 | validation loss: 0.00735207\n",
      "train loss: 0.00666038 | validation loss: 0.00745748\n",
      "train loss: 0.00602268 | validation loss: 0.00739313\n",
      "train loss: 0.00737475 | validation loss: 0.00733559\n",
      "train loss: 0.00772386 | validation loss: 0.00744459\n",
      "train loss: 0.00838969 | validation loss: 0.00728488\n",
      "train loss: 0.00744751 | validation loss: 0.00727172\n",
      "Epoch 56/100\n",
      "train loss: 0.00633883 | validation loss: 0.00761004\n",
      "train loss: 0.00643695 | validation loss: 0.00734528\n",
      "train loss: 0.01202724 | validation loss: 0.00719655\n",
      "train loss: 0.0121018 | validation loss: 0.00717822\n",
      "train loss: 0.00706663 | validation loss: 0.00733606\n",
      "train loss: 0.01229754 | validation loss: 0.00722892\n",
      "train loss: 0.00643671 | validation loss: 0.00729616\n",
      "Epoch 57/100\n",
      "train loss: 0.00628185 | validation loss: 0.00727476\n",
      "train loss: 0.00744078 | validation loss: 0.00714095\n",
      "train loss: 0.00920537 | validation loss: 0.00736441\n",
      "train loss: 0.0058817 | validation loss: 0.0072942\n",
      "train loss: 0.00611482 | validation loss: 0.00741031\n",
      "train loss: 0.00928749 | validation loss: 0.0073942\n",
      "train loss: 0.00484199 | validation loss: 0.00773113\n",
      "Epoch 58/100\n",
      "train loss: 0.00572463 | validation loss: 0.0075074\n",
      "train loss: 0.00645475 | validation loss: 0.00707464\n",
      "train loss: 0.00844446 | validation loss: 0.00748608\n",
      "train loss: 0.00610368 | validation loss: 0.0071582\n",
      "train loss: 0.00611367 | validation loss: 0.00715968\n",
      "train loss: 0.00864918 | validation loss: 0.00677113\n",
      "train loss: 0.00502152 | validation loss: 0.00701741\n",
      "Epoch 59/100\n",
      "train loss: 0.00857955 | validation loss: 0.00712495\n",
      "train loss: 0.00407075 | validation loss: 0.00696065\n",
      "train loss: 0.00819191 | validation loss: 0.00684097\n",
      "train loss: 0.00585184 | validation loss: 0.00698694\n",
      "train loss: 0.00502626 | validation loss: 0.00689801\n",
      "train loss: 0.00704183 | validation loss: 0.00682927\n",
      "train loss: 0.00458041 | validation loss: 0.00687604\n",
      "Epoch 60/100\n",
      "train loss: 0.00443241 | validation loss: 0.00680403\n",
      "train loss: 0.00630708 | validation loss: 0.00692428\n",
      "train loss: 0.00669289 | validation loss: 0.00699629\n",
      "train loss: 0.00621443 | validation loss: 0.00700541\n",
      "train loss: 0.00718261 | validation loss: 0.00658899\n",
      "train loss: 0.00718016 | validation loss: 0.00662203\n",
      "train loss: 0.00598357 | validation loss: 0.00690402\n",
      "Epoch 61/100\n",
      "train loss: 0.00615766 | validation loss: 0.00664298\n",
      "train loss: 0.00637727 | validation loss: 0.00697695\n",
      "train loss: 0.00745689 | validation loss: 0.00667606\n",
      "train loss: 0.00590496 | validation loss: 0.00652889\n",
      "train loss: 0.00469912 | validation loss: 0.00673209\n",
      "train loss: 0.00584399 | validation loss: 0.00697418\n",
      "train loss: 0.00662317 | validation loss: 0.00677902\n",
      "Epoch 62/100\n",
      "train loss: 0.00672319 | validation loss: 0.00694127\n",
      "train loss: 0.0052729 | validation loss: 0.00695502\n",
      "train loss: 0.00628079 | validation loss: 0.00676615\n",
      "train loss: 0.00712866 | validation loss: 0.00655605\n",
      "train loss: 0.00461718 | validation loss: 0.00657923\n",
      "train loss: 0.00594293 | validation loss: 0.0067038\n",
      "train loss: 0.00537451 | validation loss: 0.00676617\n",
      "Epoch 63/100\n",
      "train loss: 0.0047633 | validation loss: 0.00692017\n",
      "train loss: 0.00525675 | validation loss: 0.00686558\n",
      "train loss: 0.00673896 | validation loss: 0.00664441\n",
      "train loss: 0.00678141 | validation loss: 0.00658794\n",
      "train loss: 0.00672761 | validation loss: 0.00667921\n",
      "train loss: 0.00572246 | validation loss: 0.00646652\n",
      "train loss: 0.00803295 | validation loss: 0.00671845\n",
      "Epoch 64/100\n",
      "train loss: 0.00612844 | validation loss: 0.00663656\n",
      "train loss: 0.00393085 | validation loss: 0.00683374\n",
      "train loss: 0.00570749 | validation loss: 0.00672477\n",
      "train loss: 0.00719924 | validation loss: 0.00666989\n",
      "train loss: 0.00665139 | validation loss: 0.0065321\n",
      "train loss: 0.00521165 | validation loss: 0.0065786\n",
      "train loss: 0.00612812 | validation loss: 0.00674321\n",
      "Epoch 65/100\n",
      "train loss: 0.00584128 | validation loss: 0.00642394\n",
      "train loss: 0.00661872 | validation loss: 0.00670707\n",
      "train loss: 0.00623664 | validation loss: 0.00656236\n",
      "train loss: 0.00467173 | validation loss: 0.00676317\n",
      "train loss: 0.00611589 | validation loss: 0.00655152\n",
      "train loss: 0.00624778 | validation loss: 0.00677153\n",
      "train loss: 0.00607215 | validation loss: 0.00671067\n",
      "Epoch 66/100\n",
      "train loss: 0.00748292 | validation loss: 0.00670266\n",
      "train loss: 0.00850542 | validation loss: 0.00651299\n",
      "train loss: 0.0068885 | validation loss: 0.00666366\n",
      "train loss: 0.00520358 | validation loss: 0.00656148\n",
      "train loss: 0.00469998 | validation loss: 0.00640169\n",
      "train loss: 0.00579863 | validation loss: 0.0068345\n",
      "train loss: 0.004616 | validation loss: 0.00651422\n",
      "Epoch 67/100\n",
      "train loss: 0.00514726 | validation loss: 0.00639138\n",
      "train loss: 0.0051064 | validation loss: 0.00658354\n",
      "train loss: 0.00592183 | validation loss: 0.00635832\n",
      "train loss: 0.00435937 | validation loss: 0.00648131\n",
      "train loss: 0.00692399 | validation loss: 0.00620602\n",
      "train loss: 0.00739638 | validation loss: 0.00632983\n",
      "train loss: 0.00541925 | validation loss: 0.00640602\n",
      "Epoch 68/100\n",
      "train loss: 0.00843105 | validation loss: 0.00624982\n",
      "train loss: 0.00689727 | validation loss: 0.00632275\n",
      "train loss: 0.00743668 | validation loss: 0.00636644\n",
      "train loss: 0.00610115 | validation loss: 0.00628032\n",
      "train loss: 0.0051481 | validation loss: 0.00631843\n",
      "train loss: 0.00780018 | validation loss: 0.00642501\n",
      "train loss: 0.00718864 | validation loss: 0.00658668\n",
      "Epoch 69/100\n",
      "train loss: 0.00494445 | validation loss: 0.00659771\n",
      "train loss: 0.0048207 | validation loss: 0.00661612\n",
      "train loss: 0.00501912 | validation loss: 0.0062875\n",
      "train loss: 0.00639553 | validation loss: 0.00634475\n",
      "train loss: 0.00551216 | validation loss: 0.00629428\n",
      "train loss: 0.00487162 | validation loss: 0.00632739\n",
      "train loss: 0.00935867 | validation loss: 0.00631417\n",
      "Epoch 70/100\n",
      "train loss: 0.00599924 | validation loss: 0.00629698\n",
      "train loss: 0.00614715 | validation loss: 0.00639864\n",
      "train loss: 0.00479132 | validation loss: 0.00639323\n",
      "train loss: 0.00576316 | validation loss: 0.00627448\n",
      "train loss: 0.00614884 | validation loss: 0.00634525\n",
      "train loss: 0.00614897 | validation loss: 0.00633436\n",
      "train loss: 0.0061735 | validation loss: 0.00623771\n",
      "Epoch 71/100\n",
      "train loss: 0.00564768 | validation loss: 0.00648384\n",
      "train loss: 0.00668984 | validation loss: 0.00627259\n",
      "train loss: 0.00469164 | validation loss: 0.0064934\n",
      "train loss: 0.00708256 | validation loss: 0.00625912\n",
      "train loss: 0.00625008 | validation loss: 0.006292\n",
      "train loss: 0.00451987 | validation loss: 0.00624612\n",
      "train loss: 0.00633521 | validation loss: 0.00621199\n",
      "Epoch 72/100\n",
      "train loss: 0.00457957 | validation loss: 0.00643093\n",
      "train loss: 0.0047647 | validation loss: 0.0064263\n",
      "train loss: 0.00481525 | validation loss: 0.00626353\n",
      "train loss: 0.00488759 | validation loss: 0.00622411\n",
      "train loss: 0.00555432 | validation loss: 0.00636821\n",
      "train loss: 0.00477273 | validation loss: 0.00637187\n",
      "train loss: 0.00536028 | validation loss: 0.0062266\n",
      "Epoch 73/100\n",
      "train loss: 0.00847792 | validation loss: 0.00634415\n",
      "train loss: 0.00472118 | validation loss: 0.00637329\n",
      "train loss: 0.00486506 | validation loss: 0.00618372\n",
      "train loss: 0.00508845 | validation loss: 0.0062388\n",
      "train loss: 0.00616105 | validation loss: 0.00623791\n",
      "train loss: 0.00618104 | validation loss: 0.00619454\n",
      "train loss: 0.00818948 | validation loss: 0.00627005\n",
      "Epoch 74/100\n",
      "train loss: 0.00661603 | validation loss: 0.00623493\n",
      "train loss: 0.00639112 | validation loss: 0.00635944\n",
      "train loss: 0.00688066 | validation loss: 0.00632347\n",
      "train loss: 0.00594415 | validation loss: 0.00629878\n",
      "train loss: 0.00549221 | validation loss: 0.00637665\n",
      "train loss: 0.0065193 | validation loss: 0.00625681\n",
      "train loss: 0.00655809 | validation loss: 0.00619133\n",
      "Epoch 75/100\n",
      "train loss: 0.00632809 | validation loss: 0.00612819\n",
      "train loss: 0.00549134 | validation loss: 0.00627465\n",
      "train loss: 0.00690526 | validation loss: 0.00629023\n",
      "train loss: 0.00805299 | validation loss: 0.0062952\n",
      "train loss: 0.00338625 | validation loss: 0.00619819\n",
      "train loss: 0.00707909 | validation loss: 0.00624799\n",
      "train loss: 0.00621701 | validation loss: 0.00630099\n",
      "Epoch 76/100\n",
      "train loss: 0.00805907 | validation loss: 0.00638663\n",
      "train loss: 0.00537839 | validation loss: 0.00636699\n",
      "train loss: 0.00511522 | validation loss: 0.00635047\n",
      "train loss: 0.00699502 | validation loss: 0.0061742\n",
      "train loss: 0.0130177 | validation loss: 0.00615999\n",
      "train loss: 0.0074803 | validation loss: 0.00609619\n",
      "train loss: 0.0055581 | validation loss: 0.00635275\n",
      "Epoch 77/100\n",
      "train loss: 0.00640074 | validation loss: 0.0061259\n",
      "train loss: 0.00739926 | validation loss: 0.00621116\n",
      "train loss: 0.0048018 | validation loss: 0.00619158\n",
      "train loss: 0.00422352 | validation loss: 0.00615816\n",
      "train loss: 0.00502197 | validation loss: 0.00642458\n",
      "train loss: 0.00604153 | validation loss: 0.00637869\n",
      "train loss: 0.00400084 | validation loss: 0.00612911\n",
      "Epoch 78/100\n",
      "train loss: 0.00419215 | validation loss: 0.00616865\n",
      "train loss: 0.00532685 | validation loss: 0.00610774\n",
      "train loss: 0.00562256 | validation loss: 0.0061249\n",
      "train loss: 0.00497315 | validation loss: 0.00624733\n",
      "train loss: 0.00709083 | validation loss: 0.00612961\n",
      "train loss: 0.00480498 | validation loss: 0.00627134\n",
      "train loss: 0.00496231 | validation loss: 0.00614412\n",
      "Epoch 79/100\n",
      "train loss: 0.00579549 | validation loss: 0.00612523\n",
      "train loss: 0.00500657 | validation loss: 0.00606955\n",
      "train loss: 0.00555842 | validation loss: 0.00617184\n",
      "train loss: 0.00381381 | validation loss: 0.00610411\n",
      "train loss: 0.00954153 | validation loss: 0.0061869\n",
      "train loss: 0.0068494 | validation loss: 0.00611712\n",
      "train loss: 0.00567654 | validation loss: 0.00612171\n",
      "Epoch 80/100\n",
      "train loss: 0.00524784 | validation loss: 0.00620281\n",
      "train loss: 0.00447203 | validation loss: 0.0061307\n",
      "train loss: 0.0044085 | validation loss: 0.00613851\n",
      "train loss: 0.00449877 | validation loss: 0.00616773\n",
      "train loss: 0.00405346 | validation loss: 0.0061317\n",
      "train loss: 0.00401151 | validation loss: 0.0061051\n",
      "train loss: 0.00447037 | validation loss: 0.00611392\n",
      "Epoch 81/100\n",
      "train loss: 0.00544138 | validation loss: 0.00604635\n",
      "train loss: 0.00665517 | validation loss: 0.00608404\n",
      "train loss: 0.00659691 | validation loss: 0.0061846\n",
      "train loss: 0.00614966 | validation loss: 0.00613425\n",
      "train loss: 0.00639372 | validation loss: 0.0060665\n",
      "train loss: 0.0062856 | validation loss: 0.00604761\n",
      "train loss: 0.00778254 | validation loss: 0.00607427\n",
      "Epoch 82/100\n",
      "train loss: 0.00462205 | validation loss: 0.00612699\n",
      "train loss: 0.00520698 | validation loss: 0.00597648\n",
      "train loss: 0.00501353 | validation loss: 0.00606938\n",
      "train loss: 0.00776752 | validation loss: 0.0060702\n",
      "train loss: 0.00612032 | validation loss: 0.00612534\n",
      "train loss: 0.00489044 | validation loss: 0.00610779\n",
      "train loss: 0.00483088 | validation loss: 0.00605662\n",
      "Epoch 83/100\n",
      "train loss: 0.00785356 | validation loss: 0.00612633\n",
      "train loss: 0.00491625 | validation loss: 0.00616401\n",
      "train loss: 0.00627631 | validation loss: 0.00606751\n",
      "train loss: 0.0068002 | validation loss: 0.00609711\n",
      "train loss: 0.00581761 | validation loss: 0.00606712\n",
      "train loss: 0.00719531 | validation loss: 0.00622341\n",
      "train loss: 0.00567088 | validation loss: 0.00600594\n",
      "Epoch 84/100\n",
      "train loss: 0.00478414 | validation loss: 0.00608789\n",
      "train loss: 0.00619412 | validation loss: 0.00608785\n",
      "train loss: 0.00421151 | validation loss: 0.00612424\n",
      "train loss: 0.00468544 | validation loss: 0.00605965\n",
      "train loss: 0.00434655 | validation loss: 0.00601814\n",
      "train loss: 0.00475872 | validation loss: 0.00611551\n",
      "train loss: 0.00655519 | validation loss: 0.00604693\n",
      "Epoch 85/100\n",
      "train loss: 0.00516517 | validation loss: 0.00609345\n",
      "train loss: 0.00423249 | validation loss: 0.00604881\n",
      "train loss: 0.00514531 | validation loss: 0.00605229\n",
      "train loss: 0.00649412 | validation loss: 0.00605738\n",
      "train loss: 0.00567488 | validation loss: 0.00600641\n",
      "train loss: 0.00648296 | validation loss: 0.00608479\n",
      "train loss: 0.00422698 | validation loss: 0.00630912\n",
      "Epoch 86/100\n",
      "train loss: 0.00464933 | validation loss: 0.00610231\n",
      "train loss: 0.00501009 | validation loss: 0.00602679\n",
      "train loss: 0.00808158 | validation loss: 0.00604869\n",
      "train loss: 0.00744628 | validation loss: 0.00617674\n",
      "train loss: 0.00458776 | validation loss: 0.00605911\n",
      "train loss: 0.00479565 | validation loss: 0.00601833\n",
      "train loss: 0.00537689 | validation loss: 0.00605665\n",
      "Epoch 87/100\n",
      "train loss: 0.00476247 | validation loss: 0.00614962\n",
      "train loss: 0.00714114 | validation loss: 0.00605361\n",
      "train loss: 0.00716735 | validation loss: 0.00599758\n",
      "train loss: 0.00550183 | validation loss: 0.00601838\n",
      "train loss: 0.00749678 | validation loss: 0.00604694\n",
      "train loss: 0.00495232 | validation loss: 0.00598813\n",
      "train loss: 0.00809918 | validation loss: 0.00619695\n",
      "Epoch 88/100\n",
      "train loss: 0.00458456 | validation loss: 0.00604423\n",
      "train loss: 0.00484494 | validation loss: 0.00605105\n",
      "train loss: 0.00636548 | validation loss: 0.00597475\n",
      "train loss: 0.00770833 | validation loss: 0.00599064\n",
      "train loss: 0.00394449 | validation loss: 0.00599518\n",
      "train loss: 0.00570481 | validation loss: 0.00613732\n",
      "train loss: 0.00881696 | validation loss: 0.00603783\n",
      "Epoch 89/100\n",
      "train loss: 0.00621685 | validation loss: 0.00603647\n",
      "train loss: 0.00588669 | validation loss: 0.00598812\n",
      "train loss: 0.00735243 | validation loss: 0.00602714\n",
      "train loss: 0.00600428 | validation loss: 0.00598732\n",
      "train loss: 0.00488768 | validation loss: 0.00597913\n",
      "train loss: 0.00569024 | validation loss: 0.00595832\n",
      "train loss: 0.00433501 | validation loss: 0.00606256\n",
      "Epoch 90/100\n",
      "train loss: 0.00587071 | validation loss: 0.00597572\n",
      "train loss: 0.0050228 | validation loss: 0.00600303\n",
      "train loss: 0.00510943 | validation loss: 0.00604739\n",
      "train loss: 0.00738582 | validation loss: 0.00595445\n",
      "train loss: 0.0053256 | validation loss: 0.00595927\n",
      "train loss: 0.00481039 | validation loss: 0.00589724\n",
      "train loss: 0.00580956 | validation loss: 0.00595553\n",
      "Epoch 91/100\n",
      "train loss: 0.00554683 | validation loss: 0.00597957\n",
      "train loss: 0.00831753 | validation loss: 0.0059497\n",
      "train loss: 0.00379055 | validation loss: 0.00591883\n",
      "train loss: 0.00792095 | validation loss: 0.00598865\n",
      "train loss: 0.00543354 | validation loss: 0.00601206\n",
      "train loss: 0.00567923 | validation loss: 0.00595709\n",
      "train loss: 0.00513197 | validation loss: 0.00596746\n",
      "Epoch 92/100\n",
      "train loss: 0.00456977 | validation loss: 0.00600552\n",
      "train loss: 0.00480237 | validation loss: 0.00601307\n",
      "train loss: 0.00679434 | validation loss: 0.00599146\n",
      "train loss: 0.00667012 | validation loss: 0.00593264\n",
      "train loss: 0.00610426 | validation loss: 0.00589213\n",
      "train loss: 0.00912806 | validation loss: 0.00594272\n",
      "train loss: 0.00458214 | validation loss: 0.0059279\n",
      "Epoch 93/100\n",
      "train loss: 0.00415797 | validation loss: 0.00601583\n",
      "train loss: 0.00411069 | validation loss: 0.00598184\n",
      "train loss: 0.00487261 | validation loss: 0.00596458\n",
      "train loss: 0.00507625 | validation loss: 0.00591763\n",
      "train loss: 0.00405107 | validation loss: 0.005939\n",
      "train loss: 0.00375991 | validation loss: 0.00587387\n",
      "train loss: 0.00774742 | validation loss: 0.00586309\n",
      "Epoch 94/100\n",
      "train loss: 0.00802402 | validation loss: 0.00591751\n",
      "train loss: 0.00555499 | validation loss: 0.00587557\n",
      "train loss: 0.00913997 | validation loss: 0.00586894\n",
      "train loss: 0.00367517 | validation loss: 0.00600461\n",
      "train loss: 0.00617824 | validation loss: 0.00590071\n",
      "train loss: 0.00624288 | validation loss: 0.00590579\n",
      "train loss: 0.00362328 | validation loss: 0.00582352\n",
      "Epoch 95/100\n",
      "train loss: 0.00507013 | validation loss: 0.0058789\n",
      "train loss: 0.00691646 | validation loss: 0.00587606\n",
      "train loss: 0.00629735 | validation loss: 0.00588612\n",
      "train loss: 0.00720983 | validation loss: 0.0058789\n",
      "train loss: 0.00626421 | validation loss: 0.00589686\n",
      "train loss: 0.00610602 | validation loss: 0.00583695\n",
      "train loss: 0.00737668 | validation loss: 0.00593757\n",
      "Epoch 96/100\n",
      "train loss: 0.00491117 | validation loss: 0.00590189\n",
      "train loss: 0.00530878 | validation loss: 0.00595701\n",
      "train loss: 0.00701858 | validation loss: 0.00587629\n",
      "train loss: 0.00547254 | validation loss: 0.00591577\n",
      "train loss: 0.00490928 | validation loss: 0.00602324\n",
      "train loss: 0.0082216 | validation loss: 0.00590843\n",
      "train loss: 0.00588844 | validation loss: 0.00583215\n",
      "Epoch 97/100\n",
      "train loss: 0.0070011 | validation loss: 0.00588082\n",
      "train loss: 0.00456846 | validation loss: 0.00581439\n",
      "train loss: 0.0071047 | validation loss: 0.0059552\n",
      "train loss: 0.00664552 | validation loss: 0.00594603\n",
      "train loss: 0.00605439 | validation loss: 0.0058245\n",
      "train loss: 0.00417248 | validation loss: 0.00608106\n",
      "train loss: 0.00534123 | validation loss: 0.00593742\n",
      "Epoch 98/100\n",
      "train loss: 0.0044885 | validation loss: 0.00586911\n",
      "train loss: 0.00547078 | validation loss: 0.00602849\n",
      "train loss: 0.00566024 | validation loss: 0.00588153\n",
      "train loss: 0.00456984 | validation loss: 0.0059473\n",
      "train loss: 0.00825291 | validation loss: 0.00582636\n",
      "train loss: 0.0048952 | validation loss: 0.00580973\n",
      "train loss: 0.0058124 | validation loss: 0.0058408\n",
      "Epoch 99/100\n",
      "train loss: 0.00526857 | validation loss: 0.00587439\n",
      "train loss: 0.00725691 | validation loss: 0.00595378\n",
      "train loss: 0.00809468 | validation loss: 0.00587889\n",
      "train loss: 0.00514214 | validation loss: 0.00581648\n",
      "train loss: 0.00503191 | validation loss: 0.00599845\n",
      "train loss: 0.00863446 | validation loss: 0.00590541\n",
      "train loss: 0.00356512 | validation loss: 0.00593981\n",
      "Epoch 100/100\n",
      "train loss: 0.00656847 | validation loss: 0.00584409\n",
      "train loss: 0.00482687 | validation loss: 0.00588707\n",
      "train loss: 0.00464633 | validation loss: 0.00595635\n",
      "train loss: 0.00637906 | validation loss: 0.00594076\n",
      "train loss: 0.00431732 | validation loss: 0.00581111\n",
      "train loss: 0.00793034 | validation loss: 0.00578715\n",
      "train loss: 0.00528082 | validation loss: 0.00585318\n"
     ]
    }
   ],
   "source": [
    "ae_5.train_loop(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
